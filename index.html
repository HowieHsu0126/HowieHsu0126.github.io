<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Chance favors the prepared mind."><meta name="keywords" content=""><meta name="author" content="Haowei"><meta name="copyright" content="Haowei"><title>阿玮的米奇妙妙屋 | Haowei Hub</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b052bfe0975d2cbea2572b7ad21630a2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/iShot_2023-06-27_20.19.54.png"></div><div class="author-info__name text-center">Haowei</div><div class="author-info__description text-center">Chance favors the prepared mind.</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/HowieHsu0126">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">10</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">13</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div></div></div><nav id="nav" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Haowei Hub</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">博客</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="site-info"><div id="site-title">Haowei Hub</div><div id="site-sub-title">阿玮的米奇妙妙屋</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/HowieHsu0126" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fab"></i></a><a class="social-icon" href="https://www.zhihu.com/people/howie-97-70" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-quora fab"></i></a><a class="social-icon" href="https://www.linkedin.com/in/haoweixu/" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-linkedin fab"></i></a><a class="social-icon" href="mailto://howie0126@163.com" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-envelope  fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/29/Transformer%20for%20Graphs%EF%BC%9AAn%20Overview%20from%20Architecture%20Perspective/">Transformer for Graphs：An Overview from Architecture Perspective</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87/">综述论文</a></span><div class="content">1 📑Metadata

摘要 - Recently, Transformer model, which has achieved great success
in many artiﬁcial intelligence ﬁelds, has demonstrated its great
pote ...</div><a class="more" href="/2023/07/29/Transformer%20for%20Graphs%EF%BC%9AAn%20Overview%20from%20Architecture%20Perspective/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/28/%E5%A6%82%E4%BD%95%E6%8A%8A%E8%BF%91%E5%8D%81%E9%A1%B5%E7%9A%84%E8%AE%BA%E6%96%87%E8%AF%BB%E6%88%90%E5%8D%8A%E9%A1%B5/">我是如何把近十页的论文读成半页的</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%AF%BB/">读</a></span><div class="content">
相关视频：李沐：如何读论文


1 阅读流程
论文通常分为6个部分：Title、Abstract、Introduction (包含Related
works)、Method、Experiments、Conclusion。
沐神建议大家分三遍来读一篇论文。
第一遍，目的是大概知道论文在讲什么，适不适 ...</div><a class="more" href="/2023/07/28/%E5%A6%82%E4%BD%95%E6%8A%8A%E8%BF%91%E5%8D%81%E9%A1%B5%E7%9A%84%E8%AE%BA%E6%96%87%E8%AF%BB%E6%88%90%E5%8D%8A%E9%A1%B5/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/27/Representing%20Long-Range%20Context%20for%20Graph%20Neural%20Networks%20with%20Global%20Attention/">Representing Long-Range Context for Graph Neural Networks with Global Attention</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><div class="content">1 📑Metadata

[!abstract] 摘要
Graph neural networks are powerful architectures for structured
datasets. However, current methods struggle to represent  ...</div><a class="more" href="/2023/07/27/Representing%20Long-Range%20Context%20for%20Graph%20Neural%20Networks%20with%20Global%20Attention/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/24/PRODIGY-Enabling%20In-context%20Learning%20Over%20Graphs/">PRODIGY：Enabling In-context Learning Over Graphs</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Exploration/">Exploration</a></span><div class="content">1 📑Metadata

摘要
[[In-context learning]] is the ability of a pretrained model to adapt to
novel and diverse downstream tasks by conditioning on prompt ...</div><a class="more" href="/2023/07/24/PRODIGY-Enabling%20In-context%20Learning%20Over%20Graphs/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/24/Graph%20Inductive%20Biases%20in%20Transformers%20without%20Message%20Passing/">Graph Inductive Biases in Transformers without Message Passing</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><div class="content">1 📑Metadata

摘要
Transformers for graph data are increasingly widely studied and
successful in numerous learning tasks. Graph [[inductive biases]] are ...</div><a class="more" href="/2023/07/24/Graph%20Inductive%20Biases%20in%20Transformers%20without%20Message%20Passing/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/24/GPS%EF%BC%9ARecipe%20for%20a%20General,%20Powerful,%20Scalable%20Graph%20Transformer/">Recipe for a General, Powerful, Scalable Graph Transformer</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><div class="content">1 📑Metadata

摘要
We propose a recipe on how to build a general, powerful, scalable
(GPS) graph Transformer with linear complexity and state-of-the-art ...</div><a class="more" href="/2023/07/24/GPS%EF%BC%9ARecipe%20for%20a%20General,%20Powerful,%20Scalable%20Graph%20Transformer/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/06/26/Search-to-aggregate-neighborhood-for-graph-neural-network/">Search to Aggregate NEighborhood for Graph Neural Network</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/">经典论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Fine-tune/">Fine-tune</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2/">神经架构搜索</a></span><div class="content">1 📑Metadata

信息

标题：Search to
aggregate neighborhood for graph neural network
作者: Zhao, Huan; Yao, Quanming; Tu, Weiwei
团队: [[QuanmingYao]]
年份: 2021
 ...</div><a class="more" href="/2023/06/26/Search-to-aggregate-neighborhood-for-graph-neural-network/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/06/24/%E9%9B%85%E6%80%9D%E5%86%99%E4%BD%9C%E7%9C%9F%E7%BB%8F%E6%80%BB%E7%BA%B2/">雅思写作真经总纲</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%8B%B1%E8%AF%AD/">英语</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E9%9B%85%E6%80%9D/">雅思</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%86%99%E4%BD%9C/">写作</a></span><div class="content">1 应试
1.1 5 字决：思、准、通、转、稳

1.2 考试内容

1. 写作是雅思考试的第三部分, 在听力、阅读之后, 考试时间为考试日
(通常为周六)上午 11 点至 12点。 2. Task 1 和 Task 2
的分值权重为 3 比 7。 3. Task 2 中议论文
(Argumenta ...</div><a class="more" href="/2023/06/24/%E9%9B%85%E6%80%9D%E5%86%99%E4%BD%9C%E7%9C%9F%E7%BB%8F%E6%80%BB%E7%BA%B2/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/06/23/A-new-model-for-learning-in-graph-domains/">A New Model for Learning in Graph Domains</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-23</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/">经典论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/GNN/">GNN</a></span><div class="content">1 📑摘要与结论
1.1 摘要

原文： In several applications the information is naturally
represented by graphs. Traditional approaches cope with graphical data
stru ...</div><a class="more" href="/2023/06/23/A-new-model-for-learning-in-graph-domains/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/06/23/DARTS/">DARTS：Differentiable Architecture Search</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-23</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/">经典论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Fine-tune/">Fine-tune</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2/">神经架构搜索</a></span><div class="content">
This paper addresses the scalability challenge of architecture search
by formulating the task in a differentiable manner. Unlike conventional
approac ...</div><a class="more" href="/2023/06/23/DARTS/#more" style="margin-top: 14px">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div class="layout" id="footer"><div class="copyright">&copy;2023 By Haowei</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Be open-minded, be impatient, be hopeful.</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>