<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Chance favors the prepared mind."><meta name="keywords" content=""><meta name="author" content="Haowei"><meta name="copyright" content="Haowei"><title>阿玮的米奇妙妙屋 | Haowei Hub</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b052bfe0975d2cbea2572b7ad21630a2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/iShot_2023-06-27_20.19.54.png"></div><div class="author-info__name text-center">Haowei</div><div class="author-info__description text-center">Chance favors the prepared mind.</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/HowieHsu0126">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">16</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">21</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">4</span></a></div></div></div><nav id="nav" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Haowei Hub</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">博客</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="site-info"><div id="site-title">Haowei Hub</div><div id="site-sub-title">阿玮的米奇妙妙屋</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/HowieHsu0126" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fab"></i></a><a class="social-icon" href="https://www.zhihu.com/people/howie-97-70" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-quora fab"></i></a><a class="social-icon" href="https://www.linkedin.com/in/haoweixu/" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-linkedin fab"></i></a><a class="social-icon" href="mailto://howie0126@163.com" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-envelope  fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2023/10/18/Reinforcement%20Learning%20in%20Healthcare/">Reinforcement Learning in Healthcare</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-10-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%AF%B4/">说</a></span><div class="content">1 Slide 1: Introduction

Good morning/afternoon everyone. Today, I'm excited to talk about an
emerging field that combines Artificial Intelligence and ...</div><a class="more" href="/2023/10/18/Reinforcement%20Learning%20in%20Healthcare/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/10/18/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%91%A8%E6%9C%AB%E5%AE%8C%E6%88%90%E4%B8%80%E7%AF%87%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/">如何在周末完成一篇学术论文</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-10-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%86%99/">写</a></span><div class="content">
油管上看了UMN化学系某教授的工作日常有感，写完不禁感慨自己的科研效率还是不高。

1 前期准备工作

回顾之前在阅读相关文献时所做的笔记
更新相关论文的检索
确定论文的读者

确定论文的种类：研究 or 综述 or 教学
想投稿的杂志
最重要的读者是审稿人


2 整体构思

撰写初稿是最具创造 ...</div><a class="more" href="/2023/10/18/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%91%A8%E6%9C%AB%E5%AE%8C%E6%88%90%E4%B8%80%E7%AF%87%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/10/18/Veracity-aware%20and%20Event-driven%20Personalized%20News%20Recommendation%20for%20Fake%20News%20Mitigation/">Veracity-aware and Event-driven Personalized News Recommendation for Fake News Mitigation</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-10-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF/">前沿</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E9%A1%B6%E4%BC%9A/">顶会</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></span><div class="content">1 摘要

Despite the tremendous efforts by social media platforms and
factcheck services for fake news detection, fake news and misinformation
still spre ...</div><a class="more" href="/2023/10/18/Veracity-aware%20and%20Event-driven%20Personalized%20News%20Recommendation%20for%20Fake%20News%20Mitigation/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/10/18/Transformers%20are%20Graph%20Neural%20Networks/">Transformers are Graph Neural Networks</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-10-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%8D%9A%E5%AE%A2/">博客</a></span><div class="content">1 Transformer
1.1 Representation Learning for
NLP


image.png


NNs build representations of input data as vectors/embeddings, which
encode useful sta ...</div><a class="more" href="/2023/10/18/Transformers%20are%20Graph%20Neural%20Networks/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/10/18/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">A Brief Review of Numerical Analysis</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-10-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%9F%BA%E6%9C%AC%E5%8A%9F/">基本功</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/">数值分析</a></span><div class="content">
参考资料：

数值分析 第五版 (李庆扬，王能超，易大义)
数值计算方法与算法（张韵华，奚梅成，陈效群）
数值方法 （关治，陆金甫）
Linear Algebra Done Right（Sheldon Axler）


1 基础
1.1 误差
1.1.1 误差定义

绝对误差: 设  为精确值,  ...</div><a class="more" href="/2023/10/18/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/08/02/Do%20Transformers%20Really%20Perform%20Bad%20for%20Graph%20Representation/">Do Transformers Really Perform Bad for Graph Representation?</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-08-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><div class="content">1 📑Metadata

The Transformer architecture has become a dominant choice in many
domains, such as natural language processing and computer vision. Yet, ...</div><a class="more" href="/2023/08/02/Do%20Transformers%20Really%20Perform%20Bad%20for%20Graph%20Representation/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/29/Transformer%20for%20Graphs%EF%BC%9AAn%20Overview%20from%20Architecture%20Perspective/">Transformer for Graphs：An Overview from Architecture Perspective</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87/">综述论文</a></span><div class="content">1 📑Metadata

摘要 - Recently, Transformer model, which has achieved great success
in many artiﬁcial intelligence ﬁelds, has demonstrated its great
pote ...</div><a class="more" href="/2023/07/29/Transformer%20for%20Graphs%EF%BC%9AAn%20Overview%20from%20Architecture%20Perspective/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/28/%E5%A6%82%E4%BD%95%E6%8A%8A%E8%BF%91%E5%8D%81%E9%A1%B5%E7%9A%84%E8%AE%BA%E6%96%87%E8%AF%BB%E6%88%90%E5%8D%8A%E9%A1%B5/">我是如何把近十页的论文读成半页的</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%A7%91%E7%A0%94%E7%B4%A0%E5%85%BB/">科研素养</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%AF%BB/">读</a></span><div class="content">
相关视频：李沐：如何读论文


1 阅读流程
论文通常分为6个部分：Title、Abstract、Introduction (包含Related
works)、Method、Experiments、Conclusion。
沐神建议大家分三遍来读一篇论文。
第一遍，目的是大概知道论文在讲什么，适不适 ...</div><a class="more" href="/2023/07/28/%E5%A6%82%E4%BD%95%E6%8A%8A%E8%BF%91%E5%8D%81%E9%A1%B5%E7%9A%84%E8%AE%BA%E6%96%87%E8%AF%BB%E6%88%90%E5%8D%8A%E9%A1%B5/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/27/Representing%20Long-Range%20Context%20for%20Graph%20Neural%20Networks%20with%20Global%20Attention/">Representing Long-Range Context for Graph Neural Networks with Global Attention</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Model/">Model</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><div class="content">1 📑Metadata

[!abstract] 摘要
Graph neural networks are powerful architectures for structured
datasets. However, current methods struggle to represent  ...</div><a class="more" href="/2023/07/27/Representing%20Long-Range%20Context%20for%20Graph%20Neural%20Networks%20with%20Global%20Attention/#more" style="margin-top: 14px">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/24/PRODIGY-Enabling%20In-context%20Learning%20Over%20Graphs/">PRODIGY：Enabling In-context Learning Over Graphs</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Exploration/">Exploration</a></span><div class="content">1 📑Metadata

摘要
[[In-context learning]] is the ability of a pretrained model to adapt to
novel and diverse downstream tasks by conditioning on prompt ...</div><a class="more" href="/2023/07/24/PRODIGY-Enabling%20In-context%20Learning%20Over%20Graphs/#more" style="margin-top: 14px">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div class="layout" id="footer"><div class="copyright">&copy;2023 By Haowei</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Be open-minded, be impatient, be hopeful.</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>