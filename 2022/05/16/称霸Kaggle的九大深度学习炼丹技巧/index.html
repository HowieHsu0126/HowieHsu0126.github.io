<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="称霸Kaggle的九大深度学习炼丹技巧"><meta name="keywords" content="科研 - 深度学习,公众号"><meta name="author" content="Haowei"><meta name="copyright" content="Haowei"><title>称霸Kaggle的九大深度学习炼丹技巧 | Haowei-Hub</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b052bfe0975d2cbea2572b7ad21630a2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A7%B0%E9%9C%B8Kaggle%E7%9A%84%E4%B9%9D%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7"><span class="toc-number">1.</span> <span class="toc-text">称霸Kaggle的九大深度学习炼丹技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E8%80%8C%E4%B8%8D%E6%98%AF%E5%8D%95%E4%B8%80%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.1.</span> <span class="toc-text">使用多个而不是单一学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E5%90%88%E9%80%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.2.</span> <span class="toc-text">如何找到合适的学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB"><span class="toc-number">1.3.</span> <span class="toc-text">余弦退火</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%A6%E9%87%8D%E5%90%AF%E7%9A%84SGD%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">带重启的SGD算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E6%A0%BC%E5%8C%96%E4%BD%A0%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">人格化你的激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E9%97%AE%E9%A2%98%E4%B8%AD%E9%9D%9E%E5%B8%B8%E6%9C%89%E6%95%88"><span class="toc-number">1.6.</span> <span class="toc-text">迁移学习在NLP问题中非常有效</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E4%B8%8A%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">1.7.</span> <span class="toc-text">深度学习在处理结构化数据上的优势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%EF%BC%9ADropout%E5%B1%82%E3%80%81%E5%B0%BA%E5%AF%B8%E8%AE%BE%E7%BD%AE%E3%80%81TTA"><span class="toc-number">1.8.</span> <span class="toc-text">更多内置函数：Dropout层、尺寸设置、TTA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E5%8A%9B%E5%BE%88%E5%85%B3%E9%94%AE"><span class="toc-number">1.9.</span> <span class="toc-text">创新力很关键</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.10.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Haowei</div><div class="author-info__description text-center">Chance favors the prepared mind.</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/HowieHsu0126">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">60</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">22</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://gitee.com/Howie0126">Gitee</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Haowei-Hub</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">博客</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">称霸Kaggle的九大深度学习炼丹技巧</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-05-16</time><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="称霸Kaggle的九大深度学习炼丹技巧"><a href="#称霸Kaggle的九大深度学习炼丹技巧" class="headerlink" title="称霸Kaggle的九大深度学习炼丹技巧"></a>称霸Kaggle的九大深度学习炼丹技巧</h1><p>在各种Kaggle竞赛的排行榜上，都有不少刚刚进入深度学习领域的程序员。</p>
<h2 id="使用多个而不是单一学习率"><a href="#使用多个而不是单一学习率" class="headerlink" title="使用多个而不是单一学习率"></a><strong>使用多个而不是单一学习率</strong></h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/US10Gcd0tQEfcffueY0reDaT8agHibMbkl6VPJicIaSLBOMT46hKst5wjTztibed2dJsrke6B0nRpRPvJXnC2mlSg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeU7MVRZnvPUvbx1b2ecmLcplbKs3N8pMoQ4gJEhxAL7wJNqKp1TsOq6w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>差分学习率</strong>（Differential Learning rates）意味着在训练时变换网络层比提高网络深度更重要。</p>
<p>基于已有模型来训练深度学习网络，这是一种被验证过很可靠的方法，可以在计算机视觉任务中得到更好的效果。</p>
<p>大部分已有网络（如Resnet、VGG和Inception等）都是在ImageNet数据集训练的，因此我们要根据所用数据集<strong>与ImageNet图像的相似性</strong>，来适当改变网络权重。</p>
<p>在修改这些权重时，我们通常要对模型的最后几层进行修改，因为这些层被用于检测基本特征（如边缘和轮廓），不同数据集有着不同基本特征。</p>
<p>首先，要使用Fast.ai库来<strong>获得</strong>预训练的模型，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from fastai.conv_learner import *</span><br><span class="line"></span><br><span class="line"># import library for creating learning object for convolutional #networks</span><br><span class="line">model = VVG16()</span><br><span class="line"></span><br><span class="line"># assign model to resnet, vgg, or even your own custom model</span><br><span class="line">PATH = &#x27;./folder_containing_images&#x27; </span><br><span class="line">data = ImageClassifierData.from_paths(PATH)</span><br><span class="line"></span><br><span class="line"># create fast ai data object, in this method we use from_paths where </span><br><span class="line"># inside PATH each image class is separated into different folders</span><br><span class="line"></span><br><span class="line">learn = ConvLearner.pretrained(model, data, precompute=True)</span><br><span class="line"></span><br><span class="line"># create a learn object to quickly utilise state of the art</span><br><span class="line"># techniques from the fast ai library</span><br></pre></td></tr></table></figure>

<p>创建学习对象之后（learn object），通过<strong>快速冻结</strong>前面网络层并<strong>微调</strong>后面网络层来解决问题：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">learn.freeze()</span><br><span class="line"></span><br><span class="line"># freeze layers up to the last one, so weights will not be updated.</span><br><span class="line"></span><br><span class="line">learning_rate = 0.1</span><br><span class="line">learn.fit(learning_rate, epochs=3)</span><br><span class="line"></span><br><span class="line"># train only the last layer for a few epochs</span><br></pre></td></tr></table></figure>

<p>当后面网络层产生了良好效果，我们会应用<strong>差分学习率</strong>来改变前面网络层。在实际中，一般将学习率的缩小倍数设置为<strong>10</strong>倍：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">learn.unfreeze()</span><br><span class="line"></span><br><span class="line"># set requires_grads to be True for all layers, so they can be updated</span><br><span class="line"></span><br><span class="line">learning_rate = [0.001, 0.01, 0.1]</span><br><span class="line"># learning rate is set so that deepest third of layers have a rate of 0.001, # middle layers have a rate of 0.01, and final layers 0.1.</span><br><span class="line"></span><br><span class="line">learn.fit(learning_rate, epochs=3)</span><br><span class="line"># train model for three epoch with using differential learning rates</span><br></pre></td></tr></table></figure>

<h2 id="如何找到合适的学习率"><a href="#如何找到合适的学习率" class="headerlink" title="如何找到合适的学习率"></a><strong>如何找到合适的学习率</strong></h2><p>学习率是神经网络训练中最重要的超参数，没有之一，但之前在实际应用中很难为神经网络选择最佳的学习率。</p>
<p>Leslie Smith的一篇<strong>周期性学习率论文</strong>发现了答案，这是一个相对不知名的发现，直到它被Fast.ai课程推广后才逐渐被广泛使用。</p>
<p>这篇论文是：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a></p>
<p>在这种方法中，我们尝试使用较低学习率来训练神经网络，但是在每个批次中以指数形式增加，相应代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">learn.lr_find()</span><br><span class="line"># run on learn object where learning rate is increased  exponentially</span><br><span class="line"></span><br><span class="line">learn.sched.plot_lr()</span><br><span class="line"># plot graph of learning rate against iterations</span><br></pre></td></tr></table></figure>

<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeUV6mO6TjwXhSJBxe7b03Xpag42yGzic2YqLlwvr0diaYC8YaFxrYDY5xw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>同时，记录每个学习率对应的Loss值，然后画出学习率和Loss值的关系图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.sched.plot()</span><br><span class="line"># plots the loss against the learning rate</span><br></pre></td></tr></table></figure>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwb0qia3sxLcBUOzDpAlKfJ6duNdBTHhNzPEOwECjYrJGen0Vb9Ij7O3tEt6cEibYBIDh0zfIPaaiaw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>通过找出学习率最高且Loss值仍在下降的值来确定最佳学习率。在上述情况中，该值将为0.01。</p>
<h2 id="余弦退火"><a href="#余弦退火" class="headerlink" title="余弦退火"></a>余弦退火</h2><p>在采用<strong>批次随机梯度下降</strong>算法时，神经网络应该越来越接近Loss值的全局最小值。当它逐渐接近这个最小值时，学习率应该变得更小来使得模型不会超调且尽可能接近这一点。</p>
<p>余弦退火（Cosine annealing）利用余弦函数来<strong>降低学习率</strong>，进而解决这个问题，如下图所示：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeUg2pXrUCeK8PZicnVgzYJlCCWyqzZjwzVTtVp7AiatpcntV9aVMmn3DJQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>从上图可以看出，随着x的增加，余弦值首先<strong>缓慢</strong>下降，然后<strong>加速</strong>下降，<strong>再次缓慢</strong>下降。这种下降模式能和学习率配合，以一种十分有效的计算方式来产生很好的效果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.fit(0.1, 1)</span><br><span class="line"># Calling learn fit automatically takes advantage of cosine annealing</span><br></pre></td></tr></table></figure>

<p>我们可以用Fast.ai库中的**learn.fit()**函数，来快速实现这个算法，在整个周期中不断降低学习率，如下图所示：</p>
<p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/image-20220516001424367.png" alt="image-20220516001424367"></p>
<p>同时，在这种方法基础上，我们可以进一步引入重启机制。</p>
<h2 id="带重启的SGD算法"><a href="#带重启的SGD算法" class="headerlink" title="带重启的SGD算法"></a><strong>带重启的SGD算法</strong></h2><p>在训练时，<strong>梯度下降</strong>算法可能陷入局部最小值，而不是全局最小值。</p>
<p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/image-20220516001453664.png" alt="image-20220516001453664"></p>
<p>梯度下降算法可以通过<strong>突然提高学习率</strong>，来“跳出”局部最小值并找到通向全局最小值的路径。这种方式称为<strong>带重启的</strong>随机梯度下降方法（stochastic gradient descent with restarts, <strong>SGDR</strong>），这个方法在Loshchilov和Hutter的ICLR论文中展示出了很好的效果。</p>
<p>这篇论文是：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts</a></p>
<p>用Fast.ai库可以快速导入SGDR算法。当调用learn.fit(learning_rate, epochs)函数时，学习率在每个周期开始时重置为参数输入时的初始值，然后像上面余弦退火部分描述的那样，逐渐减小。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwb0qia3sxLcBUOzDpAlKfJUILfVLRM5WT49z9BL9TCGIDERcq81suEIoKzIESw13PicwJBuw8yEDA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>每当学习率下降到最小点，在上图中为每100次迭代，我们称为一个循环。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cycle_len = 1</span><br><span class="line"># decide how many epochs it takes for the learning rate to fall to</span><br><span class="line"># its minimum point. In this case, 1 epoch</span><br><span class="line"></span><br><span class="line">cycle_mult=2</span><br><span class="line"># at the end of each cycle, multiply the cycle_len value by 2</span><br><span class="line"></span><br><span class="line">learn.fit(0.1, 3, cycle_len=2, cycle_mult=2)</span><br><span class="line"># in this case there will be three restarts. The first time with</span><br><span class="line"># cycle_len of 1, so it will take 1 epoch to complete the cycle.</span><br><span class="line"># cycle_mult=2 so the next cycle with have a length of two epochs, </span><br><span class="line"># and the next four.</span><br></pre></td></tr></table></figure>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwb0qia3sxLcBUOzDpAlKfJUq4vBsSEOdO02VtjsSVQD1FJjHrgpL0BLCOwiavCIxEBib3ZD3l1czlg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>利用这些参数，和使用差分学习率，这些技巧是Fast.ai用户在图像分类问题上取得良好效果的关键。</p>
<p>Fast.ai论坛有个帖子专门讨论Cycle_mult和cycle_len函数，地址在这里：<br><a target="_blank" rel="noopener" href="http://forums.fast.ai/t/understanding-cycle-len-and-cycle-mult/9413/8">http://forums.fast.ai/t/understanding-cycle-len-and-cycle-mult/9413/8</a></p>
<p>更多关于学习率的详细内容可参考这个Fast.ai课程：<br><a target="_blank" rel="noopener" href="http://course.fast.ai/lessons/lesson2.html">http://course.fast.ai/lessons/lesson2.html</a></p>
<h2 id="人格化你的激活函数"><a href="#人格化你的激活函数" class="headerlink" title="人格化你的激活函数"></a>人格化你的激活函数</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/US10Gcd0tQEfcffueY0reDaT8agHibMbkl6VPJicIaSLBOMT46hKst5wjTztibed2dJsrke6B0nRpRPvJXnC2mlSg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Softmax只喜欢选择一样东西；</p>
<p>Sigmoid想知道你在[-1, 1]区间上的位置，并不关心你超出这些值后的增加量；</p>
<p>Relu是一名俱乐部保镖，要将负数拒之门外。</p>
<p>……</p>
<p>以这种思路对待激活函数，看起来很愚蠢，但是<strong>安排一个角色</strong>后能确保把他们用到正确任务中。</p>
<p>正如fast.ai创始人Jeremy Howard指出，不少学术论文中也把Softmax函数用在多分类问题中。在DL学习过程中，我也看到它在论文和博客中多次使用不当。</p>
<h2 id="迁移学习在NLP问题中非常有效"><a href="#迁移学习在NLP问题中非常有效" class="headerlink" title="迁移学习在NLP问题中非常有效"></a>迁移学习在NLP问题中非常有效</h2><p>正如预训练好的模型在计算机视觉任务中很有效一样，已有研究表明，自然语言处理（NLP）模型也可以从这种方法中受益。</p>
<p>在Fast.ai第4课中，Jeremy Howard用迁移学习方法建立了一个模型，来判断IMDB上的电影评论是积极的还是消极的。</p>
<p>这种方法的效果立竿见影，他所达到的准确率超过了Salesforce论文中展示的所有先前模型：<br><a target="_blank" rel="noopener" href="https://einstein.ai/research/learned-in-translation-contextualized-word-vectors%E3%80%82">https://einstein.ai/research/learned-in-translation-contextualized-word-vectors。</a></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeUqd0vkXuXh1PBblfpcBuxQGLiaTzZULdWoicRsdL95G0ElA2icicJUkUjjw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>这个模型的关键在于先训练模型来获得对语言的一些理解，然后再使用这种预训练好的模型作为新模型的一部分来分析情绪。</p>
<p>为了创建第一个模型，我们训练了一个循环神经网络（RNN）来预测文本序列中的下个单词，这称为<strong>语言建模</strong>。当训练后网络的准确率达到一定值，它对每个单词的编码模式就会传递给用于情感分析的新模型。</p>
<p>在上面的例子中，我们看到这个语言模型与另一个模型集成后用于情感分析，但是这种方法可以应用到<strong>其他任何NLP任务</strong>中，包括<strong>翻译</strong>和<strong>数据提取</strong>。</p>
<p>而且，计算机视觉中的一些技巧，也同样适用于此，如上面提到的冻结网络层和使用差分学习率，在这里也能取得更好的效果。</p>
<p>这种方法在NLP任务上的使用涉及很多细节，这里就不贴出代码了，可访问相应课程和代码。</p>
<p>课程：<br><a target="_blank" rel="noopener" href="http://course.fast.ai/lessons/lesson4.html">http://course.fast.ai/lessons/lesson4.html</a></p>
<p>代码：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb">https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb</a></p>
<h2 id="深度学习在处理结构化数据上的优势"><a href="#深度学习在处理结构化数据上的优势" class="headerlink" title="深度学习在处理结构化数据上的优势"></a><strong>深度学习在处理结构化数据上的优势</strong></h2><p>Fast.ai课程中展示了深度学习在处理结构化数据上的突出表现，且无需借助特征工程以及领域内的特定知识。</p>
<p>这个库充分利用了PyTorch中<strong>embedding</strong>函数，允许将<strong>分类变量</strong>快速转换为嵌入矩阵。</p>
<p>他们展示出的技术比较简单直接，只需将分类变量转换为数字，然后为每个值分配嵌入向量：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeUwFf14RWWbCZNhDT2FhmVr7nRod3Ka9sc9ZeFkA1szKG3lxFX6pViaGQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>在这类任务上，传统做法是创建虚拟变量，即进行一次热编码。与之相比，这种方式的优点是用四个数值代替一个数值来描述每一天，因此可获得更高的数据维度和更丰富的关系。</p>
<p>这种方法在Rossman Kaggle比赛中获得第三名，惜败于两位利用专业知识来创建许多额外特征的领域专家。</p>
<p>相关课程：<br><a target="_blank" rel="noopener" href="http://course.fast.ai/lessons/lesson4.html">http://course.fast.ai/lessons/lesson4.html</a></p>
<p>代码：<br><a target="_blank" rel="noopener" href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb">https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb</a></p>
<p>这种用深度学习来减少对特征工程依赖的思路，也被Pinterest证实过。他也提到过，他们正努力通过深度学习模型，期望用更少的工作量来获得更好的效果。</p>
<h2 id="更多内置函数：Dropout层、尺寸设置、TTA"><a href="#更多内置函数：Dropout层、尺寸设置、TTA" class="headerlink" title="更多内置函数：Dropout层、尺寸设置、TTA"></a>更多内置函数：Dropout层、尺寸设置、TTA</h2><p>4月30日，Fast.ai团队在斯坦福大学举办的DAWNBench竞赛中，赢得了基于Imagenet和CIFAR10的分类任务。在Jeremy的夺冠总结中，他将这次成功归功于fast.ai库中的一些额外函数。</p>
<p>其中之一是<strong>Dropout</strong>层，由Geoffrey Hinton两年前在一篇开创性的论文中提出。它最初很受欢迎，但在最近的计算机视觉论文中似乎有所忽略。这篇论文是：</p>
<p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting：<br><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
<p>然而，PyTorch库使它的实现变得很简单，用Fast.ai库加载它就更容易了。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/CuZzSF5jYZzSNKCk1wt8DCX3VzNhbVeU8b2qlibpx7aBoN1qefqtfKaBicPDr8yCJibHnnW8hgtBbOLXicJHhG5l6w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Dropout函数能<strong>减弱过拟合</strong>效应，因此要在CIFAR-10这样一个相对较小的数据集上取胜，这点很重要。在创建learn对象时，Fast.ai库会自动加入dropout函数，同时可使用ps变量来修改参数，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learn = ConvLearner.pretrained(model, data, ps=0.5, precompute=True)</span><br><span class="line"># creates a dropout of 0.5 (i.e. half the activations) on test dataset. </span><br><span class="line"># This is automatically turned off for the validation set</span><br></pre></td></tr></table></figure>

<p>有一种很简单有效的方法，经常用来处理过拟合效应和提高准确性，它就是<strong>训练小尺寸图像</strong>，然后<strong>增大尺寸</strong>并<strong>再次训练</strong>相同模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># create a data object with images of sz * sz pixels </span><br><span class="line">def get_data(sz): </span><br><span class="line">    tmfs = tfms_from_model(model, sz)</span><br><span class="line">    # tells what size images should be, additional transformations such</span><br><span class="line">    # image flips and zooms can easily be added here too</span><br><span class="line"></span><br><span class="line">    data = ImageClassifierData.from_paths(PATH, tfms=tfms)</span><br><span class="line">    # creates fastai data object of create size</span><br><span class="line"></span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line">learn.set_data(get_data(299))</span><br><span class="line"># changes the data in the learn object to be images of size 299</span><br><span class="line"># without changing the model.</span><br><span class="line"></span><br><span class="line">learn.fit(0.1, 3)</span><br><span class="line"># train for a few epochs on larger versions of images, avoiding overfitting</span><br></pre></td></tr></table></figure>

<p>还有一种先进技巧，可将准确率提高若干个百分点，它就是<strong>测试时增强</strong>（test time augmentation, <strong>TTA</strong>）。这里会为原始图像造出多个不同版本，包括不同区域裁剪和更改缩放程度等，并将它们输入到模型中；然后对多个版本进行计算得到平均输出，作为图像的最终输出分数，可调用learn.TTA()来使用该算法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds, target = learn.TTA()</span><br></pre></td></tr></table></figure>

<p>这种技术很有效，因为原始图像显示的区域可能会缺少一些重要特征，在模型中输入图像的多个版本并取平均值，能解决上述问题。</p>
<h2 id="创新力很关键"><a href="#创新力很关键" class="headerlink" title="创新力很关键"></a>创新力很关键</h2><p>在DAWNBench比赛中，Fast.ai团队提出的模型不仅速度最快，而且计算成本低。要明白，要构建成功的DL应用，不只是一个利用大量GPU资源的计算任务，而应该是一个需要创造力、直觉和创新力的问题。</p>
<p>本文中讨论的一些突破，包括Dropout层、余弦退火和带重启的SGD方法等，实际上是研究者针对一些问题想到的不同解决方式。与简单地增大训练数据集相比，能<strong>更好地提升准确率</strong>。</p>
<p>硅谷的很多大公司有大量GPU资源，但是，不要认为他们的先进效果遥不可及，你也能靠创新力提出一些新思路，来挑战效果排行榜。</p>
<p>事实上，有时计算力的局限也是一种机会，因为需求是创新的动力源泉。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://blog.floydhub.com/ten-techniques-from-fast-ai/">https://blog.floydhub.com/ten-techniques-from-fast-ai/</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Haowei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://howiehsu0126.github.io/2022/05/16/称霸Kaggle的九大深度学习炼丹技巧/">http://howiehsu0126.github.io/2022/05/16/称霸Kaggle的九大深度学习炼丹技巧/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://howiehsu0126.github.io">Haowei-Hub</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">科研 - 深度学习</a><a class="post-meta__tags" href="/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/">公众号</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2022/05/06/Deep-Learning-Papers-Reading-Roadmap/"><span>Deep Learning Papers Reading Roadmap</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="lv-container" data-id="city" data-uid="MTAyMC81NDgxNS8zMTI4NQ"><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By Haowei</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">AI | Math | English & French | Computer Science | Life</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>