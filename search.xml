<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>我整理了三百多篇论文,得出了写文献综述的这些经验</title>
      <link href="/2021/12/06/%E6%88%91%E6%95%B4%E7%90%86%E4%BA%86%E4%B8%89%E7%99%BE%E5%A4%9A%E7%AF%87%E8%AE%BA%E6%96%87,%E5%BE%97%E5%87%BA%E4%BA%86%E5%86%99%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0%E7%9A%84%E8%BF%99%E4%BA%9B%E7%BB%8F%E9%AA%8C/"/>
      <url>/2021/12/06/%E6%88%91%E6%95%B4%E7%90%86%E4%BA%86%E4%B8%89%E7%99%BE%E5%A4%9A%E7%AF%87%E8%AE%BA%E6%96%87,%E5%BE%97%E5%87%BA%E4%BA%86%E5%86%99%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0%E7%9A%84%E8%BF%99%E4%BA%9B%E7%BB%8F%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p>众所周知，论文是读研的一大难关，在论文之中，<strong>文献综述又是小白研究生们的一大难题</strong>。俗话说，“看猪跑不如吃<a href="https://www.zhihu.com/search?q=%E7%8C%AA%E8%82%89&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">猪肉</a>”，当然这不是一句俗话，但是<a href="https://www.zhihu.com/search?q=%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">文献综述</a></p><p>这种东西，即使我看过一些不同的经验贴，到自己真正去下笔的时候，才发现一切都不像自己想得那么简单。</p><p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-cae27cb2902da7f19f488da7015a9e5b_b.jpg" alt="img"></p><p>上学期有一门课，老师专门提出期末作业是每人一篇文献综述，题材自选，于是从这门课程开设一半的时候，我就开始着手准备，在这个过程中，我自己总结出了一些小技巧和小问题。</p><h2 id="文献综述的目的"><a href="#文献综述的目的" class="headerlink" title="文献综述的目的"></a><strong>文献综述的目的</strong></h2><p>最终的硕士毕业论文里，文献综述的部分是不可或缺的，而让我们这些“研究者”写文献综述的目的在于总结和整理前人的关于某一个问题的研究成果，然后说明自己的这篇论文的研究在这个基础上有什么创新或者意义，又或者说是另外的一种研究方法的更新。</p><p>简言之就是，我们的论文是为了研究某个小方向的问题，文献综述就是要告诉老师们我们的研究别人不一样。</p><p>当然这是最终毕业论文文献综述的目的，对于平时的课程作业，我们所要使用的方法又有一些不同：本次课程作业呢，由于我们没有新的研究成果，因而老师让我们选择某一个比较大的问题，再选择一个时间段，把这个具体阶段中的所有研究成果都总结一下。于是我选择的是近五年有关于小说家<a href="https://www.zhihu.com/search?q=%E8%8B%8F%E7%AB%A5&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">苏童</a></p><p>的研究，对此进行整理。</p><p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-88b6224c7cd78ca9ef36fa8c8e5cec87_1440w.jpg" alt="img"></p><p>而在最终的毕业论文写作中，实际上我们论文写作的方向是很细致的，比如“苏童某个作品中的女性研究”，因此在文献综述中，我们需要把从过去到现在所有关于这个小问题的研究成果进行罗列，在此基础上得出自己创新的结论。</p><p>当时老师给出一个模板：**<a href="https://www.zhihu.com/search?q=%E7%BB%AA%E8%AE%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">绪论</a>**</p><p><strong>/研究缘起/研究意义/研究现状/研究内容/选题内容/概念界定/研究思路/研究方法/创新之处</strong></p><p>“研究缘起/研究意义/研究现状”基本上就是文献综述需要包含的内容，当然以上模板这是一个参考，具体去读不同的毕业论文时，会发现每个人使用的部分会有所不同，但这顺着这个思路，就可以大致知道我们写文献综述的目的是什么了。</p><h2 id="如何着手"><a href="#如何着手" class="headerlink" title="如何着手"></a><strong>如何着手</strong></h2><p>这里可以说一个<strong>我关于文科论文写作的小窍门</strong>，当然可能很多人多如此，就是在不知道怎么写的时候，先去广泛地看论文。就从<a href="https://www.zhihu.com/search?q=%E7%9F%A5%E7%BD%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">知网</a></p><p>开始，把自己心里想要写的这个题材下载量最高十几篇或者几十篇的论文都看一遍，然后再把最新的成果论文都看一遍，其实在这之中会发现，往往越知名的学者写的论文，可能套路和章法都不同，那这就不是为了看模板，而是看内核、看方法。从而找到最新的一些论文，就可以从中总结出一些模板出来。</p><p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-22285574d69c3cd6a4fa88e9be675f4b_1440w.jpg" alt="img"></p><p>例如，最简单的，我们去知网直接搜索：某某文献综述，其实是会看到前些年的研究者是如何对过去五年、十年或者更久的成果进行总结的，然后把他们的模板进行总结。比如一般会分几个部分，每个部分他们又是从什么角度去分类的。</p><p>比如，有关于苏童研究的文献综述，<strong>一般都是从内容、语言、技巧、其他</strong>（包括翻译和改编）这些方面归纳的，但是每一个具体的部分，每篇文献综述的侧重点都不太一样。有的是选择把内容以不同的题材进行分类，有的是把内容背后的思潮进行总结，有的是将内容用不同的关键词以及内在的联系进行划分，看完这些，就会斟酌，在自己的这篇文献综述中，想用什么样的思路和方法，有了某个侧重的角度，再去下笔就更容易一些。</p><h2 id="整理材料"><a href="#整理材料" class="headerlink" title="整理材料"></a><strong>整理材料</strong></h2><p>既然是要总结和归纳前任的成果，那么就必须要<strong>广泛地阅读最近这些年的研究论文</strong>。</p><p>这里<strong>有一个技巧</strong>是，我们把之前找到的文献综述进行一个整理，就可以大致看出再更早的那些年关于此项研究的成果都有哪些，那么自己在整理最近几年的成果时，就有了可比较项。</p><p>因为我选定的是最近五年，所以把在知网、读书馆、读秀以及杂志报刊等<strong>资料的搜索范围都限定在最近的五年</strong>，然后就是必不可少的一步：踏踏实实地看。</p><p>这个过程听起来好像是挺吓人的，因为却是有好几百篇，但是<strong>最开始看的时候，从摘要看起就可以了</strong>。看完摘要，大概就对这篇文章的研究内容和方法有个把握，接着就需要拿个小本本，来做总结。</p><p>把研究同一个问题的归一类，把研究方法类似的归一类，尽量把标题和作者都列下来，方便写参考文献的时候使用。这个过程就比较漫长了，前后断断续续花了我两周的时间，不过每天只整理一点，因为论文看多了脑子就糊住了，所以做任何作业都是要“拉长战线”慢慢来做，避免<a href="https://www.zhihu.com/search?q=%E8%84%B1%E5%8F%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">脱发</a></p><p>和焦躁。</p><h2 id="开始动笔"><a href="#开始动笔" class="headerlink" title="开始动笔"></a><strong>开始动笔</strong></h2><p>做好了上面几步的准备工作，其实就已经完成了三分之二，最后一步梳理成文其实相比于之前已经很简单了。选定一个自己写作思路，先把大纲和框架列出来，如果是对之前的研究有了突破和创新的地方，要着重罗列出来，选择具体的两到三篇即可，如果某个部分没有这种超越，也要表明自己的看法。</p><p>在这个部分需要注意一点，我当初写的时候，犯了一个<strong>“太过于全面”的错误</strong>，后来被老师纠正。就是说我们在总结成果的时候，重复的成果不需要罗列太详细，新出来的但是站不住脚的成果，只需要列出一两个，也不必都拿出来细说，我就是不知道什么该写什么不该写，所以初稿写了一万二，后来被老师指点说，其实详略得当是很重要的。</p><p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-daad9a9933d38d8941841fa15bf2d04c_1440w.jpg" alt="img"></p><p>例如同样对一个问题的研究，只需要选择一两个最全面、最权威的论文，其他的只要写出一些要点和创新点，对于被当成例子的论文，一定要好好阅读，必要的时候做一些引用，这样自己才能做到对自己的文献综述的整个结构“心中有数”。</p><p>同时，在分类上，我当时出现了一个问题，就是不同类别概念模糊。因为我混淆了两种不同的分类方法。</p><p>比如我用A方法来做分类时可以分为1、2、3，用B方法分类时可以分为4、5、6，这应该是两篇不同的论文思路，但是我在初稿的时候，把分类做成了1、2、4、5，因为我认为内容有交叉，然而最后的结果是很混乱的，所以，在做成果分类时，切忌试图去囊括一切，只要从某个角度切入即可，不必要也不可能十分全面。</p><p>在文章的结构上，基本遵循上述三点：研究缘起/研究意义/研究现状。后两者顺序可以调换，同时老师也会强调，单独做一篇文献综述的作业或者课堂展示的时候，要有自己的见解，毕竟已经看了那么多篇论文，无论如何心中也能够对这个问题有所认识。同时，这个时候也可以趁机会填补一些自己关于这部分知识中的漏洞，论文中提到的其他成果、作品等等，自己可以在写作的这个过程里扩充知识量，这样看来，平时学习写文献总是是非常有意义的。</p><h2 id="做好课堂展示"><a href="#做好课堂展示" class="headerlink" title="做好课堂展示"></a><strong>做好课堂展示</strong></h2><p>一般来说，如果以课堂展示的形式呈现一篇文献综述的话，需要一个ppt。在课件上和论文里，文献综述的形式也是很不同的。</p><p>ppt里只需要摘要、大纲和具体的例子，剩下的内容可以自己结合论文来进行讲述。课件仅仅是辅助，为了让不够了解的人能够一目了然地知道自己所讲解的内容即可。</p><p>研究和学习的过程，就是一个不断积累、大量阅读，不断出错又勤于总结、改正的过程，无论是什么经验和技巧，最后还是要落实到个人的努力和踏实地<a href="https://www.zhihu.com/search?q=%E7%A7%AF%E7%B4%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:83425474%7D">积累</a></p><p>上，这很难，其实也很容易，熟能生巧。厚积薄发，永远是没错的。</p><blockquote><p>本文来源：募格课堂（ID:mugeketang)。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 论文撰写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你写论文时发现了哪些非常神的网站</title>
      <link href="/2021/12/05/%E4%BD%A0%E5%86%99%E8%AE%BA%E6%96%87%E6%97%B6%E5%8F%91%E7%8E%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E9%9D%9E%E5%B8%B8%E7%A5%9E%E7%9A%84%E7%BD%91%E7%AB%99/"/>
      <url>/2021/12/05/%E4%BD%A0%E5%86%99%E8%AE%BA%E6%96%87%E6%97%B6%E5%8F%91%E7%8E%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E9%9D%9E%E5%B8%B8%E7%A5%9E%E7%9A%84%E7%BD%91%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h1 id="你写论文时发现了哪些非常神的网站？"><a href="#你写论文时发现了哪些非常神的网站？" class="headerlink" title="你写论文时发现了哪些非常神的网站？"></a>你写论文时发现了哪些非常神的网站？</h1><p>神网站算不上，都是一些为写作带来便利的网站。而且作为一些最常用网站的替代品，这些网站有的更精确，有的更便捷。以下内容包括：</p><ol><li><p>选择正确的英文词语：语料库<a href="https://link.zhihu.com/?target=http://linggle.com/">Linggle </a>和 <a href="https://link.zhihu.com/?target=http://www.netspeak.org/">Netspeak</a></p></li><li><p>方便地查询杂志影响力和本领域有哪些杂志：<a href="https://link.zhihu.com/?target=http://www.scimagojr.com/index.php">Scimago Journal &amp; Country Rank</a></p></li><li><p>获取全文利器：<a href="https://link.zhihu.com/?target=http://sci-hub.cc/">Sci-Hub: removing barriers in the way of science</a></p></li><li><p>更精确定位的学术搜索引擎（目前只有计算机类）：<a href="https://link.zhihu.com/?target=https://www.semanticscholar.org/">Semantic Scholar</a></p></li><li><p>提升LaTeX效率的小工具：<a href="https://link.zhihu.com/?target=http://detexify.kirelabs.org/classify.html">Detexify LaTeX handwritten symbol recognition</a></p></li></ol><hr><blockquote><p>作者：lumosxx<br>链接：<a href="https://www.zhihu.com/question/35931336/answer/2189901735">https://www.zhihu.com/question/35931336/answer/2189901735</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><ol><li>Turnitin：英国最常用的查重软件，也是很多高校指定工具。英国的学术论文的禁忌就是抄袭，一旦论文重复率太高很可能就被叫去学校开听证会了。所以在有重要论文并且学校不给予提前查重时，可以使用这个网站提前查一次。</li><li>Cite this for me：如果你写reference总是需要耗时很久的话，赶快码住这个网站。它提供了很多英语论文的常用格式，如Harvard.有时候可能和你学校的要求有些许偏差，大家手动调整一下就行了。</li><li>hubspot blog ideas generator：在网站输入任何你可以想到的愿意写的关键词，这些词可以完全没有联系（最多五个）。他们会给你推荐一些论文标题，不过都是比较适合blog的标题。这些标题一般都不太正式，需要大家自行改动一下，也是提供一个思路。</li><li>Grammarly：语法纠错网站，其实语法错误真的是学术论文一个特别大的扣分点（如：用到非写作词汇，单三，复数，时态等等）。大家在提交论文前，可以先导入这个网站检查一下。</li><li>Academia：你可以作为获取文献的网站，输入你感兴趣的话题，它会定期给你推送相关文献。另外，这个网站也被称为大型学术社交平台，相当于学术界的linkin,据说很多大牛都有注册。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 论文撰写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我经常觉得很焦虑，怎么活在当下呢？</title>
      <link href="/2021/12/04/%E6%88%91%E7%BB%8F%E5%B8%B8%E8%A7%89%E5%BE%97%E5%BE%88%E7%84%A6%E8%99%91%EF%BC%8C%E6%80%8E%E4%B9%88%E6%B4%BB%E5%9C%A8%E5%BD%93%E4%B8%8B%E5%91%A2%EF%BC%9F/"/>
      <url>/2021/12/04/%E6%88%91%E7%BB%8F%E5%B8%B8%E8%A7%89%E5%BE%97%E5%BE%88%E7%84%A6%E8%99%91%EF%BC%8C%E6%80%8E%E4%B9%88%E6%B4%BB%E5%9C%A8%E5%BD%93%E4%B8%8B%E5%91%A2%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>以下方法都是出自《<a href="https://www.zhihu.com/search?q=%E5%BA%94%E5%AF%B9%E7%84%A6%E8%99%91%EF%BC%9A%E4%B9%9D%E7%A7%8D%E6%B6%88%E9%99%A4%E7%84%A6%E8%99%91%E3%80%81%E6%81%90%E6%83%A7%E5%92%8C%E5%BF%A7%E8%99%91%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">应对焦虑：九种消除焦虑、恐惧和忧虑的简单方法</a>》这本宝典，由于篇幅受限，我只能介绍其中5种：</p><ol><li><p><a href="https://www.zhihu.com/search?q=%E8%85%B9%E5%BC%8F%E5%91%BC%E5%90%B8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">腹式呼吸</a></p><p>呼吸的方式反映人的紧张程度，也可以加重或者减轻<a href="https://www.zhihu.com/search?q=%E7%84%A6%E8%99%91%E7%97%87%E7%8A%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">焦虑症状</a>。</p><p>当你感到紧张的时候，呼吸变得浅而快，因为这种呼吸只使用到胸腔，所以也叫作胸式呼吸。比胸式呼吸更加浅而快时，呼吸就是急促的，急促的呼吸会引起一些与焦虑相关的身体症状，例如头晕、心悸、出汗、<a href="https://www.zhihu.com/search?q=%E5%88%BA%E7%97%9B%E6%84%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">刺痛感</a>等。而当你非常放松的时候，呼吸是深而慢的，你会发现当自己吸气时，腹部明显隆起，所以这种呼吸叫做腹式呼吸。</p><p>紧张感通常适合胸式呼吸一起出现的，它很难与腹式呼吸共存，所以，通过改变呼吸方式，就能中止<a href="https://www.zhihu.com/search?q=%E6%81%B6%E6%80%A7%E5%BE%AA%E7%8E%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">恶性循环</a>，从而起到一定的缓解焦虑作用。</p><p>而方法也简单，当你焦虑的时候，做几分钟的腹式呼吸，大口地吸气（4秒），屏住气息（7秒），然后深深地吐气（8秒），如此反复就好。</p></li><li><p><a href="https://www.zhihu.com/search?q=%E6%AD%A3%E5%BF%B5%E5%86%A5%E6%83%B3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">正念冥想</a></p><p>正念冥想已被证实可有效改善多种情绪问题，如焦虑、抑郁，还能提升人的专注力。</p><p><a href="https://www.zhihu.com/search?q=%E6%AD%A3%E5%BF%B5&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">正念</a>是采取一种接纳的态度，将负面情绪看作是自然的潮起潮落、云舒云卷，不去对抗和挣扎，使之减轻对自己的影响。</p><p>它的做法非常简单，形式非常类似于<a href="https://www.zhihu.com/search?q=%E5%9D%90%E7%A6%85&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">坐禅</a>，但不要求背诵佛经、也不需要敲<a href="https://www.zhihu.com/search?q=%E6%9C%A8%E9%B1%BC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">木鱼</a>，你只需要把自己的注意力集中在任何一个对象上，最常见的是呼吸和身体感觉。然后观察和感受它们原来的样子，而不必做出任何评判，在这个过程中，你必定会冒出一些其它的念头，例如关于身体感觉的、关于最近焦虑的事情、或者其它的任意念头，都不必担心，简单地把注意力拉回到对象上就好。</p><p>一次训练的时间通常在15~20分钟。它可以帮助你不再重视那些困扰你的想法，而是把它们轻轻地放在一边，继续从而那些真正重要的事情。当然，正念是需要坚持训练才能看到效果的。</p></li><li><p>运动</p><p>在所有自我调节的方法当中，运动对于焦虑情绪的改善效果是最快速和明显的，甚至可以与药物相媲美，且无任何副作用。</p><p>只要是强度达到一定水平的有氧运动，都可以减轻焦虑。常见的是慢跑、快走、游泳、爬山、骑自行车、挥拍运动等，其中最为推荐的是挥拍类运动，因为人际交往和娱乐活动，对焦虑情绪同样也具有一定的减缓作用。对此每个人的爱好都不同，当然，有些人习惯蹦迪也可以。</p><p>运动的强度以中等强度为宜，可以把<a href="https://www.zhihu.com/search?q=%E8%BD%BB%E5%BA%A6%E5%87%BA%E6%B1%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">轻度出汗</a>作为一个衡量标准。在频率上，每周三到五次的运动，每次持续30分钟，就能达到不错的效果了。</p></li><li><p>改变认知</p><p>这种方法的基本假设是：人的负面情绪是由负面认知导致的，负面认知本身是一种不合理的、有缺陷的认知，用一种更为理性、现实的想法来取代它，就能改变负面情绪，以及由此而产生的非适应性行为。</p><p>例如：</p><p>事件A：情人节当天，狗子收到我的礼物之后，只看了一眼就放在旁边了。</p><p>认知B：他一定是觉得我送得不好。他对我根本就不重视。你知道我挑得多么用心吗？</p><p>情绪C：不安全感、担忧、受伤、<a href="https://www.zhihu.com/search?q=%E7%94%9F%E6%B0%94&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">生气</a>。</p><p>行为D：整个约会心不在焉、煞有介事，或生闷气，搞得双方闷闷不乐。</p><p>理性回归E：他可能是想回到家再慢慢拆开看。比起礼物，他可能更重视接下来的约会，接下来我们会有浪漫的时刻。对待这份礼物的态度，也不能代表对待感情本身的态度。</p><p>整个环节的关键在于B，要识别出扭曲认知，并且找出其中的不合理之处，并且修改它，常见的扭曲认知有：</p><p>① 过滤</p><p>只关注负面信息，忽略所有积极的方面。如: 别人评论你的表现，前面说了一大堆优点，最后提出一点点小建议，结果回家后就只对那个“小建议”耿耿于怀。理性回归：找到别人说的那些优点。</p><p>② 极化思维</p><p>事情非黑即白、非好即坏，如果不是十全十美就是一无是处。如: 要是我不能一举拿下奥运金牌，就枉费了这么多年的苦练，没有人会看得起第二名。理性回归：第一名也是秒杀74亿人，第二名也是秒杀74亿人，有啥区别。</p><p>③ 过度泛化</p><p>仅根据一个证据或者单一事件就得出一般性结论。夸大问题的频发性，使用负面综合标签。如: 初恋遇到了渣男，从此认为男人都是大猪蹄子。理性回归：好男人还是很多的，我有过经历，辨别<a href="https://www.zhihu.com/search?q=%E5%A4%A7%E7%8C%AA%E8%B9%84%E5%AD%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">大猪蹄子</a>的能力应该比别人强。</p></li><li><p>呵护自己</p><p>① 不当<a href="https://www.zhihu.com/search?q=%E5%B7%A5%E4%BD%9C%E7%8B%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2254634665%7D">工作狂</a></p><p>许多人之所以焦虑，就是因为给自己的压力太大了，别人是“鸡娃”，他们是“鸡自己”，这些人往往有工作成瘾的倾向。所谓工作成瘾，是指非常不健康地过度投入到工作中，忽略了自己的身体需求、人际交往和情感需要。所以，在各大企业放开996的浪潮下，不妨也给自己减轻一些压力，腾出一些时间去休息、娱乐、与他人相处等。</p><p>② 拒绝负面信息</p><p>网络上充斥着许多负面的消息，谁谁又出轨了，老板又PUA员工了，妙龄女子又被骗了，某国又秘密研制武器了，30岁买不起房人生就失败了等等。这些负面消息，大多是不客观的，有严重的夸大成分，因为这样更利于传播，就算是客观的，至少也是经过挑选后的事实，而现实生活中，负面的事件并没有网上展现的那么多。所以，容易焦虑的人，应该尽量少接触网上的这些信息，活在当下，感受生活中的平凡、小确幸，以及偶尔的沮丧。</p></li></ol><p>以上，希望能帮到你。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 心理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研究生新生要怎么看论文？</title>
      <link href="/2021/12/04/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E8%A6%81%E6%80%8E%E4%B9%88%E7%9C%8B%E8%AE%BA%E6%96%87%EF%BC%9F/"/>
      <url>/2021/12/04/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E8%A6%81%E6%80%8E%E4%B9%88%E7%9C%8B%E8%AE%BA%E6%96%87%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>我是<a href="https://www.zhihu.com/search?q=%E4%B8%B4%E5%BA%8A%E5%8C%BB%E5%AD%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:563333253%7D">临床医学</a>出身，这些阅读论文的技巧是我小老板的小老板传下来的（￣^￣゜），然后我结合自身经验改动了一下，供题主参考。当然了，这是给研究生入门用的小技巧，如果是知乎上那些高中就能发CNS的大牛请自动跳过。</p><p>假如导师给了你一个课题的大致方向却没有告诉你具体该怎么办（相信这样的导师不在少数），我的经验是分三步走：</p><p><strong>第一步，这是个啥玩意？</strong>这一步最简单，推荐找1到2篇该方向的综述，哪怕中文的都行，你得先搞懂你要研究的东西到底是个什么鬼。否则接下来看文献都是一头雾水。</p><p><strong>第二步，别人都做了啥？</strong>这一步最耗时间。以你导师给的大方向为关键词搜索论文（最好是英文），把搜出来的近十年的所有论文的<strong>摘要</strong>全部看一遍（友情提示：如果英文不过关，把摘要输入翻译软件机翻都没问题）。当然，看到后来会发现很多论文大同小异，可以不用看摘要了，但起码要把<strong>标题</strong>全部看一遍。<strong>一边看一边做笔记，把每篇研究的精髓用一句话凝练出来，否则很快会忘记前面看了什么！</strong>当然了，如果遇到类似的文献可以合并同类项。</p><p><strong>第三步，别人是怎么做的/我该怎么做？</strong>现在题主大概已经有上万篇该领域文献的阅读量了（虽然都是粗读），恭喜你，你已经成为该领域的键盘大牛了！</p><p>回过头去再看一遍你自己整理出来的笔记，你会发现，你已经能够比较深刻的认识到该领域<strong>别人做过什么，热点是什么，还有什么可以创新了</strong>（友情提示：<a href="https://www.zhihu.com/search?q=%E7%A0%94%E7%A9%B6%E7%94%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:563333253%7D">研究生</a>嘛，只要稍微创新一点点、和别人有一点点不同就够了。研究生想发大文章的请略过……）。这时候你脑子里应该可以有个大致的规划，明白自己具体的研究方向是什么。</p><p>接下来，你要去找导师了。姜还是老的辣，你要让导师帮你把研究方向把把关，省的走歪路！</p><p>最后，你只需要找出几十篇和你研究方向类似的或者对你研究有帮助的论文（越新、影响因子越高越好），把全文搞到手开始精读，搞明白别人是怎么做的，自己接下来应该怎么做。</p><hr><p>我之前带过的研究生很多都存在读文献的严重误区：<strong>从头读到尾恨不得嚼透每个单词</strong>，<strong>读完后束之高阁让记忆随风飘散</strong>等等。</p><p><strong>关于问题中的：「粗糙、不精致、细节不准确 」</strong></p><ul><li>一方面可能是因为选择论文这个环节出了问题，没有选择优秀的、精准的论文去阅读</li><li>另一方面可能是对于论文的表达形式有误解，毕竟科研论文不是一个事无巨细把研究环节的每一步都展现的文体</li></ul><p>那到底怎么高效读文献才能汲取到科研养分为我们的学术水平增长提供帮助呢？</p><p><strong>大家不妨思考几个很关键的问题：</strong></p><blockquote><p>**1. 如何判断手头的这篇文献和自己的领域相关值得<a href="https://www.zhihu.com/search?q=%E7%B2%BE%E5%BA%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:553782865%7D">精度</a>**<strong>？</strong></p><p><strong>2. 那些在阅读中遇到的生单词 (学术专业词汇除外) 真的会对通篇的理解形成严重障碍吗？</strong></p><p><strong>3.“读懂”的定义是什么？没有输出反馈的文献阅读对我们的科研积累到底有没有价值？</strong></p></blockquote><p>如果你在之前的文献阅读中已经思考过这几个问题，那么恭喜你，你对于文献阅读的目的和意义有了比较深入的思考。<strong>接下来的这篇干货分享中会为大家逐一解答这些疑惑。</strong></p><p><strong>「正文」</strong></p><p><strong>★ 读文献的方法</strong></p><p>文献阅读流程大致可以归纳如下图:</p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-617eda1d90e6d0851cabfb743e2c4c5b_r.jpg" alt="preview" style="zoom: 80%;" /><p><strong>第一步, 初筛论文（Surveying the Article）</strong></p><p><strong>「这一步的目的是帮助你确定这篇文章之于你的研究方向是否值得一读，该怎么读」</strong>。<strong>标题和关键字</strong>当然是最先吸引我们注意力的两个因素。<strong>摘要Abstract</strong>部分也会提供给我们一篇文章的主要研究手段和研究成果。接下来我们需要跳到<strong>结论Conclusion</strong>部分，如果一篇文章的结论部分和你的课题高度相关，那么这篇文章你可以继续读下去，反之从时间和效率的角度考量则需要略过。</p><p><strong>第二步, 阅读文章 （Read the Paper）</strong></p><p><strong>「这一步的目的是帮你确认这篇文章是否值得精读」。</strong>这时很多同学会习惯性地从引言开始，往后逐一仔细研读文章的各个段落。此时最高效的方式是<strong>先扫一遍图表以及他们的标题、图注，</strong>对于文章的数据内容有一个自己的初判，科研小白们可以考察一下自己对于图表的解读是否和后面即将读到的文章内容一致。然后我们<strong>回到引言</strong>部分，去了解研究开展的背景，以及为什么作者要展开这项研究。通过这两部分的阅读可以帮助读者进一步了解该项研究的内容和目标，通过和自身科研课题的对比来确定是到此为止，还是精读剩下的章节。</p><p><strong>第三步, 精读文献（Intensive Reading- dig deeply and get the details）</strong></p><p><strong>「这一步的目的是让你了解文章的细节信息，包括实验具体步骤等，对于自身科研项目的开展设计有重要参考价值」。</strong>精读环节我们需要深度挖掘的是<strong>文章的结果和讨论部分，尤其是实验部分</strong>。通过对细节信息的研读，我们能够了解作者是如何开展实验，获取初始数据，进行数据分析，解读数据内涵等等。一篇优秀的、和自己科研课题高度相关的文献是值得精读的，但<strong>切忌从头精读到结尾</strong>，这样容易让自己迷失在细节中无法高屋建瓴地去把握文章的中心和重点。</p><p><strong>第四步, 做笔记（Take Notes）</strong></p><p><strong>「这一步的目的是帮助读者梳理对于文章的理解，列出对自己有价值核心信息，也方便自己在后面的科研工作中能够通过笔记来快速定位相关文献」。</strong>这里不推荐仅仅在在打印的纸质文献上或者PDF文档里面做笔记，后期阅读文献多起来以后会造成索引困难。可以选择一个自己用的顺手的文献管理软件（<strong>Endnote</strong>、<strong>Mendeley</strong>都行）来完成做笔记的工作，这样后面整理文献、引用文献、复读文献都会节约很多时间。</p><p>很多人对于做笔记到底写什么各执一词，这里我觉得每个人在科研的不同阶段对于文章的关注点可能不尽相同，所以很难一言以蔽之。比如初涉科研的小白，文献阅读能力和论文写作能力比较欠缺，那么可以在笔记中「<strong>用一句话（英文）概括实验、结果、讨论章节中的每一段内容，组成一个阅读笔记</strong>」。这样既可以锻炼英语书写表达能力，也可以逼迫自己「<strong>在理解的基础上进行一定量的输出，这是一个加深理解和记忆的过程</strong>」。</p><p>对于阅读科研文献比较熟练，有一些科研工作经历的人来说，这个笔记的内容可能是文中某个新的实验方法、异于其他研究的实验条件、阅读时自己的新想法等等。精读文献并认真做笔记并不代表读者对于这篇文章的消化过程就此终止，我个人觉得优秀的科研论文、大牛的研究著作依然是常读常新，每位从事科研学习和工作的人在不同的时期都能从中汲取养分。</p><p><strong>★★ 读文献的心态</strong></p><p>提笔谈这个话题当然是写给初涉科研的同学们。很多时候学生向我反映读英文文献的障碍，多以“读不懂、看不懂、看过以后脑子里面什么也没留下”来草草描述困难所在，其实很多时候大家是被不认识的单词“唬”住了。<strong>随着阅读的推进遇到的生单词逐渐增加，畏难情绪也会越来越浓重，这个时候浮躁的心情很容易跑出来支配你的大脑，让你“误以为”自己一无所获</strong>。文献阅读中个别动词和副词的含义不知晓并不会对文章大意的理解造成巨大影响（专业基础词汇和术语除外），我们在读文献时要摆正心态，泛读通读过程中不必刻意去关注这些生单词，<strong>要适应在有生单词的情境下去概括和提炼文章的重点</strong>。查单词的工作可以在精读过程中、甚至是读完以后去进行。</p><p><strong>★★★ 读文献的几个 tips</strong></p><blockquote><p><strong>1.</strong> <strong>参考文献信息多</strong>。每篇文章的参考文献可能包含不少你感兴趣的相关文献，可以从中筛选一下做延伸阅读；</p><p><strong>2.</strong> <strong>关注近五年的文献</strong>。无论是你的开题报告还是期刊投稿文都对于近五年的相关文献比例有一定要求，广泛阅读这部分科研成果对于科研人把握学科发展趋势、确定科研选题来说至关重要；</p><p><strong>3.</strong> <strong>关注核心期刊和学科大牛</strong>。这点不用多说了，学科大牛课题组和核心期刊的相关文献千万不要错过。</p></blockquote><hr><p>这是我在读研究生的时候，跟一个博士大牛师兄学的，当时导师要我做一个方向的research，看了半个月，头皮挠破了也弄不出来，就觉得<strong>文献越看越多，研究方法也越来越多，可是自己还是什么都不懂</strong>。于是导师叹了一口气，把这个工作交给一个博士师兄，师兄用一天把文献分类做好了。。我什么想法？只想跪下抱师兄大腿（没出息的我）。。以下是正文，本文大约1100字，全文干货，阅读时间5min。</p><p>因为工作原因（划掉），需要看很多论文，也在咨询的过程中，被问到怎么去看论文，因此在这说说我的做法，跟大家一起讨论。当我还是个萌新的时候，也犯过这样的错误：<strong>每篇文章从头读到尾，还在文章上做了大量笔记，但其实看过之后就扔到一边，看笔记？不存在的。</strong></p><p>那到底该怎么去阅读文献呢？如何去针对一个课题做research呢？我认为<strong>最高效的方法就是：做表</strong>，这里我们来举个栗子。</p><p>例如，我们针对海南地区的休闲旅游进行分析。那么针对这个问题，我们首先要确定几件事：</p><ol><li>海南地区的休闲旅游<strong>现在是一个什么样的状态</strong>：这个状态可以从几个方面进行阐述：</li></ol><blockquote><p>（旅游的经济收益、旅游的人次、旅游的种类项目、旅游具体的活动安排、与其他地区对比的独特性等等，这个可以在各种文献中去找到针对本部分的描述，明确了这部分有哪些数据了，那么我们要做的就是搜集我们所研究的目标地区近几年的这些数据）。</p></blockquote><p>*<strong>本部分文献的research工作：搜集描述现状的论据，并以此为参考，去寻找自己研究目标的描述方法。*</strong></p><ol start="2"><li><p><strong>现状中反应了什么样的问题</strong>：如果要让我们直接去通过针对自己搜集到的数据去分析问题，一方面难度会比较大（针对萌新），可能你只能看着一堆数据干瞪眼，而不知道这堆数据到底反映了一个什么样的状况；另一方面，可能会有误判，也就是说，例如某个数据，你觉得它已经非常高了，但在实际上来说，这个值并不高，这是不是就尴尬了？<br> <strong>因此本部分的research工作：搜集文献中现状部分所反映的问题，看看这些问题到底是以怎样的论断去论证，给自己的论文打个底。</strong></p></li><li><p><strong>问题的解决方法</strong>论文的主旨一般都在于：描述现状-提出问题-解决问题。因此，最后也都要落到解决问题上来，那么针对问题的解决方法，你一个萌新？确定不聆听一下各位前辈大佬专家师兄师姐的教诲吗？什么？你说你找不到前辈大佬专家师兄师姐的教诲？那拜托去看看他们的论文好吗？</p><p> <strong>因此本部分的research工作：搜集论文文献中针对问题的解决方法，结合自己本身论文的需要，针对这些方法去挑拣、重构、优化。也就是我们所说的，站在“巨人”的肩膀上看世界。</strong></p></li></ol><p>那么我们的表怎么做呢？<img src="https://gitee.com/Howie0126/blogImg/raw/master/img/v2-6af9e41aa5a4882a27332322584799d4_720w.jpg" alt="img"></p><p>由图可见，分为以下几个部分：</p><blockquote><ol><li>序号（下载的文献文件名也要标序号，将表与文献对应起来，在写作过程中方便查找）</li><li>文献名称</li><li>文献类型(期刊、硕论、博论)</li><li>发表年份</li><li>关键部分（用来记录本篇文章你认为关键的地方，标注好内容和页码）</li><li>研究方法（可以看出此类问题，大家都是采用一个什么样的方法去解决的）</li><li>存在问题（如前所述）</li><li>产生原因（有些文献可能没有，没有即不写）</li><li>解决方法（如前所述）</li><li>创新点</li><li>下一步工作/不足之处（别人论文里的不足可能是你idea的来源)</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow 2.7有哪些新变化？</title>
      <link href="/2021/12/02/TensorFlow-2-7%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B0%E5%8F%98%E5%8C%96%EF%BC%9F/"/>
      <url>/2021/12/02/TensorFlow-2-7%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B0%E5%8F%98%E5%8C%96%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>近期重磅上线的 TensorFlow 2.7 通过更加清晰的错误消息、简化的堆栈信息提升了易用性，并为迁移到 TF2 的用户增加了新工具和文档。</p><ul><li><p>TensorFlow 2.7</p><p><a href="https://github.com/tensorflow/tensorflow/releases">https://github.com/tensorflow/tensorflow/releases</a></p></li></ul><p><strong>改善调试体验</strong></p><p>调试代码的过程，是机器学习框架用户体验的一个基本组成部分。在 TensorFlow 2.7 中，我们大幅改善了 TensorFlow  的调试体验，提高了其效率和用户体验，这些改善包括以下三个主要变化：简化堆栈错误信息、在自定义 Keras 层的错误中显示额外的上下文信息，以及对 Keras 和 TensorFlow 中所有错误消息进行广泛审查。</p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/640" alt="图片" style="zoom: 25%;" /><p><strong>简化堆栈错误信息</strong></p><p>TensorFlow 现在默认对出现错误时显示的堆栈信息进行过滤，以隐藏任何来自 TensorFlow 内部代码的报错信息，让信息集中在对您而言比较重要的地方：您自己的代码。如此一来，堆栈信息变得更简单、更简短，让您能够更加轻松地理解和修复代码中的问题。</p><p>如果您实际上是在调试 TensorFlow 代码库本身（例如准备 TensorFlow 的 PR），您可以通过调用 <code>tf.debugging.disable_traceback_filtering()</code> 来关闭过滤机制。</p><p><strong>针对 Keras 层异常的自动上下文注入</strong></p><p>编写低阶代码最常见的用例之一是创建自定义的 Keras 层，所以我们想要尽可能地降低您调试层的难度，提高调试的效率。对层进行调试时，您要做的第一件事就是打印其输入的形状和 dtype，以及其 <code>training</code> 和 <code>mask</code> 参数的值。现在，我们将这些信息自动添加到所有自定义 Keras 层的堆栈信息中。</p><p>在下图中可以看到堆栈信息过滤和调用上下文信息显示的实际效果：</p><img src="https://gitee.com/Howie0126/blogImg/raw/master/img/tf.jpg" alt="1" style="zoom:67%;" /><p>TensorFlow 2.7 中简化的堆栈信息</p><p><strong>审查并改进 TensorFlow 和</strong> </p><p><strong>Keras 代码库中的所有错误消息</strong></p><p>最后，我们审查了 Keras 和 TensorFlow 代码库中的每一条错误消息（数以千计的错误位置！），并对它们进行了改进，以确保其遵循用户体验的最佳实践。一条合格的错误消息需要能够告诉您框架的预期，指出不符合框架预期的操作，并给出修复问题的相应提示。</p><p><strong>改进 tf.function 错误消息</strong></p><p>通过在用户代码中加入指向错误源的回溯，我们改进了两种常见的 <code>tf.function</code> 错误消息：运行时错误消息和“计算图”张量错误消息。对于其他模糊和不准确的 <code>tf.function</code> 错误消息，我们也进行了更新，提高了其清晰度和准确性。</p><p>对于由用户代码引起的运行时错误消息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line"> l = tf.<span class="built_in">range</span>(tf.random.uniform((), minval=<span class="number">1</span>, maxval=<span class="number">10</span>, dtype=tf.int32))</span><br><span class="line"> <span class="keyword">return</span> l[<span class="number">20</span>]</span><br></pre></td></tr></table></figure><p>旧的错误消息摘要如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Python stack trace of the function call …</span></span><br><span class="line"></span><br><span class="line">InvalidArgumentError:  <span class="built_in">slice</span> index <span class="number">20</span> of dimension <span class="number">0</span> out of bounds.</span><br><span class="line">         [[node strided_slice (defined at &lt;<span class="string">&#x27;ipython-input-8-250c76a76c0e&#x27;</span>&gt;:<span class="number">5</span>) ]] [Op:__inference_f_75]</span><br><span class="line"></span><br><span class="line">Errors may have originated <span class="keyword">from</span> an <span class="built_in">input</span> operation.</span><br><span class="line">Input Source operations connected to node strided_slice:</span><br><span class="line"> <span class="built_in">range</span> (defined at<span class="string">&#x27;:4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Function call stack:</span></span><br><span class="line"><span class="string">f</span></span><br></pre></td></tr></table></figure><p>新的错误消息摘要如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Python stack trace of the function call …</span></span><br><span class="line"></span><br><span class="line">InvalidArgumentError:  <span class="built_in">slice</span> index <span class="number">20</span> of dimension <span class="number">0</span> out of bounds.</span><br><span class="line">         [[node strided_slice</span><br><span class="line"> (defined at:<span class="number">5</span>)</span><br><span class="line">]] [Op:__inference_f_15]</span><br><span class="line"></span><br><span class="line">Errors may have originated <span class="keyword">from</span> an <span class="built_in">input</span> operation.</span><br><span class="line">Input Source operations connected to node strided_slice:</span><br><span class="line">In[<span class="number">0</span>] <span class="built_in">range</span> (defined at:<span class="number">4</span>)       </span><br><span class="line">In[<span class="number">1</span>] strided_slice/stack:      </span><br><span class="line">In[<span class="number">2</span>] strided_slice/stack_1:    </span><br><span class="line">In[<span class="number">3</span>] strided_slice/stack_2:</span><br><span class="line"></span><br><span class="line">Operation defined at: (most recent call last)</span><br><span class="line"><span class="comment"># … Stack trace of the error within the function …</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>  File <span class="string">&quot;&quot;</span>, line <span class="number">7</span>, <span class="keyword">in</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>  File <span class="string">&quot;&quot;</span>, line <span class="number">5</span>, <span class="keyword">in</span> f</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">return</span> l[<span class="number">20</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>主要的区别在于：现在执行 tf.function 时引发的运行时错误包含堆栈信息，可以显示错误在用户代码中的来源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Original error message and information …</span></span><br><span class="line"><span class="comment"># … More stack frames …</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>  File <span class="string">&quot;&lt;ipython-input-3-250c76a76c0e&gt;&quot;</span>, line <span class="number">7</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>  File <span class="string">&quot;&lt;ipython-input-3-250c76a76c0e&gt;&quot;</span>, line <span class="number">5</span>, <span class="keyword">in</span> f</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">return</span> l[<span class="number">20</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>对于由以下用户代码引起的“计算图”张量错误消息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_function</span>(<span class="params">a</span>):</span></span><br><span class="line"> <span class="keyword">global</span> x</span><br><span class="line"> x = a + <span class="number">1</span><span class="comment"># Bad - leaks local tensor</span></span><br><span class="line"> <span class="keyword">return</span> a + <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">captures_leaked_tensor</span>(<span class="params">b</span>):</span></span><br><span class="line"> b += x</span><br><span class="line"> <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">leaky_function(tf.constant(<span class="number">1</span>))</span><br><span class="line">captures_leaked_tensor(tf.constant(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>旧的错误消息摘要如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Python stack trace of the function call …</span></span><br><span class="line"></span><br><span class="line">TypeError: An op outside of the function building code <span class="keyword">is</span> being passed</span><br><span class="line">a <span class="string">&quot;Graph&quot;</span> tensor. It <span class="keyword">is</span> possible to have Graph tensors</span><br><span class="line">leak out of the function building context by including a</span><br><span class="line">tf.init_scope <span class="keyword">in</span> your function building code.</span><br><span class="line">For example, the following function will fail:</span><br><span class="line"><span class="meta">  @tf.function</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">has_init_scope</span>():</span></span><br><span class="line">    my_constant = tf.constant(<span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.init_scope():</span><br><span class="line">      added = my_constant * <span class="number">2</span></span><br><span class="line">The graph tensor has name: add:<span class="number">0</span></span><br></pre></td></tr></table></figure><p>新的错误消息摘要如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Python stack trace of the function call …</span></span><br><span class="line"></span><br><span class="line">TypeError: Originated <span class="keyword">from</span> a graph execution error.</span><br><span class="line"></span><br><span class="line">The graph execution error <span class="keyword">is</span> detected at a node built at (most recent call last):</span><br><span class="line"><span class="comment"># … Stack trace of the error within the function …</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> File, line <span class="number">6</span>, <span class="keyword">in</span> leaky_function</span><br><span class="line"><span class="comment"># … More stack trace of the error within the function …</span></span><br><span class="line"></span><br><span class="line">Error detected <span class="keyword">in</span> node <span class="string">&#x27;add&#x27;</span> defined at: File <span class="string">&quot;&quot;</span>, line <span class="number">6</span>, <span class="keyword">in</span> leaky_function</span><br><span class="line"></span><br><span class="line">TypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor <span class="string">&#x27;add:0&#x27;</span> created by node <span class="string">&#x27;add&#x27;</span><span class="keyword">is</span> captured by the tf.Graph being executed <span class="keyword">as</span> an <span class="built_in">input</span>. But a tf.Graph isnot allowed to take symbolic tensors <span class="keyword">from</span> another graph <span class="keyword">as</span> its inputs. Make sure <span class="built_in">all</span> captured inputs of the executing tf.Graph are <span class="keyword">not</span> symbolic tensors. Use <span class="keyword">return</span> values, explicit Python <span class="built_in">locals</span> <span class="keyword">or</span> TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function<span class="comment">#all_outputs_of_a_tffunction_must_be_return_values for more information.</span></span><br></pre></td></tr></table></figure><p>主要的区别在于：试图捕捉从无法访问的计算图所溢出张量的错误信息，现在包含堆栈报错信息，可显示张量在用户代码中的创建位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># … Original error message and information …</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># … More stack frames …</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> File &lt;ipython-<span class="built_in">input</span>-<span class="number">5</span>-95ca3a98778f&gt;, line <span class="number">6</span>, <span class="keyword">in</span> leaky_function</span><br><span class="line"></span><br><span class="line">Error detected <span class="keyword">in</span> node <span class="string">&#x27;add&#x27;</span> defined at: File <span class="string">&quot;&lt;ipython-input-5-95ca3a98778f&gt;&quot;</span>, line <span class="number">6</span>, <span class="keyword">in</span> leaky_function</span><br><span class="line"></span><br><span class="line">TypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor <span class="string">&#x27;add:0&#x27;</span> created by node <span class="string">&#x27;add&#x27;</span><span class="keyword">is</span> captured by the tf.Graph being executed <span class="keyword">as</span> an <span class="built_in">input</span>. But a tf.Graph isnot allowed to take symbolic tensors <span class="keyword">from</span> another graph <span class="keyword">as</span> its inputs. Make sure <span class="built_in">all</span> captured inputs of the executing tf.Graph are <span class="keyword">not</span> symbolic tensors. Use <span class="keyword">return</span> values, explicit Python <span class="built_in">locals</span> <span class="keyword">or</span> TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function<span class="comment">#all_outputs_of_a_tffunction_must_be_return_values for more information.</span></span><br></pre></td></tr></table></figure><p><strong>引入 tf.experimental.ExtensionType</strong></p><p>用户定义的类型可以提高您项目的可读性、模块化程度和可维护性。TensorFlow 2.7.0 引入了 ExtensionType API，可用于创建用户定义的、面向对象的类型，与 TensorFlow 的 API  无缝协作。扩展程序类型是对复杂模型所使用的张量进行跟踪和组织的一个好方法。扩展程序类型还可以用于定义新的类张量类型，这种类型对“张量”的基本概念进行了专门化或扩展。要创建扩展程序类型，只需定义一个以 tf.experimental.ExtensionType 为基础的 Python 类，并使用类型注释来指定每个字段的类型：</p><ul><li><p>ExtensionType</p><p><a href="https://tensorflow.google.cn/guide/extension_type">https://tensorflow.google.cn/guide/extension_type</a></p></li><li><p>类型注释</p><p><a href="https://www.python.org/dev/peps/pep-0484/">https://www.python.org/dev/peps/pep-0484/</a></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorGraph</span>(<span class="params">tf.experimental.ExtensionType</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A collection of labeled nodes connected by weighted edges.&quot;&quot;&quot;</span></span><br><span class="line">  edge_weights: tf.Tensor                      <span class="comment"># shape=[num_nodes, num_nodes]</span></span><br><span class="line">  node_labels: typing.Mapping[<span class="built_in">str</span>, tf.Tensor]  <span class="comment"># shape=[num_nodes]; dtype=any</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedTensor</span>(<span class="params">tf.experimental.ExtensionType</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A tensor paired with a boolean mask, indicating which values are valid.&quot;&quot;&quot;</span></span><br><span class="line">  values: tf.Tensor</span><br><span class="line">  mask: tf.Tensor       <span class="comment"># shape=values.shape; false for missing/invalid values.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CSRSparseMatrix</span>(<span class="params">tf.experimental.ExtensionType</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Compressed sparse row matrix (https://en.wikipedia.org/wiki/Sparse_matrix).&quot;&quot;&quot;</span></span><br><span class="line">  values: tf.Tensor     <span class="comment"># shape=[num_nonzero]; dtype=any</span></span><br><span class="line">  col_index: tf.Tensor  <span class="comment"># shape=[num_nonzero]; dtype=int64</span></span><br><span class="line">  row_index: tf.Tensor  <span class="comment"># shape=[num_rows+1]; dtype=int64</span></span><br></pre></td></tr></table></figure><p><code>ExtensionType</code> 基类增加了一个构造函数和一些基于字段类型注释的特殊方法（类似于标准 Python 库中的 <code>typing.NamedTuple</code> 和 <code>@dataclasses.dataclass</code> ）。您可以通过覆盖这些默认值，或添加新的方法、属性或子类来选择性地自定义该类型。</p><ul><li><p>typing.NamedTuple</p><p><a href="https://docs.python.org/3/library/typing.html#typing.NamedTuple">https://docs.python.org/3/library/typing.html#typing.NamedTuple</a></p></li><li><p>@dataclasses.dataclass</p><p><a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass">https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass</a></p></li></ul><p>以下 TensorFlow API 支持扩展程序类型：</p><ul><li><strong>Keras：</strong>可以将扩展程序类型用作 Keras <code>Models</code> 和 <code>Layers</code> 的输入和输出。</li><li><strong>数据集：</strong>可以在 <code>Datasets</code> 中加入扩展程序类型，并通过数据集 <code>Iterators</code> 进行返回。</li><li><strong>TensorFlow hub：</strong>可以将扩展程序类型用作 <code>tf.hub</code> 模块的输入和输出。</li><li><strong>SavedModel：</strong>可以将扩展程序类型用作 <code>SavedModel</code> 函数的输入和输出。</li><li><strong>tf.function：</strong>可以将扩展程序类型用作与 <code>@tf.function</code> 修饰器一起打包的函数的参数和返回值。</li><li><strong>控制流：</strong>可以通过 <code>tf.cond</code> 和 <code>tf.while_loop</code> 之类的控制流算子来使用扩展程序类型。其中包括通过 AutoGraph 添加的控制流算子。</li><li><strong>tf.py_function：</strong>可以将扩展程序类型用作 <code>func</code> 参数至 <code>tf.py_function</code> 的参数和返回值。</li><li><strong>Tensor 算子：</strong>可以使用分派装饰器对扩展程序类型进行扩展，以支持大多数接收 Tensor 输入的 TensorFlow 算子（如，<code>tf.matmul</code>、<code>tf.gather</code> 和 <code>tf.reduce_sum</code>）。</li><li><strong>分发策略：</strong>可以将扩展程序类型用作每个副本的值。</li></ul><ul><li><p><strong>分派装饰器</strong></p><p><a href="https://tensorflow.google.cn/guide/extension_type#tensor_api_dispatch">https://tensorflow.google.cn/guide/extension_type#tensor_api_dispatch</a></p></li></ul><p>若要了解更多有关扩展程序类型的信息，请参阅扩展程序类型指南。</p><ul><li><p>扩展程序类型指南</p><p><a href="https://tensorflow.google.cn/guide/extension_type">https://tensorflow.google.cn/guide/extension_type</a></p></li></ul><p><strong>注意：</strong><code>tf.experimental</code> 前缀表明这是一个新的 API，我们希望从实际使用中收集反馈；除非有任何不可预见的设计问题，我们计划根据 TF 实验性政策将 <code>ExtensionType</code> 迁移出实验性软件包。</p><ul><li><p>TF 实验性政策</p><p><a href="https://github.com/tensorflow/community/blob/master/governance/api-reviews.md#experimental-apis">https://github.com/tensorflow/community/blob/master/governance/api-reviews.md#experimental-apis</a></p></li></ul><p><strong>TF2 迁移更加简单！</strong></p><p>为了支持有兴趣将工作负载从 TF1 迁移到 TF2 的用户，我们在 TensorFlow 网站上创建了一个新的 <code>Migrate to TF2</code> 标签，其中包括更新的指南和全新的文档，以及 Colab 中具体、可运行的示例。</p><ul><li><p>Migrate to TF2</p><p><a href="https://tensorflow.google.cn/guide/migrate">https://tensorflow.google.cn/guide/migrate</a></p></li><li><p>Colab</p><p><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></p></li></ul><p>我们还增加了一个新的 Shim 工具，可显著简化 variable_scope-based 模型向 TF2 的迁移。它有望使大多数 TF1 用户在 TF2 管道中按原样（或仅进行微小调整）运行现有模型架构，而无需重写建模代码。您可以在模型映射指南中了解更多相关信息。</p><ul><li><p>新的 Shim 工具</p><p><a href="https://tensorflow.google.cn/guide/migrate/model_mapping">https://tensorflow.google.cn/guide/migrate/model_mapping</a></p></li><li><p>模型映射</p><p><a href="https://tensorflow.google.cn/guide/migrate/model_mapping">https://tensorflow.google.cn/guide/migrate/model_mapping</a></p></li></ul><p><strong>TensorFlow Hub 上</strong></p><p><strong>新的社区贡献模型</strong></p><p>自上一版 TensorFlow 发布以来，整个社区热切合作，在 TensorFlow Hub 上提供了许多新模型。</p><ul><li><p>TensorFlow Hub</p><p><a href="https://tensorflow.google.cn/hub">https://tensorflow.google.cn/hub</a></p></li></ul><p>现在您可以找到 MLP-Mixer、Vision Transformers、Wav2Vec2、RoBERTa、ConvMixer、DistillBERT、YoloV5 等诸多模型。</p><ul><li><p>MLP-Mixer</p><p><a href="https://hub.tensorflow.google.cn/sayakpaul/collections/mlp-mixer/1">https://hub.tensorflow.google.cn/sayakpaul/collections/mlp-mixer/1</a></p></li><li><p>Vision Transformers<br><a href="https://hub.tensorflow.google.cn/sayakpaul/collections/vision_transformer/1">https://hub.tensorflow.google.cn/sayakpaul/collections/vision_transformer/1</a></p></li><li><p>Wav2Vec2</p><p><a href="https://hub.tensorflow.google.cn/s?q=wav2vec">https://hub.tensorflow.google.cn/s?q=wav2vec</a></p></li><li><p>RoBERTa</p><p><a href="https://hub.tensorflow.google.cn/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1">https://hub.tensorflow.google.cn/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1</a></p></li><li><p>ConvMixer</p><p><a href="https://hub.tensorflow.google.cn/rishit-dagli/collections/convmixer">https://hub.tensorflow.google.cn/rishit-dagli/collections/convmixer</a></p></li><li><p>DistillBERT</p><p><a href="https://hub.tensorflow.google.cn/s?q=distilbert">https://hub.tensorflow.google.cn/s?q=distilbert</a></p></li><li><p>YoloV5</p><p><a href="https://hub.tensorflow.google.cn/neso613/lite-model/yolo-v5-tflite/tflite_model/1">https://hub.tensorflow.google.cn/neso613/lite-model/yolo-v5-tflite/tflite_model/1</a></p></li></ul><p>所有这些模型都可以通过 TensorFlow Hub 使用。您可以在此处进一步了解有关发布模型的更多信息。</p><ul><li><p>此处</p><p><a href="https://tensorflow.google.cn/hub/publish">https://tensorflow.google.cn/hub/publish</a></p></li></ul><p><strong>相关信息</strong></p><p>请参阅版本说明了解更多信息。</p><ul><li><p>版本说明</p><p><a href="https://github.com/tensorflow/tensorflow/releases">https://github.com/tensorflow/tensorflow/releases</a></p></li></ul><p>欢迎随时关注 TensorFlow 博客，Twitter 或 Youtube，获悉最新动态。</p><ul><li><p>博客</p><p><a href="https://blog.tensorflow.google.cn/">https://blog.tensorflow.google.cn/</a></p></li><li><p>Twitter</p><p><a href="http://twitter.com/tensorflow">http://twitter.com/tensorflow</a></p></li><li><p>Youtube</p><p><a href="https://youtube.com/tensorflow">https://youtube.com/tensorflow</a></p></li></ul><p>您可以通过 Community Spotlight 计划 向我们提交作品，分享构建成果。通过 GitHub 提交问题，或在 TensorFlow 论坛上发帖，分享您的反馈。我们欢迎您的贡献和参与，谢谢！</p><ul><li><p>Community Spotlight 计划</p><p><a href="http://goo.gle/TFCS">http://goo.gle/TFCS</a></p></li><li><p>GitHub</p><p><a href="https://github.com/tensorflow/tensorflow/issues">https://github.com/tensorflow/tensorflow/issues</a></p></li><li><p>TensorFlow 论坛</p><p><a href="https://discuss.tensorflow.google.cn/">https://discuss.tensorflow.google.cn/</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 - TensorFlow </tag>
            
            <tag> 微信公众号 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自己辛苦学完，结果转眼就忘？到底怎么复习和巩固知识？</title>
      <link href="/2021/12/02/%E8%87%AA%E5%B7%B1%E8%BE%9B%E8%8B%A6%E5%AD%A6%E5%AE%8C%EF%BC%8C%E7%BB%93%E6%9E%9C%E8%BD%AC%E7%9C%BC%E5%B0%B1%E5%BF%98%EF%BC%9F%E5%88%B0%E5%BA%95%E6%80%8E%E4%B9%88%E5%A4%8D%E4%B9%A0%E5%92%8C%E5%B7%A9%E5%9B%BA%E7%9F%A5%E8%AF%86%EF%BC%9F/"/>
      <url>/2021/12/02/%E8%87%AA%E5%B7%B1%E8%BE%9B%E8%8B%A6%E5%AD%A6%E5%AE%8C%EF%BC%8C%E7%BB%93%E6%9E%9C%E8%BD%AC%E7%9C%BC%E5%B0%B1%E5%BF%98%EF%BC%9F%E5%88%B0%E5%BA%95%E6%80%8E%E4%B9%88%E5%A4%8D%E4%B9%A0%E5%92%8C%E5%B7%A9%E5%9B%BA%E7%9F%A5%E8%AF%86%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>主要内容：怎么复习和巩固知识：成果验证<br>验证知识的两个方式：新例预测，转换表述</p><h2 id="一、为什么要验证"><a href="#一、为什么要验证" class="headerlink" title="一、为什么要验证"></a>一、为什么要验证</h2><p>我们的疑问？<br>自己所学的知识能够不是已经被前人验证过了，为什么要自己还要验证。<br>但验证的实际是，前人脑中所建构的知识<br>而我们的目标是，在自己脑中建构相同的知识<br>并且通过有效训练，间接地让大脑自动调整神经结构，完成建构。</p><h2 id="二、什么是验证"><a href="#二、什么是验证" class="headerlink" title="二、什么是验证"></a>二、什么是验证</h2><p>自己的思考：通俗的说就是去用，带入实际<br>1.通过验证和反馈，让大脑知道是已经泛化的知识，也不用遗忘了。<br>而这个反馈信号就是通过验证来间接提供给大脑的。<br>2.比如：刚学某个知识时，怎么学都没印象，可一旦将其应用到现实中后，在很长一段时间里都不会遗忘。<br>3.验证最重要的功能是：确保我们建构的知识是正确的</p><h2 id="三、如何验证"><a href="#三、如何验证" class="headerlink" title="三、如何验证"></a>三、如何验证</h2><p>验证的是泛化能力<br>怎么泛化，用建构的知识去解决新问题</p><h3 id="实例性材料的验证方式"><a href="#实例性材料的验证方式" class="headerlink" title="实例性材料的验证方式"></a>实例性材料的验证方式</h3><p>新例预测<br>比如：初高中时<br>应试新题-应试知识-应试答案</p><h3 id="指令性材料的验证方式"><a href="#指令性材料的验证方式" class="headerlink" title="指令性材料的验证方式"></a>指令性材料的验证方式</h3><p>用另一种表述方式表达意识<br>比如：要用自己的话来表述</p><h2 id="为什么以教为学如此重要？"><a href="#为什么以教为学如此重要？" class="headerlink" title="为什么以教为学如此重要？"></a>为什么以教为学如此重要？</h2><p>以教促学是两种方法的结合<br>举个例子+换个说法<br>新例预测-举个例子<br>转换表述-换个说法<br>关键就在于教学过程，既可以验证建构知识的正确性，同时提供了一个反馈信号，让大脑更好的记住，从而巩固了知识的存储。</p><p>下期预告，课程应用串讲<br>『笔记整理者：B站-Thinking变得非凡』<br>注：如有问题和不足大家可以提出参考意见​</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B站 </tag>
            
            <tag> 科研能力 - 学习能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>顶会投稿有感</title>
      <link href="/2021/12/02/%E9%A1%B6%E4%BC%9A%E6%8A%95%E7%A8%BF%E6%9C%89%E6%84%9F/"/>
      <url>/2021/12/02/%E9%A1%B6%E4%BC%9A%E6%8A%95%E7%A8%BF%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>NeurIPS21放榜了，挑了若干相关文章看了看，包括审稿人意见。真实感慨万千。</p><p>1.看了十几篇联邦学习文章，大部分都是poster，少部分spotlight。有的文章虽然有理论有实验，但解决的问题很牵强，感觉用一个很不方便的方法解决一个不太重要的问题；有的文章把简单的方法硬绕的很难让人理解。可能还是自己水平不够吧，许多觉得并没有说明白的，审稿人却说“很清晰”。</p><p>2.审稿人的评论也很奇怪。有的文章看一眼图觉得方法挺直观，但看描述反而一大堆问号不知怎么处理的。审稿人给8分，总的评价是“reads  well”，然而给出的几条“小建议”里却是好几部分“confusing”；有审稿人说“虽然文中只用了XX几个小数据集，但对该问题已经足够了”，让我有点傻傻分不清rebuttal双方。</p><p>3.对比自己遇到的审稿人。同样遇到了“要与personalization方法比较”，而我明明是提出一个类似于FedAvg一样全局模型的方法。rebuttal中好说歹说与个性化模型的不同，收到回复“不令人信服”。恰好也遇到了有文章也有相同的问题，只见作者只说了“我们得到的是全局模型，与personalization方法不同，所以不需要比较”，得到的回复是“回答很令人满意，同意分数由X改为Y”；同样被要求分析<a href="https://www.zhihu.com/search?q=%E6%94%B6%E6%95%9B%E6%80%A7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:437952825%7D">收敛性</a></p><p>，自身水平有限没有从理论给出结果，只能通过较多的实验说明自身方法的提升性能，然而还是被无情pass。恰好也有文章有类似要求，作者也是从实验给出结果“图X和图Y表明我们方法比<a href="https://www.zhihu.com/search?q=%E5%9F%BA%E7%BA%BF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:437952825%7D">基线</a>还要平稳”，却平稳过关。当然，也有好几篇文章并没有被问到这个问题。只能感慨遇到什么样的<a href="https://www.zhihu.com/search?q=%E5%AE%A1%E7%A8%BF%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:437952825%7D">审稿人</a></p><p>，一切都是命。</p><p>总的来看，自我感觉有的文章比自己写的好，有的文章并不如自己的。spotlight的也并没有觉得比poster更好。只能再接再厉，再碰运气了。</p><p>p.s. 正好follow一篇ICLR文章，使用别人复现代码发现文章方法虽然加了一大堆貌似有理的手段，但实际结果却和不加是一样的。而文章鸡贼的在实验里没有放这个基线，而是放了自己构造的几个更差的基线。然而这都中了，只能说无耻。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 论文投稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于NLP那些你不知道的事</title>
      <link href="/2021/12/02/%E5%85%B3%E4%BA%8ENLP%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B/"/>
      <url>/2021/12/02/%E5%85%B3%E4%BA%8ENLP%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：杨夕</p><p>NLP论文学习笔记：<a href="https://github.com/km1994/nlp_paper_study">https://github.com/km1994/nlp_paper_study</a></p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=100005719&idx=1&sn=14d34d70a7e7cbf9700f804cca5be2d0&chksm=1bbff26d2cc87b7b9d2ed12c8d280cd737e270cd82c8850f7ca2ee44ec8883873ff5e9904e7e&scene=18#wechat_redirect">手机版NLP论文学习笔记</a></strong></p><p>个人介绍：大佬们好，我叫杨夕，该项目主要是本人在研读顶会论文和复现经典论文过程中，所见、所思、所想、所闻，可能存在一些理解错误，希望大佬们多多指正。</p><p>NLP 百面百搭 地址：<a href="https://github.com/km1994/NLP-Interview-Notes">https://github.com/km1994/NLP-Interview-Notes</a></p><p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=100005719&idx=3&sn=5d8e62993e5ecd4582703684c0d12e44&chksm=1bbff26d2cc87b7bf2504a8a4cafc60919d722b6e9acbcee81a626924d80f53a49301df9bd97&scene=18#wechat_redirect">手机版NLP百面百搭</a></strong></p><p>推荐系统 百面百搭 地址：<a href="https://github.com/km1994/RES-Interview-Notes">https://github.com/km1994/RES-Interview-Notes</a></p><p><strong><a href="https://mp.weixin.qq.com/s/b_KBT6rUw09cLGRHV_EUtw">手机版推荐系统百面百搭</a></strong></p></blockquote><p><a href="https://github.com/km1994/nlp_paper_study/blob/master/other_study/resource/pic/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210301212242.png"><img src="https://github.com/km1994/nlp_paper_study/raw/master/other_study/resource/pic/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210301212242.png" alt="img"></a></p><blockquote><p><strong>关注公众号 【关于NLP那些你不知道的事】 加入 【NLP &amp;&amp; 推荐学习群】一起学习！！！</strong></p></blockquote><blockquote><p>注：github 网页版 看起来不舒服，可以看 <strong><a href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=100005719&idx=1&sn=14d34d70a7e7cbf9700f804cca5be2d0&chksm=1bbff26d2cc87b7b9d2ed12c8d280cd737e270cd82c8850f7ca2ee44ec8883873ff5e9904e7e&scene=18#wechat_redirect">手机版NLP论文学习笔记</a></strong></p></blockquote><ul><li>【关于 NLP】 那些你不知道的事<ul><li>介绍<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E8%AE%BA%E6%96%87%E5%B7%A5%E5%85%B7%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 论文工具】那些你不知道的事</a></li><li>NLP 学习篇<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E7%BB%8F%E5%85%B8%E4%BC%9A%E8%AE%AE%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E7%AF%87">经典会议论文研读篇</a></li><li>理论学习篇<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E7%AF%87">经典论文研读篇</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-transformer--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 transformer 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 预训练模型】 那些的你不知道的事</a></li><li>【关于 信息抽取】 那些的你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体关系联合抽取】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 命名实体识别】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 关系抽取】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%A1%A3%E7%BA%A7%E5%88%AB%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文档级别关系抽取】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 事件抽取】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 关键词提取】 那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 新词发现】 那些你不知道的事</a></li></ul></li><li>【关于 知识图谱 】 那些的你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87%E7%AF%87-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体链指篇】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E6%B6%88%E6%AD%A7--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体消歧 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8Ekgqa--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于KGQA 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8Eneo4j---%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于Neo4j  】 那些的你不知道的事</a></li></ul></li><li>【关于 NLP Trick】 那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-dropout-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 Dropout】 那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 主动学习】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 对抗训练】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本预处理】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 半监督学习】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-gcn-in-nlp-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 GCN in NLP 】那些你不知道的事</a></li></ul></li><li>【关于 问答系统】 那些的你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-faq-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 FAQ 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E5%A4%9A%E8%BD%AE%E6%A3%80%E7%B4%A2-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 多轮检索 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-kbfaq-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 KBFAQ 】那些你不知道的事</a></li></ul></li><li>【关于 对话系统】 那些的你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3-nlu%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 自然语言理解 NLU】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E7%8A%B6%E6%80%81%E8%BF%BD%E8%B8%AAdst%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 状态追踪（DST）】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90nlg-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 自然语言生成NLG 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-e2e-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 E2E 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-rasa--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 Rasa 】 那些的你不知道的事</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本摘要】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本匹配】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 机器翻译】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本生成】 那些的你不知道的事</a></li><li>【关于 NLP分类任务】那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E7%BB%86%E7%B2%92%E5%BA%A6%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 细粒度情感分析】 那些的你不知道的事</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 中文分词】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 搜索引擎】 那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本纠错】 那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-text-to-sql-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 Text-to-SQL】 那些你不知道的事</a></li></ul></li><li>实战篇<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E9%87%8D%E7%82%B9%E6%8E%A8%E8%8D%90%E7%AF%87">重点推荐篇</a></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BC%9A%E8%AE%AE%E6%94%B6%E9%9B%86%E7%AF%87">会议收集篇</a></li><li><a href="https://github.com/km1994/nlp_paper_study#elastrsearch-%E5%AD%A6%E4%B9%A0%E7%AF%87">Elastrsearch 学习篇</a></li><li>竞赛篇<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-nlp%E6%AF%94%E8%B5%9B-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 NLP比赛】 那些你不知道的事</a></li><li>【关于 NLP 比赛方案学习】 那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-nlp-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88%E5%AD%A6%E4%B9%A0-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B-1">【关于 NLP 比赛方案学习】 那些你不知道的事</a></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90">学习资源</a></li><li><a href="https://github.com/km1994/nlp_paper_study#nlp-%E6%95%B0%E6%8D%AE%E9%9B%86">NLP 数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study#gcn_study%E5%AD%A6%E4%B9%A0%E7%AF%87">GCN_study学习篇</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li></ul></li></ul><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="【关于-论文工具】那些你不知道的事"><a href="#【关于-论文工具】那些你不知道的事" class="headerlink" title="【关于 论文工具】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/other_study/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea/">【关于 论文工具】那些你不知道的事</a></h3><ul><li>问题<ul><li>作为一名 scholar，你是否和我一样，在刚入门 NLP 时，对于陌生领域有种无从下手，心存畏惧？</li><li>作为一名 scholar，你是否还在发愁如何找好的论文？</li><li>作为一名 scholar，你是否还在为 自己 的 英文阅读 能力跟不上 很烦恼？</li><li>作为一名 scholar，你是否还在为 看到 一篇好paper，但是复现不出 code 而心累？</li><li>作为一名 scholar，你是否还在为 有Good idea，Outstanding Experimental  results，Beautiful Chinese manuscript，结果 Bad English manuscript, Poor  Journal 而奔溃？</li><li>作为一名 scholar，你是否在为搞科研没人交流而自闭？</li></ul></li><li>当你看到这一篇文档，你将不在为这些问题而烦恼，因为我们为你准备了一整套免费的从 论文查找-&gt;论文翻译-&gt;论文理解-&gt;相关代码搜索-&gt;写英文稿-&gt;科研学术交流 的路径。<ul><li>论文不会找怎么办？<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#%E9%A1%B6%E4%BC%9A%E8%B5%84%E8%AE%AF">顶会资讯</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#%E8%AE%BA%E6%96%87%E6%90%9C%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7">论文搜索和分析工具</a></li></ul></li><li>外文读不懂怎么办？<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E7%A5%9E%E5%99%A8--%E9%80%9A%E5%A4%A9%E5%A1%94">论文翻译神器 ———— 通天塔</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E5%B0%8F%E5%8A%A9%E6%89%8B--%E5%BD%A9%E4%BA%91%E5%B0%8F%E8%AF%91">论文翻译小助手 ———— 彩云小译</a></li></ul></li><li>外文没 code 怎么办？<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#papers-with-code">papers with code</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#opengithub-%E6%96%B0%E9%A1%B9%E7%9B%AE%E5%BF%AB%E6%8A%A5">OpenGitHub 新项目快报</a></li></ul></li><li>外文写起来麻烦怎么办<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#overleaf">Overleaf</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#authorea">Authorea</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#code-ocean">Code ocean</a></li></ul></li><li>搞科研没人交流怎么办？<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#shortscience">Shortscience</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#openreview">OpenReview</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0idea#scirate">Scirate</a></li></ul></li></ul></li></ul><h3 id="NLP-学习篇"><a href="#NLP-学习篇" class="headerlink" title="NLP 学习篇"></a>NLP 学习篇</h3><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h4 id="经典会议论文研读篇"><a href="#经典会议论文研读篇" class="headerlink" title="经典会议论文研读篇"></a>经典会议论文研读篇</h4><ul><li>ACL2020<ul><li>【关于 CHECKLIST】 那些你不知道的事<ul><li>阅读理由：ACL2020 best paper ，利用 软件工程 的 思想 思考 深度学习</li><li>动机：针对 train-val-test 分割方法 评估 模型性能容易出现 不全面、偏向性、可解性差问题；</li><li>方法：提出了一种模型无关和任务无关的测试方法checklist，它使用三种不同的测试类型来测试模型的独立性。</li><li>效果：checklist揭示了大型软件公司开发的商业系统中的关键缺陷，表明它是对当前实践的补充好吧。测试使用 checklist 创建的模型可以应用于任何模型，这样就可以很容易地将其纳入当前的基准测试或评估中管道。</li></ul></li></ul></li></ul><h4 id="-2"><a href="#-2" class="headerlink" title=""></a></h4><h4 id="理论学习篇"><a href="#理论学习篇" class="headerlink" title="理论学习篇"></a>理论学习篇</h4><h5 id="-3"><a href="#-3" class="headerlink" title=""></a></h5><h5 id="经典论文研读篇"><a href="#经典论文研读篇" class="headerlink" title="经典论文研读篇"></a>经典论文研读篇</h5><ul><li>那些你所不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/transformer_study/Transformer/">【关于Transformer】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/bert_study/T1_bert/">【关于Bert】 那些的你不知道的事</a></li></ul></li></ul><h5 id="-4"><a href="#-4" class="headerlink" title=""></a></h5><h5 id="【关于-transformer-】-那些的你不知道的事"><a href="#【关于-transformer-】-那些的你不知道的事" class="headerlink" title="【关于 transformer 】 那些的你不知道的事"></a>【关于 transformer 】 那些的你不知道的事</h5><ul><li><p>【关于Transformer】 那些的你不知道的事</p><p>  transformer 论文学习</p><ul><li><p>【关于Transformer】 那些的你不知道的事</p><ol><li>为什么要有 Transformer?</li><li>Transformer 作用是什么？</li><li>Transformer 整体结构怎么样？</li><li>Transformer-encoder 结构怎么样？</li><li>Transformer-decoder 结构怎么样?</li><li>传统 attention 是什么?</li><li>self-attention 长怎么样?</li><li>self-attention 如何解决长距离依赖问题？</li><li>self-attention 如何并行化？</li><li>multi-head attention 怎么解?</li><li>为什么要 加入 position embedding ？</li><li>为什么要 加入 残差模块？</li><li>Layer normalization。Normalization 是什么?</li><li>什么是 Mask？</li><li>Transformer 存在问题？</li><li>Transformer 怎么 Coding?</li></ol></li><li><p>【关于 Transformer-XL】 那些的你不知道的事</p><ul><li>动机<ul><li>RNN：主要面临梯度消失或爆炸（gradient vanishing and explosion），解决方法集中在优化方法、初始化策略、辅助记忆单元的研究上。</li><li>vanilla  Transformer：最长建模长度是固定的，无法捕捉更长依赖关系；等长输入序列的获取通常没有遵循句子或语义边界（出于高效考虑，往往就是将文本按长度一段段截取，而没有采用padding机制），可能造成上下文碎片化（context fragmentation）。</li></ul></li><li>方法<ul><li>引入循环机制（Reccurrence，让上一segment的隐含状态可以传递到下一个segment）：将循环（recurrence）概念引入了深度自注意力网络。不再从头计算每个新segment的隐藏状态，而是复用从之前segments中获得的隐藏状态。被复用的隐藏状态视为当前segment的memory，而当前的segment为segments之间建立了循环连接（recurrent connection）。因此，超长依赖性建模成为了可能，因为信息可以通过循环连接来传播。</li><li>提出一种新的相对位置编码方法，避免绝对位置编码在循环机制下的时序错乱：从之前的segment传递信息也可以解决上下文碎片化的问题。更重要的是，本文展示了使用相对位置而不是用绝对位置进行编码的必要性，这样做可以在不造成时间混乱（temporal  confusion）的情况下，实现状态的复用。因此，作为额外的技术贡献，文本引入了简单但有效的相对位置编码公式，它可以泛化至比在训练过程中观察到的长度更长的注意力长度。</li></ul></li></ul></li><li><p>【关于 SHA_RNN】 那些的你不知道的事</p><ul><li>论文名称：Single Headed Attention RNN: Stop Thinking With Your Head 单头注意力 RNN: 停止用你的头脑思考</li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/DL_algorithm/transformer_study/T4_Universal_Transformers/">【关于 Universal Transformers】 那些你不知道的事</a></p></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/DL_algorithm/transformer_study/Style_Transformer/LCNQA/">【关于Style_Transformer】 那些你不知道的事</a></p></li><li><p>【关于 Linformer 】 那些你不知道的事</p><ul><li>论文标题：《Linformer: Self-Attention with Linear Complexity》</li><li>来源：ACL 2020</li><li>链接：<a href="https://arxiv.org/abs/2006.04768">https://arxiv.org/abs/2006.04768</a></li><li>参考：<a href="https://zhuanlan.zhihu.com/p/149890569">https://zhuanlan.zhihu.com/p/149890569</a></li></ul></li><li><p>【关于 Performer 】 那些你不知道的事</p><p>【推荐阅读】</p><ul><li>阅读理由：Transformer 作者 Krzysztof Choromanski 针对 Transformer 问题的重新思考与改进</li><li>动机：Transformer 有着巨大的内存和算力需求，因为它构造了一个注意力矩阵，需求与输入呈平方关系;</li><li>思路：使用一个高效的（线性）广义注意力框架（generalized attention framework），允许基于不同相似性度量（核）的一类广泛的注意力机制。</li><li>优点：该方法在保持线性空间和时间复杂度的同时准确率也很有保证，也可以应用到独立的 softmax 运算。此外，该方法还可以和可逆层等其他技术进行互操作。</li></ul></li><li><p>【关于 Efficient Transformers: A Survey】 那些你不知道的事</p><ul><li>一、摘要</li><li>二、Transformer 介绍</li><li>三、Efficient Transformers<ul><li>3.1 Fixed patterns（FP）<ul><li>3.1.1 Fixed patterns（FP） 介绍</li><li>3.1.2 Fixed patterns（FP） 类别</li></ul></li><li>3.2 Combination of Patterns (CP)<ul><li>3.2.1 Combination of Patterns (CP) 介绍</li><li>3.2.2 Combination of Patterns (CP)  类别</li><li>3.2.3 Fixed patterns（FP） vs 多Combination of Patterns (CP)</li></ul></li><li>3.3 Learnable Patterns (LP)<ul><li>3.3.1 Learnable Patterns (LP) 介绍</li><li>3.3.2 Learnable Patterns (LP)  类别</li><li>3.3.3 Learnable Patterns (LP)  优点</li></ul></li><li>3.4 Memory<ul><li>3.4.1 Memory 介绍</li><li>3.4.2 Memory 类别</li></ul></li><li>3.5 Low-Rank 方法<ul><li>3.5.1 Low-Rank 方法 介绍</li><li>3.5.2 Low-Rank 方法 类别</li></ul></li><li>3.6 Kernels 方法<ul><li>3.6.1  Kernels 方法 介绍</li><li>3.6.2  Kernels 方法 代表</li></ul></li><li>3.7  Recurrence 方法<ul><li>3.7.1  Recurrence 方法 介绍</li><li>3.7.2  Kernels 方法 代表</li></ul></li></ul></li><li>四、Transformer 变体 介绍<ul><li>4.1 引言</li><li>4.2 Memory Compressed Transformer</li><li>4.3 Image Transformer</li><li>4.4 Set Transformer</li><li>4.5 Sparse Transformer</li><li>4.6 Axial Transformer</li><li>4.7 Longformer</li><li>4.8  Extended Transformer Construction (ETC)（2020）</li><li>4.9  BigBird（2020）</li><li>4.10  Routing Transformer</li><li>4.11  Reformer（2020）</li><li>4.12  Sinkhorn Transformers</li><li>4.13  Linformer</li><li>4.14   Linear Transformer</li><li>4.15  Performer（2020）</li><li>4.16  Synthesizer models（2020）</li><li>4.17  Transformer-XL（2020）</li><li>4.18  Compressive Transformers</li></ul></li><li>五、总结</li></ul></li></ul></li></ul><h5 id="-5"><a href="#-5" class="headerlink" title=""></a></h5><h5 id="【关于-预训练模型】-那些的你不知道的事"><a href="#【关于-预训练模型】-那些的你不知道的事" class="headerlink" title="【关于 预训练模型】 那些的你不知道的事"></a>【关于 预训练模型】 那些的你不知道的事</h5><ul><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/bert_study/">【关于Bert】 那些的你不知道的事</a>：Bert论文研读</p><ul><li><p>【关于Bert】 那些的你不知道的事</p><ul><li>阅读理由：NLP 的 创世之作</li><li>动机：word2vec 的多义词问题 &amp;&amp; GPT 单向 Transformer &amp;&amp; Elmo 双向LSTM</li><li>介绍：Transformer的双向编码器</li><li>思路：<ul><li>预训练：Task 1：Masked LM &amp;&amp; Task 2：Next Sentence Prediction</li><li>微调：直接利用 特定任务数据 微调</li></ul></li><li>优点：NLP 所有任务上都刷了一遍 SOTA</li><li>缺点：<ul><li>[MASK]预训练和微调之间的不匹配</li><li>Max Len 为 512</li></ul></li><li>【关于SpanBert】 那些的你不知道的事<ul><li>论文：SpanBERT: Improving Pre-training by Representing and Predicting Spans</li><li>论文地址：<a href="https://arxiv.org/abs/1907.10529">https://arxiv.org/abs/1907.10529</a></li><li>github：<a href="https://github.com/facebookresearch/SpanBERT">https://github.com/facebookresearch/SpanBERT</a></li><li>动机：旨在更好地表示和预测文本的 span;</li><li>论文方法-&gt;扩展了BERT：<ul><li>（1）屏蔽连续的随机 span，而不是随机标记；</li><li>（2）训练 span 边界表示来预测屏蔽 span 的整个内容，而不依赖其中的单个标记表示。</li></ul></li></ul></li></ul></li><li><p>【关于 XLNet 】 那些你不知道的事</p><ul><li>阅读理由：Bert 问题上的改进</li><li>动机：<ul><li>Bert 预训练和微调之间的不匹配</li><li>Bert 的 Max Len 为 512</li></ul></li><li>介绍：广义自回归预训练方法</li><li>思路：<ul><li>预训练：<ul><li>Permutation Language Modeling【解决Bert 预训练和微调之间的不匹配】</li><li>Two-Stream Self-Attention for Target-Aware Representations【解决PLM出现的目标预测歧义】</li><li>XLNet将最先进的自回归模型Transformer-XL的思想整合到预训练中【解决 Bert 的 Max Len 为 512】</li></ul></li><li>微调：直接利用 特定任务数据 微调</li></ul></li><li>优点：</li><li>缺点：</li></ul></li><li><p>【关于 Bart】 那些你不知道的事</p><ul><li>论文：Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</li><li>来源：Facebook</li><li>论文地址：<a href="https://mp.weixin.qq.com/s/42rYlyjQsh4loFKRdhJlIg">https://mp.weixin.qq.com/s/42rYlyjQsh4loFKRdhJlIg</a></li><li>开源代码：<a href="https://github.com/renatoviolin/Bart_T5-summarization">https://github.com/renatoviolin/Bart_T5-summarization</a></li><li>阅读理由：Bert 问题上的改进</li><li>动机：<ul><li>BERT：用掩码替换随机 token，双向编码文档。由于缺失 token 被单独预测，因此 BERT 较难用于生成任务;</li><li>GPT：使用自回归方式预测 token，这意味着 GPT 可用于生成任务。但是，该模型仅基于左侧上下文预测单词，无法学习双向交互</li></ul></li><li>介绍：用于预训练序列到序列模型的去噪自动编码器</li><li>思路：<ul><li>预训练：<ul><li>(1) 使用任意噪声函数破坏文本;<ul><li>Token Masking（token 掩码）：按照 BERT 模型，BART 采样随机 token，并用 [MASK]标记 替换它们；</li><li>Sentence Permutation（句子排列变换）：按句号将文档分割成多个句子，然后以随机顺序打乱这些句子；</li><li>Document Rotation（文档旋转）：随机均匀地选择 token，旋转文档使文档从该 token 开始。该任务的目的是训练模型识别文档开头；</li><li>Token Deletion（token 删除）：从输入中随机删除 token。与 token 掩码不同，模型必须确定缺失输入的位置；</li><li>Text Infilling（文本填充）：采样多个文本段，文本段长度取决于泊松分布 (λ = 3)。用单个掩码 token 替换每个文本段。长度为 0 的文本段对应掩码 token 的插入；</li></ul></li><li>(2) 学习模型以重建原始文本。</li><li>Two-Stream Self-Attention for Target-Aware Representations【解决PLM出现的目标预测歧义】</li><li>XLNet将最先进的自回归模型Transformer-XL的思想整合到预训练中【解决 Bert 的 Max Len 为 512】</li></ul></li><li>微调：<ul><li>Sequence Classification Task 序列分类任务: 将相同的输入，输入到encoder和decoder中，最后将decoder的最后一个隐藏节点作为输出，输入到分类层（全连接层）中，获取最终的分类的结果;</li><li>Token Classification Task 序列分类任务: 将完整文档输入到编码器和解码器中，使用解码器最上方的隐藏状态作为每个单词的表征。该表征的用途是分类 token;</li><li>Sequence Generation Task 序列生成任务: 编码器的输入是输入序列，解码器以自回归的方式生成输出;</li><li>Machine Translation 机器翻译: 将BART的encoder端的embedding层替换成randomly  initialized  encoder，新的encoder也可以用不同的vocabulary。通过新加的Encoder，我们可以将新的语言映射到BART能解码到English(假设BART是在English的语料上进行的预训练)的空间. 具体的finetune过程分两阶段:<ol><li>第一步只更新randomly initialized encoder + BART positional embedding + BART的encoder第一层的self-attention 输入映射矩阵。</li><li>第二步更新全部参数，但是只训练很少的几轮。</li></ol></li></ul></li></ul></li><li>优点：它使用标准的基于 Transformer 的神经机器翻译架构，尽管它很简单，但可以看作是对 BERT（由于双向编码器）、GPT（带有从左到右的解码器）和许多其他最近的预训练方案的泛化.</li><li>缺点：</li></ul></li><li><p>【关于 RoBERTa】 那些你不知道的事</p><ul><li>阅读理由：Bert 问题上的改进</li><li>动机：<ul><li>确定方法的哪些方面贡献最大可能是具有挑战性的</li><li>训练在计算上是昂贵的的，限制了可能完成的调整量</li></ul></li><li>介绍：A Robustly Optimized BERT Pretraining Approach</li><li>思路：<ul><li>预训练：<ul><li>去掉下一句预测(NSP)任务</li><li>动态掩码</li><li>文本编码</li></ul></li><li>微调：直接利用 特定任务数据 微调</li></ul></li><li>优点：</li><li>缺点：</li></ul></li><li><p>【关于 ELECTRA 】 那些的你不知道的事</p><ul><li>阅读理由：Bert 问题上的改进 【不推荐阅读，存在注水！】</li><li>动机：<ul><li>只有15%的输入上是会有loss</li></ul></li><li>介绍：判别器 &amp; 生成器 【但是最后发现非 判别器 &amp; 生成器】</li><li>思路：<ul><li>预训练：<ul><li>利用一个基于MLM的Generator来替换example中的某些个token，然后丢给Discriminator来判别</li></ul></li><li>微调：直接利用 特定任务数据 微调</li></ul></li><li>优点：</li><li>缺点：</li></ul></li><li><p>【关于 Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT】 那些你不知道的事</p><ul><li>论文链接：<a href="https://arxiv.org/pdf/2004.14786.pdf">https://arxiv.org/pdf/2004.14786.pdf</a></li><li>代码链接：<a href="https://github.com/bojone/perturbed_masking">https://github.com/bojone/perturbed_masking</a></li><li>动机<ul><li>通过引入少量的附加参数，probe learns 在监督方式中使用特征表示（例如，上下文嵌入）来  解决特定的语言任务（例如，依赖解析）。这样的probe  tasks  的有效性被视为预训练模型编码语言知识的证据。但是，这种评估语言模型的方法会因 probe 本身所学知识量的不确定性而受到破坏</li></ul></li><li>Perturbed Masking<ul><li>介绍：parameter-free probing technique</li><li>目标：analyze and interpret pre-trained models，测量一个单词xj对预测另一个单词xi的影响，然后从该单词间信息中得出全局语言属性（例如，依赖树）。</li></ul></li><li>整体思想很直接，句法结构，其实本质上描述的是词和词之间的某种关系，如果我们能从BERT当中拿到词和词之间相互“作用”的信息，就能利用一些算法解析出句法结构。</li></ul></li><li><p>【关于 GRAPH-BERT】 那些你不知道的事</p><p>)</p><ul><li>论文名称：GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</li><li>论文地址：<a href="https://arxiv.org/abs/2001.05140">https://arxiv.org/abs/2001.05140</a></li><li>论文代码：<a href="https://github.com/jwzhanggy/Graph-Bert">https://github.com/jwzhanggy/Graph-Bert</a></li><li>动机<ul><li>传统的GNN技术问题：<ul><li>模型做深会存在suspended animation和over smoothing的问题。</li><li>由于 graph 中每个结点相互连接的性质，一般都是丢进去一个完整的graph给他训练而很难用batch去并行化。</li></ul></li></ul></li><li>方法：提出一种新的图神经网络模型GRAPH-BERT (Graph based  BERT)，该模型只依赖于注意力机制，不涉及任何的图卷积和聚合操作。Graph-Bert  将原始图采样为多个子图，并且只利用attention机制在子图上进行表征学习，而不考虑子图中的边信息。因此Graph-Bert可以解决上面提到的传统GNN具有的性能问题和效率问题。</li></ul></li><li><p>【关于自训练 + 预训练 = 更好的自然语言理解模型 】 那些的你不知道的事</p><p>)</p><ul><li>论文标题：Self-training Improves Pre-training for Natural Language Understanding</li><li>论文地址：<a href="https://arxiv.org/abs/2010.02194">https://arxiv.org/abs/2010.02194</a></li><li>动机<ul><li>问题一: do  pre-training and self-training capture the same information,  or  are  they  complementary?</li><li>问题二: how can we obtain large amounts of unannotated data from specific domains?</li></ul></li><li>方法<ul><li>问题二解决方法：提出 SentAugment 方法 从 web 上获取有用数据；</li><li>问题一解决方法：使用标记的任务数据训练一个 teacher 模型，然后用它对检索到的未标注句子进行标注，并基于这个合成数据集训练最终的模型。</li></ul></li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/bert_study/Bert_zip">【关于 Bert 模型压缩】 那些你不知道的事</a></p><ul><li><p>【关于 Bert 模型压缩】 那些你不知道的事</p><ul><li>阅读理由：Bert 在工程上问题上的改进</li><li>动机：<ul><li>内存占用；</li><li>功耗过高；</li><li>带来很高的延迟；</li><li>限制了 Bert 系列模型在移动和物联网等嵌入式设备上的部署；</li></ul></li><li>介绍：BERT 瘦身来提升速度</li><li>模型压缩思路：<ul><li>低秩因式分解：在输入层和输出层使用嵌入大小远小于原生Bert的嵌入大小，再使用简单的映射矩阵使得输入层的输出或者最后一层隐藏层的输出可以通过映射矩阵输入到第一层的隐藏层或者输出层；</li><li>跨层参数共享：隐藏层中的每一层都使用相同的参数，用多种方式共享参数，例如只共享每层的前馈网络参数或者只共享每层的注意力子层参数。默认情况是共享每层的所有参数；</li><li>剪枝：剪掉多余的连接、多余的注意力头、甚至LayerDrop[1]直接砍掉一半Transformer层</li><li>量化：把FP32改成FP16或者INT8；</li><li>蒸馏：用一个学生模型来学习大模型的知识，不仅要学logits，还要学attention score；</li></ul></li><li>优点：BERT 瘦身来提升速度</li><li>缺点：<ul><li>精度的下降</li><li>低秩因式分解 and 跨层参数共享 计算量并没有下降；</li><li>剪枝会直接降低模型的拟合能力；</li><li>量化虽然有提升但也有瓶颈；</li><li>蒸馏的不确定性最大，很难预知你的BERT教出来怎样的学生；</li></ul></li></ul></li><li><p>【关于 Distilling Task-Specific Knowledge from BERT into Simple Neural Networks】那些你不知道的事</p><ul><li>动机：<ul><li>随着 BERT 的横空出世，意味着 上一代用于语言理解的较浅的神经网络（RNN、CNN等） 的 过时？</li><li>BERT模型是真的大，计算起来太慢了？</li><li>是否可以将BERT（一种最先进的语言表示模型）中的知识提取到一个单层BiLSTM 或 TextCNN 中？</li></ul></li><li>思路：<ol><li>确定 Teacher 模型（Bert） 和 Student 模型（TextCNN、TextRNN）;</li><li>蒸馏的两个过程：<ol><li>第一，在目标函数附加logits回归部分；</li><li>第二，构建迁移数据集，从而增加了训练集，可以更有效地进行知识迁移。</li></ol></li></ol></li></ul></li><li><p>【关于 AlBert 】 那些你不知道的事</p><ul><li><p>模型压缩方法：低秩因式分解 + 跨层参数共享</p></li><li><p>模型压缩方法介绍：</p><ul><li>低秩因式分解：<ul><li>动机：Bert的参数量大部分集中于模型的隐藏层架构上，在嵌入层中只有30,000词块，其所占据的参数量只占据整个模型参数量的小部分；</li><li>方法：将输入层和输出层的权重矩阵分解为两个更小的参数矩阵；</li><li>思路：在输入层和输出层使用嵌入大小远小于原生Bert的嵌入大小，再使用简单的映射矩阵使得输入层的输出或者最后一层隐藏层的输出可以通过映射矩阵输入到第一层的隐藏层或者输出层；</li><li>优点：在不显著增加词嵌入大小的情况下能够更容易增加隐藏层大小；</li></ul></li><li>参数共享【跨层参数共享】：<ul><li>动机：隐藏层 参数 大小 一致；</li><li>方法：隐藏层中的每一层都使用相同的参数，用多种方式共享参数，例如只共享每层的前馈网络参数或者只共享每层的注意力子层参数。默认情况是共享每层的所有参数；</li><li>优点：防止参数随着网络深度的增加而增大；</li></ul></li></ul></li><li><p>其他改进策略：</p><ul><li><p>句子顺序预测损失(SOP)<strong>代替</strong>Bert中的下一句预测损失(NSP)</p><p>：</p><ul><li>动机：通过实验证明，Bert中的下一句预测损失(NSP) 作用不大；</li><li>介绍：用预测两个句子是否连续出现在原文中替换为两个连续的句子是正序或是逆序，用于进一步提高下游任务的表现</li></ul></li></ul></li><li><p>优点：参数量上有所降低；</p></li><li><p>缺点：其加速指标仅展示了训练过程，由于ALBERT的隐藏层架构<strong>采用跨层参数共享策略并未减少训练过程的计算量</strong>，加速效果更多来源于低维的嵌入层；</p></li></ul></li><li><p>【关于 FastBERT】 那些你不知道的事</p><ul><li>模型压缩方法：知识蒸馏</li><li>模型压缩方法介绍：<ul><li>样本自适应机制（Sample-wise adaptive mechanism）<ul><li>思路：<ul><li>在每层Transformer后都去预测样本标签，如果某样本预测结果的置信度很高，就不用继续计算了，就是自适应调整每个样本的计算量，容易的样本通过一两层就可以预测出来，较难的样本则需要走完全程。</li></ul></li><li>操作：<ul><li>给每层后面接一个分类器，毕竟分类器比Transformer需要的成本小多了</li></ul></li></ul></li><li>自蒸馏（Self-distillation）<ul><li>思路：<ul><li>在预训练和精调阶段都只更新主干参数；</li><li>精调完后freeze主干参数，用分支分类器（图中的student）蒸馏主干分类器（图中的teacher）的概率分布</li></ul></li><li>优点：<ul><li>非蒸馏的结果没有蒸馏要好</li><li>不再依赖于标注数据。蒸馏的效果可以通过源源不断的无标签数据来提升</li></ul></li></ul></li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/bert_study/distilbert/">【关于 distilbert】 那些你不知道的事</a></p></li><li><p>【关于 TinyBert】 那些你不知道的事</p><ul><li>模型压缩方法：知识蒸馏</li><li>tinybert的创新点：学习了teacher Bert中更多的层数的特征表示；</li><li>模型压缩方法介绍：<ul><li>基于transformer的知识蒸馏模型压缩<ul><li>学习了teacher Bert中更多的层数的特征表示；</li><li>特征表示：<ul><li>词向量层的输出；</li><li>Transformer layer的输出以及注意力矩阵；</li><li>预测层输出(仅在微调阶段使用)；</li></ul></li></ul></li><li>bert知识蒸馏的过程<ul><li>左图：整体概括了知识蒸馏的过程<ul><li>左边：Teacher BERT；</li><li>右边：Student TinyBERT</li><li>目标：将Teacher BERT学习到的知识迁移到TinyBERT中</li></ul></li><li>右图：描述了知识迁移的细节；<ul><li>在训练过程中选用Teacher BERT中每一层transformer layer的attention矩阵和输出作为监督信息</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/bert_study/ACL2020_UnsupervisedBert">【关于 Perturbed Masking】那些你不知道的事</a></p><ul><li>论文：Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</li><li>论文链接：<a href="https://arxiv.org/pdf/2004.14786.pdf">https://arxiv.org/pdf/2004.14786.pdf</a></li><li>代码链接：<a href="https://github.com/bojone/perturbed_masking">https://github.com/bojone/perturbed_masking</a></li><li>动机： 通过引入少量的附加参数，probe learns 在监督方式中使用特征表示（例如，上下文嵌入）来  解决特定的语言任务（例如，依赖解析）。这样的probe  tasks  的有效性被视为预训练模型编码语言知识的证据。但是，这种评估语言模型的方法会因 probe 本身所学知识量的不确定性而受到破坏。</li><li>方法介绍：<ul><li>Perturbed Masking<ul><li>介绍：parameter-free probing technique</li><li>目标：analyze and interpret pre-trained models，测量一个单词xj对预测另一个单词xi的影响，然后从该单词间信息中得出全局语言属性（例如，依赖树）。</li></ul></li></ul></li><li>思想：整体思想很直接，句法结构，其实本质上描述的是词和词之间的某种关系，如果我们能从BERT当中拿到词和词之间相互“作用”的信息，就能利用一些算法解析出句法结构。</li></ul></li><li><p><a href="(https://github.com/km1994/nlp_paper_study/tree/master/bert_study/Chinese/">【关于中文预训练模型】那些你不知道的事</a></p><ul><li><a href="(https://github.com/km1994/nlp_paper_study/tree/master/bert_study/Chinese/ChineseBERT/">【关于ChineseBERT】那些你不知道的事</a></li><li>论文名称：ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information</li><li>会议： ACL2021</li><li>论文地址：<a href="https://arxiv.org/abs/2106.16038">https://arxiv.org/abs/2106.16038</a></li><li>论文源码地址：<a href="https://github.com/ShannonAI/ChineseBert">https://github.com/ShannonAI/ChineseBert</a></li><li>模型下载：<a href="https://huggingface.co/hfl/chinese-bert-wwm-ext/tree/main">https://huggingface.co/hfl/chinese-bert-wwm-ext/tree/main</a></li><li>动机：最近的中文预训练模型忽略了中文特有的两个重要方面：字形和拼音，它们为语言理解携带重要的句法和语义信息。</li><li>论文工作：提出了 ChineseBERT，它将汉字的 {\it glyph} 和 {\it pinyin} 信息合并到语言模型预训练中。<ul><li>embedding 层：将 字符嵌入（char embedding）、字形嵌入（glyph embedding）和拼音嵌入（pinyin embedding） 做拼接；</li><li>Fusion Layer 层：将 拼接后的 embedding 向量 做 Fusion 得到 一个 d 维的 Fusion embedding;</li><li>位置拼接：将 Fusion embedding 和 位置嵌入（position embedding）、片段嵌入（segment embedding）相加；</li><li>Transformer-Encoder层;</li></ul></li><li>改进点：<ul><li>在底层的融合层（Fusion Layer）融合了除字嵌入（Char Embedding）之外的字形嵌入（Glyph  Embedding）和拼音嵌入（Pinyin Embedding），得到融合嵌入（Fusion  Embedding），再与位置嵌入相加，就形成模型的输入；</li><li>抛弃预训练任务中的NSP任务。 由于预训练时没有使用NSP任务，因此模型结构图省略了片段嵌入（segment embedding）。实际上下游任务输入为多个段落时（例如：文本匹配、阅读理解等任务），是采用了segment embedding；</li></ul></li><li>实验结果：在大规模未标记的中文语料库上进行预训练，提出的 ChineseBERT 模型在训练步骤较少的情况下显着提高了基线模型的性能。  porpsoed 模型在广泛的中文 NLP 任务上实现了新的 SOTA  性能，包括机器阅读理解、自然语言推理、文本分类、句子对匹配和命名实体识别中的竞争性能。</li></ul></li></ul><h5 id="-6"><a href="#-6" class="headerlink" title=""></a></h5><h5 id="【关于-信息抽取】-那些的你不知道的事"><a href="#【关于-信息抽取】-那些的你不知道的事" class="headerlink" title="【关于 信息抽取】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/">【关于 信息抽取】 那些的你不知道的事</a></h5><h6 id="-7"><a href="#-7" class="headerlink" title=""></a></h6><h6 id="【关于-实体关系联合抽取】-那些的你不知道的事"><a href="#【关于-实体关系联合抽取】-那些的你不知道的事" class="headerlink" title="【关于 实体关系联合抽取】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/">【关于 实体关系联合抽取】 那些的你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/PURE/">【关于 PURE】 那些你不知道的事</a> 【强烈推荐】<ul><li>论文：A Frustratingly Easy Approach for Joint Entity and Relation Extraction</li><li>阅读理由：反直觉！陈丹琦用pipeline方式刷新关系抽取SOTA</li><li>方法：建立两个 encoders，并独立训练:<ul><li>encoder 1：entity model<ul><li>方法：建立在 span-level representations 上</li></ul></li><li>encoder 2：relation model：只依赖于实体模型作为输入特征<ul><li>方法：builds on contextual representations specific to a given pair of span</li></ul></li></ul></li><li>优点：<ul><li>很简单，但我们发现这种流水线方法非常简单有效；</li><li>使用同样的预先训练的编码器，我们的模型在三个标准基准（ACE04，ACE05，SciERC）上优于所有以前的联合模型；</li></ul></li><li>问题讨论：<ul><li>Q1、关系抽取最care什么？<ul><li>解答：引入实体类别信息会让你的关系模型有提升</li></ul></li><li>Q2、共享编码 VS 独立编码 哪家强？<ul><li>解答：由于两个任务各自是不同的输入形式，并且需要不同的特征去进行实体和关系预测，也就是说：使用单独的编码器确实可以学习更好的特定任务特征。</li></ul></li><li>Q3：误差传播不可避免？还是不存在？<ul><li>解答：并不认为误差传播问题不存在或无法解决，而需要探索更好的解决方案来解决此问题</li></ul></li><li>Q4：Effect of Cross-sentence Context<ul><li>解答：使用跨句上下文可以明显改善实体和关系</li></ul></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/PRGC">【关于 PRGC】 那些你不知道的事</a><ul><li>论文：PRGC: Potential Relation and Global Correspondence Based JointRelational Triple Extraction</li><li>来源：ACL 2021</li><li>论文地址：<a href="https://arxiv.org/pdf/2106.09895">https://arxiv.org/pdf/2106.09895</a></li><li>开源代码：<a href="https://github.com/hy-struggle/PRGC">https://github.com/hy-struggle/PRGC</a></li><li>动机：从非结构化文本中联合提取实体和关系是信息提取中的一项关键任务。最近的方法取得了可观的性能，但仍然存在一些固有的局限性：<ul><li>关系预测的冗余：TPLinker 为了避免曝光偏差，它利用了相当复杂的解码器，导致了稀疏的标签，关系冗余；</li><li>span-based 的提取泛化性差和效率低下;</li></ul></li><li>论文方法：<ul><li>从新颖的角度将该任务分解为三个子任务：<ul><li>Relation  Judgement；</li><li>Entity  Extraction；</li><li>Subject-object Alignment；</li></ul></li><li>然后提出了一个基于 Potential Relation and Global Correspondence (PRGC) 的联合关系三重提取框架：<ul><li><strong>Potential Relation Prediction</strong>：给定一个句子，模型先预测一个可能存在关系的子集，以及得到一个全局矩阵；</li><li><strong>Relation-Specific Sequence Tagging</strong>：然后执行序列标注，标注存在的主体客体，以处理 subjects  and  object 之间的重叠问题；</li><li><strong>Global Correspondence</strong>：枚举所有实体对，由全局矩阵裁剪；</li></ul></li><li>实验结果：PRGC 以更高的效率在公共基准测试中实现了最先进的性能，并在重叠三元组的复杂场景中提供了一致的性能增益</li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96%E6%80%BB%E7%BB%93.md">【关于 实体关系联合抽取】那些你不知道的事</a><ol><li>pipeline  方法<ol><li>思路：先命名实体识别（ NER） , 在 关系抽取（RE）</li><li>问题：<ol><li>忽略两任务间的相关性</li><li>误差传递。NER 的误差会影响 RE 的性能</li></ol></li></ol></li><li>end2end 方法<ol><li>解决问题：实体识别、关系分类</li><li>思路：<ol><li>实体识别<ol><li>BIOES 方法：提升召回？和文中出现的关系相关的实体召回</li><li>嵌套实体识别方法：解决实体之间有嵌套关系问题</li><li>头尾指针方法：和关系分类强相关？和关系相关的实体召回</li><li>copyre方法</li></ol></li><li>关系分类：<ol><li>思路：判断 【实体识别】步骤所抽取出的实体对在句子中的关系</li><li>方法：<ol><li>方法1：1. 先预测头实体，2. 再预测关系、尾实体</li><li>方法2：1. 根据预测的头、尾实体预测关系</li><li>方法3：1. 先找关系，再找实体 copyre</li></ol></li><li>需要解决的问题：<ol><li>关系重叠</li><li>关系间的交互</li></ol></li></ol></li></ol></li></ol></li><li>论文介绍<ol><li>【paper 1】Joint entity recognition and relation extraction as a multi-head selection problem</li><li>【paper 2】Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy[ACL2017]</li><li>【paper 3】GraphRel:Modeling Text as Relational Graphs for Joint Entity and Relation Extraction [ACL2019]</li><li>【paper 4】CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning [AAAI2020]</li><li>【paper 5】Span-based Joint Entity and Relation Extraction with Transformer Pre-training [ECAI 2020]</li><li>【paper 6】A Novel Cascade Binary Tagging Framework for Relational Triple Extraction[ACL2020]</li><li>【paper 7】END-TO-END NAMED ENTITY RECOGNITION AND RELATION EXTRACTION USING PRE-TRAINED LANGUAGE MODELS</li></ol></li></ol></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/T2014_joint_extraction/">Incremental Joint Extraction of Entity Mentions and Relations</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/JointER/">【关于 Joint NER】那些你不知道的事</a><ul><li>论文名称：Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/ACL2019_GraphRel/">【关于 GraphRel】 那些的你不知道的事</a><ul><li>论文名称：论文名称：GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</li><li>动机<ul><li>想要自动提取特征的联合模型<ul><li>通过堆叠Bi-LSTM语句编码器和GCN (Kipf和Welling, 2017)依赖树编码器来自动学习特征</li><li>用以考虑线性和依赖结构<ul><li>类似于Miwa和Bansal(2016)（一样是堆叠的）<ul><li>方法<ul><li>每个句子使用Bi-LSTM进行自动特征学习</li><li>提取的隐藏特征由连续实体标记器和最短依赖路径关系分类器共享</li></ul></li><li>问题<ul><li>然而，在为联合实体识别和关系提取引入共享参数时：<ul><li>它们仍然必须将标记者预测的实体提及通过管道连接起来</li><li>形成关系分类器的提及对</li></ul></li></ul></li></ul></li></ul></li><li>考虑重叠关系</li><li>如何考虑关系之间的相互作用<ul><li>2nd-phase relation-weighted GCN</li><li>重叠关系(常见）<ul><li>情况<ul><li>两个三元组的实体对重合</li><li>两个三元组都有某个实体mention</li></ul></li><li>推断<ul><li>困难（对联合模型尤其困难，因为连实体都还不知道）</li></ul></li></ul></li></ul></li></ul></li><li>方法：<ul><li>学习特征<ul><li>通过堆叠Bi-LSTM语句编码器和GCN (Kipf和Welling, 2017)依赖树编码器来自动学习特征</li></ul></li><li>第一阶段的预测<ul><li>GraphRel标记实体提及词，预测连接提及词的关系三元组</li><li>用关系权重的边建立一个新的全连接图（中间图）</li><li>指导：关系损失和实体损失</li></ul></li><li>第二阶段的GCN<ul><li>通过对这个中间图的操作</li><li>考虑实体之间的交互作用和可能重叠的关系</li><li>对每条边进行最终分类</li><li>在第二阶段，基于第一阶段预测的关系，我们为每个关系构建完整的关系图，并在每个图上应用GCN来整合每个关系的信息，进一步考虑实体与关系之间的相互作用。</li></ul></li></ul></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/ERE_study/T20ACL_HBT_su/">【关于 关系抽取 之 HBT】 那些的你不知道的事</a><ul><li>论文名称：A Novel Hierarchical Binary Tagging Framework for Relational Triple Extraction</li><li>论文会议：ACL 2020</li><li>keras4bert 版本：<a href="https://github.com/bojone/lic2020_baselines">https://github.com/bojone/lic2020_baselines</a> 【苏神 Lic2020 baseline】</li><li>pytorch 版本：<a href="https://github.com/powerycy/Lic2020-">https://github.com/powerycy/Lic2020-</a> 【逸神 pytorch 复现版本】</li><li>动机：<ul><li>pipeline approach<ul><li>思路<ul><li>实体抽取：利用一个命名实体识别模型 识别句子中的所有实体；</li><li>关系分类：利用 一个关系分类模型 对每个实体对执行关系分类。 【这一步其实可以理解为文本分类任务，但是和文本分类任务的区别在于，关系分类不仅需要学习句子信息，还要知道 实体对在 句子中 位置信息】</li></ul></li><li>问题<ul><li>误差传递问题：由于 该方法将 实体-关系联合抽取任务 分成 实体抽取+关系分类 两个任务处理，所以 实体抽取任务的错误无法在后期阶段进行纠正，因此这种方法容易遭受错误传播问题；</li></ul></li></ul></li><li>feature-based models and neural network-based models<ul><li>思路<ul><li>通过用学习表示替换人工构建的特征，基于神经网络的模型在三重提取任务中取得了相当大的成功。</li></ul></li><li>问题<ul><li>大多数现有方法无法正确处理句子包含多个相互重叠的关系三元组的情况。</li></ul></li></ul></li><li>基于Seq2Seq模型  and GCN<ul><li>思路：<ul><li>提出了具有复制机制以提取三元组的序列到序列（Seq2Seq）模型。 他们基于Seq2Seq模型，进一步研究了提取顺序的影响，并通过强化学习获得了很大的改进。</li></ul></li><li>问题：<ul><li>过多 negative examples：在所有提取的实体对中，很多都不形成有效关系，从而产生了太多的negative examples；</li><li>EPO(Entity Pair Overlap) 问题：当同一实体参与多个关系时，分类器可能会感到困惑。 没有足够的训练样例的情况下，分类器就很难准确指出实体参与的关系；</li></ul></li></ul></li></ul></li><li>方式：实现了一个不受重叠三元组问题困扰的HBT标注框架(Hierarchical Binary Tagging  Framework)来解决RTE任务；论文并不是学习关系分类器f（s，o）→r，而是学习关系特定的标记器fr（s）→o；每个标记器都可以识别特定关系下给定 subject 的可能 object(s)。 或不返回任何 object，表示给定的主题和关系没有 triple。</li><li>核心思想：把关系(Relation)建模为将头实体(Subject)映射到尾实体(Object)的函数，而不是将其视为实体对上的标签。</li><li>思路：<ul><li>首先，我们确定句子中所有可能的 subjects；</li><li>然后针对每个subjects，我们应用特定于关系的标记器来同时识别所有可能的 relations 和相应的 objects。</li></ul></li><li>结构：<ul><li>BERT Encoder层：使用 Bert 做 Encoder，其实就是 用 Bert 做 Embedding 层使用。</li><li>Hierarchical Decoder层<ul><li>Subject tagger 层：用于 提取 Subject;</li><li>Relation-Specific Object Taggers 层：由一系列relation-specific object taggers（之所以这里是多个taggers是因为有多个可能的relation）；</li></ul></li></ul></li></ul></li></ul><h6 id="-8"><a href="#-8" class="headerlink" title=""></a></h6><h6 id="【关于-命名实体识别】那些你不知道的事"><a href="#【关于-命名实体识别】那些你不知道的事" class="headerlink" title="【关于 命名实体识别】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/">【关于 命名实体识别】那些你不知道的事</a></h6><ul><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_studyEMNLP2018_AutoNER">【关于 AutoNER】 那些你不知道的事</a></p><ul><li>论文名称：Learning Named Entity Tagger using Domain-Specific Dictionary</li><li>会议： EMNLP2018</li><li>论文地址：<a href="https://arxiv.org/abs/1809.03599">https://arxiv.org/abs/1809.03599</a></li><li>项目地址：<a href="https://github.com/shangjingbo1226/AutoNER">https://github.com/shangjingbo1226/AutoNER</a></li><li>论文动机：<ul><li>基于机器学习的命名实体识别方法：需要 手工标注特征；</li><li>基于深度学习的命名实体识别方法：需要大量标准数据；</li><li>远程监督（结合外部词典）标注数据：生成的嘈杂标签对学习</li></ul></li><li>论文方法：提出了两种神经模型，以适应字典中嘈杂的远程监督：<ul><li>首先，在传统的序列标记框架下，我们提出了一个修改后的模糊 CRF 层来处理具有多个可能标签的标记。</li><li>在确定远程监督中嘈杂标签的性质后，我们超越了传统框架，提出了一种新颖、更有效的神经模型 AutoNER，该模型具有新的 Tie or Break 方案。</li><li>讨论了如何改进远程监督以获得更好的 NER 性能。</li></ul></li><li>实验结果：在三个基准数据集上进行的大量实验表明，仅使用字典而不需要额外的人力时，AutoNER 实现了最佳性能，并通过最先进的监督基准提供了具有竞争力的结果。</li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B3%E4%BA%8E-continual-learning-for-ner%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 Continual Learning for NER】那些你不知道的事</a></p><ul><li>会议：AAAI2021</li><li>论文：Continual Learning for Named Entity Recognition</li><li>论文下载地址：<a href="https://assets.amazon.science/65/61/ecffa8df45ad818c3f69fb1cf72b/continual-learning-for-named-entity-recognition.pdf">https://assets.amazon.science/65/61/ecffa8df45ad818c3f69fb1cf72b/continual-learning-for-named-entity-recognition.pdf</a></li><li>动机：业务扩展，需要新增 实体类型（eg:像 Sirior Alexa 这样的语音助手不断地为其功能引入新的意图，因此<strong>新的实体类型经常被添加到他们的插槽填充模型</strong>中）</li><li>方法：研究 将 知识蒸馏（KD） 应用于 NER 的 CL 问题，通过 将  “teacher”模型的预测合并到“student”模型的目标函数中，该模型正在接受训练以执行类似但略有修改的任务。  通过学习输出概率分布，而不仅仅是标签，使得学生表现得与教师相似。</li><li>论文贡献：<ul><li>(i) 我们展示了如何使 CL 技术适应 NLU 域，以逐步学习 NER 的新实体类型；</li><li>(ii) 我们在两个 EnglishNER 数据集上的结果表明，我们的 CL 方法使模型能够不断学习新的实体类型，而不会失去识别先前获得的类型的能力；</li><li>(iii) 我们表明我们的半监督策略实现了与全监督设置相当的结果。</li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/UnlabeledEntityProblem/">【关于 NER数据存在漏标问题】那些你不知道的事</a></p><ul><li>一、摘要</li><li>二、为什么 数据会存在漏标？</li><li>三、什么是 带噪学习</li><li>四、NER 数据漏标问题所带来后果？</li><li>五、NER 性能下降 <strong>原因</strong>是什么？</li><li>六、论文所提出的方法是什么？</li><li>七、数据漏标，会导致NER指标下降有多严重？</li><li>八、对「未标注实体问题」的解决方案有哪些？</li><li>九、如何降噪：改变标注框架+负采样？<ul><li>9.1 第一步：改变标注框架</li><li>9.2 第二步：负采样</li></ul></li><li>十、负样本采样，效果如何？</li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/ICLR2021_LEX_BERT/">【关于 LEX-BERT】 那些你不知道的事</a>【强烈推荐】</p><ul><li>推荐理由：在 query 中 引入 标签信息的方法，秒杀 Flat NER，登上 2021 年 Chinese NER SOTA。</li><li>论文名称：《Lex-BERT: Enhancing BERT based NER with lexicons》</li><li>动机：尽管它在NER任务中的表现令人印象深刻，但最近已经证明，添加词汇信息可以显著提高下游性能。然而，没有任何工作在不引入额外结构的情况下将单词信息纳入BERT。在我们的工作中，我们提出了词法BERT（lex-bert），这是一种在基于BERT的NER模型中更方便的词汇借用方法</li><li>方法：<ul><li>LEX-BERT V1：Lex BERT的第一个版本通过在单词的左右两侧插入特殊标记来识别句子中单词的 span。特殊标记不仅可以标记单词的起始位置和结束位置，还可以为句子提供实体类型信息</li><li>LEX-BERT V2：对于在句子中加宽的单词，我们没有在句子中单词的周围插入起始和结束标记，而是在句子的末尾附加一个标记[x]。请注意，我们将标记的位置嵌入与单词的起始标记绑定</li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/NestedNER/">【关于 嵌套命名实体识别（Nested NER）】那些你不知道的事</a></p><ul><li>【关于 Biaffine Ner 】 那些你不知道的事<ul><li>动机：NER 研究 关注于 扁平化NER，而忽略了 实体嵌套问题；</li><li>方法： 在本文中，我们使用基于图的依存关系解析中的思想，以通过 biaffine model 为模型提供全局的输入视图。 biaffine model 对句子中的开始标记和结束标记对进行评分，我们使用该标记来探索所有跨度，以便该模型能够准确地预测命名实体。</li><li>工作介绍：在这项工作中，我们将NER重新确定为开始和结束索引的任务，并为这些对定义的范围分配类别。我们的系统在多层BiLSTM之上使用biaffine模型，将分数分配给句子中所有可能的跨度。此后，我们不用构建依赖关系树，而是根据候选树的分数对它们进行排序，然后返回符合 Flat 或  Nested NER约束的排名最高的树 span；</li><li>实验结果：我们根据三个嵌套的NER基准（ACE 2004，ACE 2005，GENIA）和五个扁平的NER语料库（CONLL  2002（荷兰语，西班牙语），CONLL  2003（英语，德语）和ONTONOTES）对系统进行了评估。结果表明，我们的系统在所有三个嵌套的NER语料库和所有五个平坦的NER语料库上均取得了SoTA结果，与以前的SoTA相比，实际收益高达2.2％的绝对百分比。</li></ul></li><li>【关于 Biaffine 代码解析】 那些你不知道的事<ul><li>摘要</li><li>一、数据处理模块<ul><li>1.1 原始数据格式</li><li>1.2 数据预处理模块 data_pre()<ul><li>1.2.1 数据预处理 主 函数</li><li>1.2.2  训练数据加载 load_data(file_path)</li><li>1.2.3 数据编码 encoder(sentence, argument)</li></ul></li><li>1.3 数据转化为 MyDataset 对象</li><li>1.4 构建 数据 迭代器</li><li>1.5 最后数据构建格式</li></ul></li><li>二、模型构建 模块<ul><li>2.1 主题框架介绍</li><li>2.2 embedding layer</li><li>2.2 BiLSTM</li><li>2.3 FFNN</li><li>2.4 biaffine model</li><li>2.5 冲突解决</li><li>2.6 损失函数</li></ul></li><li>三、学习率衰减 模块</li><li>四、loss 损失函数定义<ul><li>4.1 span_loss 损失函数定义</li><li>4.2 focal_loss 损失函数定义</li></ul></li><li>四、模型训练</li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/NER_study/NERtrick.md">【关于 NER trick】 那些你不知道的事</a></p></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/ACL2019/ACL2019_TENER/">【关于TENER】 那些你不知道的事</a></p><ul><li>论文名称：TENER: Adapting Transformer Encoder for Name Entity Recognition</li><li>动机：<ul><li><ol><li>Transformer 能够解决长距离依赖问题；</li></ol></li><li><ol><li>Transformer 能够并行化；</li></ol></li><li><ol><li>然而，Transformer 在 NER 任务上面效果不好。</li></ol></li></ul></li><li>方法：<ul><li>第一是经验发现。 引入：相对位置编码</li><li>第二是经验发现。 香草变压器的注意力分布是缩放且平滑的。 但是对于NER，因为并非所有单词都需要参加，所以很少注意是合适的。  给定一个当前单词，只需几个上下文单词就足以判断其标签。 平稳的注意力可能包括一些嘈杂的信息。  因此，我们放弃了点生产注意力的比例因子，而使用了无比例且敏锐的注意力。</li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/DynamicArchitecture/">【关于DynamicArchitecture】 那些你不知道的事</a></p><ul><li><p>介绍：Dynamic Architecture范式通常需要设计相应结构以融入词汇信息。</p></li><li><p>论文：</p><ul><li><p>【关于 LatticeLSTM 】那些你不知道的事</p><ul><li>想法：在 char-based 的 LSTM 中引入词汇信息</li><li>做法：<ul><li>根据大量语料生成词典；</li><li>若当前字符与前面的字符无法组成词典中词汇，则按 LSTM 的方法更新记忆状态；</li><li>若当前字符与前面的字符组成词典中词汇，从最新词汇中提取信息，联合更新记忆状态；</li></ul></li><li>存在问题：<ul><li>计算性能低下，导致其<strong>不能充分利用GPU进行并行化</strong>。究其原因主要是每个字符之间的增加word cell（看作节点）数目不一致；</li><li>信息损失：<ul><li>1）每个字符只能获取以它为结尾的词汇信息，对于其之前的词汇信息也没有持续记忆。如对于「大」，并无法获得‘inside’的「长江大桥」信息。</li><li>2）由于RNN特性，采取BiLSTM时其前向和后向的词汇信息不能共享，导致 Lattice LSTM <strong>无法有效处理词汇信息冲突问题</strong></li></ul></li><li>可迁移性差：只适配于LSTM，不具备向其他网络迁移的特性。</li></ul></li></ul></li><li><p>【关于 LR-CNN 】那些你不知道的事</p><ul><li>动机<ul><li>词信息引入问题；</li><li>lattice LSTM 问题：<ul><li>基于 RNN 结构方法不能充分利用 GPU 并行计算资源；<ul><li>针对句子中字符计算；</li><li>针对匹配词典中潜在词</li></ul></li><li>很难处理被合并到词典中的潜在单词之间的冲突：<ul><li>一个字符可能对应词典中多个潜在词，误导模型</li></ul></li></ul></li></ul></li><li>方法：</li><li>Lexicon-Based CNNs：采取CNN对字符特征进行编码，感受野大小为2提取bi-gram特征，堆叠多层获得multi-gram信息；同时采取注意力机制融入词汇信息（word embed）；</li><li>Refining Networks with Lexicon  Rethinking：由于上述提到的词汇信息冲突问题，LR-CNN采取rethinking机制增加feedback  layer来调整词汇信息的权值：具体地，将高层特征作为输入通过注意力模块调节每一层词汇特征分布，利用这种方式来利用高级语义来完善嵌入单词的权重并解决潜在单词之间的冲突。</li></ul></li><li><p>【关于 CGN 】那些你不知道的事 </p><ul><li><p>动机</p><ul><li><p>中文命名实体识别中，词边界 问题；</p></li><li><p>如何 引入 词边界信息：</p><ul><li>pipeline：CWS -&gt; NER<ul><li>问题：误差传递</li></ul></li><li>CWS 和 NER 联合学习<ul><li>问题：标注 CWS 数据</li></ul></li><li>利用 词典 自动构建<ul><li>优点：比 CWS 标注数据 更容易获得</li><li>问题：<ul><li>第一个挑战是整合自我匹配的词汇词；<ul><li>举例：“北京机场” (Beijing Airport) and “机场” (Airport) are the self-matched words of the character “机” (airplane)</li></ul></li><li>第二个挑战是直接整合最接近的上下文词汇词；<ul><li>举例：by directly using the semantic knowledge of the nearest  contextual words “离开” (leave), an “I-PER” tag can be predicted instead  of an “I-ORG” tag, since “希尔顿” (Hilton Hotels) cannot be taken as the  subject of the verb “离开”</li></ul></li></ul></li></ul></li></ul></li><li><p>论文思路：</p><ul><li><p>character-based Collaborative Graph：</p><ul><li><p>encoding layer：</p><ul><li>句子信息：<ul><li>s1：将 char 表示为 embedding;</li><li>s2：利用 biLSTM 捕获 上下文信息</li></ul></li><li>lexical words 信息：<ul><li>s1：将 lexical word 表示为 embedding;</li></ul></li><li>合并 contextual representation 和 word embeddings</li></ul></li><li><p>a graph layer：</p><ul><li><p>Containing graph (C-graph):</p><ul><li>思路：字与字之间无连接，词与其inside的字之间有连接；</li><li>目的：帮助 字符 捕获 self-matched lexical words 的边界和语义信息</li></ul></li><li><p>Transition graph (T-graph):</p><ul><li>思路：相邻字符相连接，词与其前后字符连接；</li><li>目的：帮助 字符 捕获 相邻 上下文 lexical 词 的 语义信息</li></ul></li><li><p>Lattice graph (L-graph):</p><ul><li>思路：通相邻字符相连接，词与其开始结束字符相连；</li><li>目的：融合 lexical knowledge</li></ul></li><li><p>GAT:</p><ul><li><p>操作：针对三种图，使用Graph Attention Network(GAN)来进行编码。最终每个图的输出</p><ul><li><blockquote><p>其中 $G_k$ 为第k个图的GAN表示，因为是基于字符级的序列标注，所以解码时只关注字符，因此从矩阵中取出前n行作为最终的图编码层的输出。</p></blockquote></li></ul></li></ul></li></ul></li><li><p>a fusion layer：</p><ul><li>目的：融合 三种 graphs 中不同 的 lexical 知识</li></ul></li><li><p>a decoding layer:</p><ul><li>操作：利用 CRF 解码</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>【关于 LGN 】那些你不知道的事 </p><ul><li>动机：<ul><li>在 char-base Chinese NER 中，同一个字符可能属于多个 lexicon word，存在 overlapping ambiguity 问题<ul><li>举例(如下图)<ul><li>字符[流] 可以 匹配词汇 [河流] 和 [流经] 两个词汇信息，但是 Lattice LSTM 只能利用 [河流]；</li></ul></li></ul></li><li>Lattice LSTM这种RNN结构仅仅依靠前一步的信息输入，而不是利用全局信息<ul><li>举例<ul><li>字符 [度]只能看到前序信息，不能充分利用 [印度河] 信息，从而造成标注冲突问题</li></ul></li></ul></li><li>Ma等人于2014年提出，想解决overlapping across strings的问题，需要引入「整个句子中的上下文」以及「来自高层的信息」；然而，现有的基于RNN的序列模型，不能让字符收到序列方向上 remain characters 的信息；</li></ul></li><li>方法：<ul><li>Graph Construction and Aggregation</li><li>Graph Construction</li><li>Local Aggregation</li><li>Global Aggregation</li><li>Recurrent-based Update Module</li></ul></li></ul></li><li><p>【关于 FLAT】 那些你不知道的事</p><ul><li>动机<ul><li>方法一：设计一个动态框架，能够兼容词汇输入；<ul><li>代表模型：<ul><li>Lattice LSTM：利用额外的单词单元编码可能的单词，并利用注意力机制融合每个位置的变量节点</li><li>LR-CNN：采用不同窗口大小的卷积核来编码 潜在词</li></ul></li><li>问题：<ul><li>RNN 和 CNN 难以解决长距离依赖问题，它对于 NER 是有用的，例如： coreference（共指）</li><li>无法充分利用 GPU 的并行计算能力</li></ul></li></ul></li><li>方法二：将 Lattice 转化到图中并使用 GNN 进行编码：<ul><li>代表模型<ul><li>Lexicon-based GN(LGN)</li><li>Collaborative GN(CGN)</li></ul></li><li>问题<ul><li>虽然顺序结构对于NER仍然很重要，并且 Graph 是一般的对应物，但它们之间的差距不可忽略;</li><li>需要使用 LSTM 作为底层编码器，带有顺序感性偏置，使模型变得复杂。</li></ul></li></ul></li></ul></li><li>方法：将Lattice结构展平，将其从一个有向无环图展平为一个平面的Flat-Lattice Transformer结构，由多个span构成：每个字符的head和tail是相同的，每个词汇的head和tail是skipped的。</li></ul></li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/ACL2019/">【关于 ACL 2019 中的NER】那些你不知道的事</a></p><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/NER_study/ACL2019/JointER/">named entity recognition using positive-unlabeled learning</a></li><li>【关于 GraphRel】那些你不知道的事<ul><li>论文名称：GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/ACL2019/Fine-GrainedEntityTypinginHyperbolicSpace/">Fine-Grained Entity Typing in Hyperbolic Space（在双曲空间中打字的细粒度实体）</a></li><li>【关于 TENER】那些你不知道的事<ul><li>论文名称：TENER: Adapting Transformer Encoder for Name Entity Recognition</li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/EMNLP2019/">【关于 EMNLP 2019 中的NER】那些你不知道的事</a></p><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/EMNLP2019/CrossWeigh%E4%BB%8E%E4%B8%8D%E5%AE%8C%E5%96%84%E7%9A%84%E6%B3%A8%E9%87%8A%E4%B8%AD%E8%AE%AD%E7%BB%83%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E6%A0%87%E6%B3%A8%E5%99%A8/">CrossWeigh从不完善的注释中训练命名实体标注器</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/EMNLP2019/%E5%88%A9%E7%94%A8%E8%AF%8D%E6%B1%87%E7%9F%A5%E8%AF%86%E9%80%9A%E8%BF%87%E5%8D%8F%E5%90%8C%E5%9B%BE%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">利用词汇知识通过协同图网络进行中文命名实体识别</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NER_study/EMNLP2019/%E4%B8%80%E7%82%B9%E6%B3%A8%E9%87%8A%E5%AF%B9%E5%BC%95%E5%AF%BC%E4%BD%8E%E8%B5%84%E6%BA%90%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E5%99%A8%E6%9C%89%E5%BE%88%E5%A4%9A%E5%A5%BD%E5%A4%84/">一点注释对引导低资源命名实体识别器有很多好处</a></li></ul></li></ul><h6 id="-9"><a href="#-9" class="headerlink" title=""></a></h6><h6 id="【关于-关系抽取】那些你不知道的事"><a href="#【关于-关系抽取】那些你不知道的事" class="headerlink" title="【关于 关系抽取】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/">【关于 关系抽取】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/T2016_LSTM_Tree/">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures【2016】</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/NRE_paper_study/ERNIE/">【关于 ERNIE】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/GraphRel/">【关于 GraphRel】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/R_BERT">【关于 R_BERT】那些你不知道的事</a></li><li>【关于 Task 1：全监督学习】那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/T1_FullySupervisedLearning/T1_Relation_Classification_via_CDNN/">Relation Classification via Convolutional Deep Neural Network</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/T1_FullySupervisedLearning/T2_Attention-Based_BiLSTM_for_RC/">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/information_extraction/NRE_paper_study/T1_FullySupervisedLearning/T3_RC_via_attention_model_new/">Relation Classification via Attention Model</a></li></ul></li><li>【关于 Task 2：远程监督学习】那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/T2_DistantSupervisedLearning/T1_Piecewise_Convolutional_Neural_Networks/">Relation Classification via Convolutional Deep Neural Network</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/NRE_paper_study/T2_DistantSupervisedLearning/T2_NRE_with_Selective_Attention_over_Instances/">NRE_with_Selective_Attention_over_Instances</a></li></ul></li></ul><h6 id="-10"><a href="#-10" class="headerlink" title=""></a></h6><h6 id="【关于-文档级别关系抽取】那些你不知道的事"><a href="#【关于-文档级别关系抽取】那些你不知道的事" class="headerlink" title="【关于 文档级别关系抽取】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/Doc-level_Relation_Extraction/">【关于 文档级别关系抽取】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/Doc-level_Relation_Extraction/DoubleGraphBasedReasoningforDocumentlevelRelationExtraction/">【关于 Double Graph Based Reasoning for Document-level Relation Extraction】 那些的你不知道的事</a></li><li>【关于 ATLOP】 那些的你不知道的事<ul><li>论文：Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling</li><li>发表会议：AAAI</li><li>论文地址：<a href="https://arxiv.org/abs/2010.11304">https://arxiv.org/abs/2010.11304</a></li><li>github：<a href="https://github.com/wzhouad/ATLOP">https://github.com/wzhouad/ATLOP</a></li><li>论文动机：<ul><li>对于文档级RE，一个文档包含多个实体对，需要同时对它们之间的关系进行分类 【语句级RE只包含一对实体对】</li><li>对于文档级RE，一个实体对可以在与不同关系关联的文档中多次出现【对于句子级RE，每个实体对只能出现一个关系】 -&gt; 多标签问题</li><li>目前对于文档关系抽取主流的做法是采用基于graph的方法来做，但是很多基于BERT的工作也能够得到很好的结果，并且在基于graph的模型的实验部分，也都证明了BERT以及BERT-like预训练模型的巨大提升，以至于让人怀疑是否有必要引入GNN？作者发现如果只用BERT的话，那么对于不同的entity pair，entity的rep都是一样的，这是一个很大的问题，那是否能够不引入graph的方式来解决这个问题呢？</li></ul></li><li>论文方法：<ul><li>localized context pooling<ul><li>解决问题：解决了 using the same entity embedding for allentity pairs 问题</li><li>方法：使用与当前实体对相关的额外上下文来增强 entity embedding。不是从头开始训练一个new context attention layer ，而是直接将预先训练好的语言模型中的注意头转移到实体级的注意上</li></ul></li><li>adaptive thresholding<ul><li>解决问题：问题 1 的 多实体对问题 和 问题 2 实体对存在多种关系问题</li><li>方法：替换为先前学习中用于多标签分类的全局阈值，该阈值为<strong>可学习的依赖实体的阈值</strong>。</li></ul></li></ul></li></ul></li></ul><h6 id="-11"><a href="#-11" class="headerlink" title=""></a></h6><h6 id="【关于-事件抽取】那些你不知道的事"><a href="#【关于-事件抽取】那些你不知道的事" class="headerlink" title="【关于 事件抽取】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/event_extraction/">【关于 事件抽取】那些你不知道的事</a></h6><ul><li>【关于 MLBiNet】那些你不知道的事<ul><li>论文：MLBiNet: A Cross-Sentence Collective Event Detection Network</li><li>会议： ACL2021</li><li>论文下载地址：<a href="https://arxiv.org/pdf/2105.09458.pdf">https://arxiv.org/pdf/2105.09458.pdf</a></li><li>论文代码：<a href="https://github.com/zjunlp/DocED">https://github.com/zjunlp/DocED</a></li><li>动机：跨句事件抽取旨在研究如何同时识别篇章内多个事件</li><li>论文方法：论文将其重新表述为 <strong>Seq2Seq 任务</strong>，并提出了一个多层双向网络 (Multi-Layer Bidirectional Network，MLBiNet) 来 <strong>融合跨句语义和关联事件信息，从而增强内各事件提及的判别</strong></li><li>论文思路： 在解码事件标签向量序列时<ul><li>首先，为建模句子内部事件关系，我们提出双向解码器用于同时捕捉前向和后向事件依赖；</li><li>然后，利用信息聚合器汇总句子语义和事件提及信息；</li><li>最后，通过迭代多个由双向解码器和信息聚合器构造的单元，并在每一层传递邻近句子的汇总信息，最终感知到整个文档的语义和事件提及信息。</li></ul></li></ul></li></ul><h6 id="-12"><a href="#-12" class="headerlink" title=""></a></h6><h6 id="【关于-关键词提取】-那些你不知道的事"><a href="#【关于-关键词提取】-那些你不知道的事" class="headerlink" title="【关于 关键词提取】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/keyword_ex_study/">【关于 关键词提取】 那些你不知道的事</a></h6><ul><li>【关于 关键词提取】 那些你不知道的事<ul><li>一、TF-IDF关键词提取算法<ul><li>1.1 理论基础</li><li>1.2 计算公式<ul><li>1.2.1 词频 （Term Frequency，TF）</li><li>1.2.2 逆文本频率(Inverse Document Frequency，IDF)</li><li>1.2.3 TF-IDF</li></ul></li><li>1.3 应用</li><li>1.4 实战篇<ul><li>1.4.1 TF-IDF算法 手撸版</li><li>1.4.2 TF-IDF算法 Sklearn 版</li><li>1.4.3 TF-IDF算法 jieba 版</li></ul></li></ul></li><li>二、PageRank算法【1】<ul><li>2.1 理论学习</li></ul></li><li>三、TextRank算法【2】<ul><li>3.1 理论学习</li><li>3.2 实战篇<ul><li>3.2.1 基于Textrank4zh的TextRank算法版</li><li>3.2.2 基于jieba的TextRank算法实现</li><li>3.2.3 基于SnowNLP的TextRank算法实现</li></ul></li></ul></li></ul></li><li>【关于 KeyBERT 】 那些你不知道的事<ul><li>论文：Sharma, P., &amp; Li, Y. (2019). Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling.</li><li>论文地址：<a href="https://www.preprints.org/manuscript/201908.0073/download/final_file">https://www.preprints.org/manuscript/201908.0073/download/final_file</a></li><li>论文代码：<a href="https://github.com/MaartenGr/KeyBERT">https://github.com/MaartenGr/KeyBERT</a></li><li>一、摘要</li><li>二、动机</li><li>三、论文方法</li><li>四、实践<ul><li>4.1 安装</li><li>4.2 KeyBERT 调用</li><li>4.3 语料预处理</li><li>4.4 利用 KeyBert 进行关键词提取</li></ul></li></ul></li><li>【关于 One2Set 】 那些你不知道的事<ul><li>论文名称：One2Set: Generating Diverse Keyphrases as a Set</li><li>论文：<a href="https://aclanthology.org/2021.acl-long.354/">https://aclanthology.org/2021.acl-long.354/</a></li><li>代码：<a href="https://github.com/jiacheng-ye/kg_one2set">https://github.com/jiacheng-ye/kg_one2set</a></li><li>会议：ACL2021</li></ul></li></ul><h6 id="-13"><a href="#-13" class="headerlink" title=""></a></h6><h6 id="【关于-新词发现】-那些你不知道的事"><a href="#【关于-新词发现】-那些你不知道的事" class="headerlink" title="【关于 新词发现】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/word_discovery/">【关于 新词发现】 那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/information_extraction/word_discovery/">【关于 新词发现】 那些你不知道的事</a></li><li>【关于 AutoPhrase】那些你不知道的事<ul><li>论文：AutoPhrase: Automated Phrase Mining from Massive Text Corpora</li><li>会议：IEEE</li><li>论文地址：<a href="https://arxiv.org/abs/1702.04457">https://arxiv.org/abs/1702.04457</a></li><li>源码 Python 版本：<a href="https://github.com/luozhouyang/AutoPhraseX">https://github.com/luozhouyang/AutoPhraseX</a></li><li>什么是 Phrase Mining？<ul><li>答：Phrase Mining 作为文本分析的基本任务之一，旨在从文本语料库中提取高质量的短语。</li></ul></li><li>hrase Mining 有何用途？<ul><li>短语挖掘在各种任务中都很重要，例如信息提取/检索、分类法构建和主题建模。</li></ul></li><li>Phrase Mining 现状？<ul><li>大多数现有方法依赖于复杂的、训练有素的语言分析器，因此在没有额外但昂贵的适应的情况下，可能在新领域和流派的文本语料库上表现不佳。虽然也有一些数据驱动的方法来从大量特定领域的文本中提取短语。</li></ul></li><li>Phrase Mining 存在问题？<ol><li>非 自动化</li><li>需要人类专家来设计规则或标记短语</li><li>依赖于 语言分析器</li><li>应用到新的领域效果不好</li></ol></li><li>论文方法 ？<ol><li>Robust Positive-Only Distant  Training：使用wiki和freebase作为显眼数据，根据知识库中的相关数据构建Positive  Phrases,根据领域内的文本生成Negative Phrases，构建分类器后根据预测的结果减少负标签带来的噪音问题。</li><li>POS-Guided Phrasal Segmentation：使用POS词性标注的结果，引导短语分词，利用POS的浅层句法分析的结果优化Phrase boundaries。</li></ol></li><li>论文效果 ？<ul><li>AutoPhrase可以支持任何语言，只要该语言中有通用知识库。与当下最先进的方法比较，新方法在跨不同领域和语言的5个实际数据集上的有效性有了显著提高。</li></ul></li></ul></li></ul><h5 id="-14"><a href="#-14" class="headerlink" title=""></a></h5><h5 id="【关于-知识图谱-】-那些的你不知道的事"><a href="#【关于-知识图谱-】-那些的你不知道的事" class="headerlink" title="【关于 知识图谱 】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/">【关于 知识图谱 】 那些的你不知道的事</a></h5><ul><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/">【关于 知识图谱 】 那些的你不知道的事</a></p><ul><li>一、知识图谱简介<ul><li>1.1 引言</li><li>1.2 什么是知识图谱呢？<ul><li>1.2.1 什么是图（Graph）呢？</li><li>1.2.2 什么是 Schema 呢？</li></ul></li><li>1.3 知识图谱的类别有哪些？</li><li>1.4 知识图谱的价值在哪呢？</li></ul></li><li>二、怎么构建知识图谱呢？<ul><li>2.1 知识图谱的数据来源于哪里？</li><li>2.2 信息抽取的难点在哪里？</li><li>2.3 构建知识图谱所涉及的技术？</li><li>2.4、知识图谱的具体构建技术是什么？<ul><li>2.4.1 实体命名识别（Named Entity Recognition）</li><li>2.4.2 关系抽取（Relation Extraction）</li><li>2.4.3 实体统一（Entity Resolution）</li><li>2.4.4 指代消解（Disambiguation）</li></ul></li></ul></li><li>三、知识图谱怎么存储？</li><li>四、知识图谱可以做什么？</li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/%E7%88%B1%E5%A5%87%E8%89%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">爱奇艺知识图谱落地实践</a></p><ul><li>原文地址：<a href="https://mp.weixin.qq.com/s?__biz=MzU2NjAxNDYwMg==&mid=2247493658&idx=1&sn=cc2d7f82aa5c5a138dc7f267aaa26c16&chksm=fcb04fffcbc7c6e9575f5dcc3d1619425e4dd7fc0737976991bff687af6b1dadcdd20f906d04&mpshare=1&scene=22&srcid=0809aEdiDZ0DSoxjfzSF8Kcf&sharer_sharetime=1628471753625&sharer_shareid=da84f0d2d31380d783922b9e26cacfe2#rd">领域应用 | 完备的娱乐行业知识图谱库如何建成？爱奇艺知识图谱落地实践</a></li><li>构建流程<ul><li>知识表示和建模：自顶向下的建模方式</li><li>数据模式（schema）定义方式：<ul><li>基于 RDF(Resource Description Framework) 三元组</li><li>RDFS（RDF Schema） 的规则</li></ul></li><li>知识获取<ul><li>实体分类<ul><li>动机：主要<strong>针对百度百科的数据</strong>，因为<strong>百度百科的数据没有类别信息</strong></li><li>思路：<ul><li>构建基于规则池的分类器，生成训练数据，训练DNN模型（self-attention）文本分类模型；</li><li>DNN分类器与规则分类器互相扩充迭代（一到两轮），最终线上使用规则分类器。</li></ul></li></ul></li><li>实体抽取<ul><li>目标：从数据中的识别和抽取实体的属性与关系信息</li></ul></li><li>知识融合<ul><li>流程：<ul><li>首先我们所有来源的实体数据都会进入原始实体库（RawEntity库），并且对原始表中的数据建立索引。</li><li>当一个原始实体rawEntity入最终实体库之前，要在原始实体库中找是否有其它原始实体和rawEntity实际上是同一个实体。步骤：<ul><li>首先在索引中根据名字、别名等字段查询出若干个可能是相同实体的候选列表，这个步骤的目的是减少接下来流程的计算量。</li><li>然后经过实体判别模型，根据模型得分识别出待合并对齐的原始实体；</li><li>最后经过属性融合模型，将各原始实体的属性字段进行融合，生成最终的实体。</li></ul></li><li>这个流程中的合并判断模型实际上是通过机器学习训练生成的二分类器。</li></ul></li></ul></li><li>知识存储</li></ul></li></ul></li></ul></li><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/ComplexKBQA/">【关于 Complex KBQA】 那些你不知道的事</a></p><ul><li><p>论文：A Survey on Complex Knowledge Base Question Answering:Methods, Challenges and Solutions</p></li><li><p>会议：IJCAI’2021</p></li><li><p>论文地址：<a href="https://www.ijcai.org/proceedings/2021/0611.pdf">https://www.ijcai.org/proceedings/2021/0611.pdf</a></p></li><li><p>动机：</p><ul><li><p>相比仅包含单个关系事实的简单问题，复杂问题通常有以下几个特征</p><ul><li><strong>需要在知识图谱中做多跳推理 (multi-hop reasoning)</strong></li><li><strong>需要考虑题目中给的限制词 (constrained relations)</strong></li><li><strong>需要考虑数字运算的情况 (numerical operations)</strong></li></ul></li><li><p>基于语义解析的方法还是信息检索的方法都将遇到新的挑战</p><p>：</p><ul><li><strong>传统方法无法支撑问题的复杂逻辑</strong></li><li><strong>复杂问题包含了更多的实体，导致在知识图谱中搜索空间变大</strong></li><li><strong>两种方法都将问题理解作为首要步骤</strong></li><li><strong>通常 Complex KBQA 数据集缺少对正确路径的标注</strong></li></ul></li></ul></li><li><p>预测答案两类主流的方法</p><ul><li>基于语义解析（SP-based）的方法<ul><li>问题理解 (question understanding) 模块</li><li>逻辑解析 (logical parsing) 模块</li><li>知识图谱实例化 (KB grounding) 模块</li><li>知识执行 (KB execution) 模块</li></ul></li><li>基于信息检索（IR-based）的方法<ul><li>子图构建 (retrieval source construction) 模块</li><li>问题表达 (question representation) 模块</li><li>基于图结构的推理 (graph based reasoning) 模块</li><li>答案排序 (answer ranking) 模块</li></ul></li></ul></li></ul></li></ul><h6 id="-15"><a href="#-15" class="headerlink" title=""></a></h6><h6 id="【关于-实体链指篇】-那些的你不知道的事"><a href="#【关于-实体链指篇】-那些的你不知道的事" class="headerlink" title="【关于 实体链指篇】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/entity_linking/">【关于 实体链指篇】 那些的你不知道的事</a></h6><ul><li>【关于  Low-resource Cross-lingual Entity Linking】 那些你不知道的事<ul><li>论文名称：Design Challenges in Low-resource Cross-lingual Entity Linking</li><li>论文地址：<a href="https://arxiv.org/pdf/2005.00692.pdf">https://arxiv.org/pdf/2005.00692.pdf</a></li><li>来源：EMNLP 2020</li></ul></li><li>【关于  GENRE】 那些你不知道的事<ul><li>论文名称：AUTOREGRESSIVE ENTITY RETRIEVAL</li><li>论文地址：<a href="https://openreview.net/pdf?id=5k8F6UU39V">https://openreview.net/pdf?id=5k8F6UU39V</a></li><li>来源：ICLR 2021</li><li>论文代码：<a href="https://github.com/facebookresearch/GENRE">https://github.com/facebookresearch/GENRE</a></li><li>介绍：实体是我们表示和聚合知识的中心。例如，维基百科等百科全书是由实体构成的（例如，一篇维基百科文章）。检索给定查询的实体的能力是知识密集型任务（如实体链接和开放域问答）的基础。理解当前方法的一种方法是将分类器作为一个原子标签，每个实体一个。它们的权重向量是通过编码实体元信息（如它们的描述）产生的密集实体表示。</li><li>缺点：<ul><li>（i）上下文和实体的亲和力主要是通过向量点积来获取的，可能会丢失两者之间的细粒度交互；</li><li>（ii）在考虑大型实体集时，需要大量内存来存储密集表示；</li><li>（iii）必须在训练时对一组适当硬的负面数据进行二次抽样[。</li></ul></li><li>工作内容介绍：在这项工作中，我们提出了第一个 GENRE，通过生成其唯一的名称，从左到右，token-by-token 的自回归方式和条件的上下文。</li><li>这使得我们能够缓解上述技术问题，<ul><li>（i）自回归公式允许我们直接捕获文本和实体名称之间的关系，有效地交叉编码两者 ；</li><li>（ii）由于我们的编码器-解码器结构的参数随词汇表大小而不是词汇量大小而缩放，因此内存足迹大大减少实体计数；</li><li>（iii）准确的softmax损失可以有效地计算，而无需对负数据进行子采样。</li></ul></li><li>实验结果：我们展示了该方法的有效性，在实体消歧、端到端实体链接和文档检索任务上对20多个数据集进行了实验，在使用竞争系统内存占用的一小部分的情况下，获得了最新的或非常有竞争力的结果。他们的实体，我们只需简单地指定新的名称，就可以添加</li></ul></li></ul><h6 id="-16"><a href="#-16" class="headerlink" title=""></a></h6><h6 id="【关于-实体消歧-】-那些的你不知道的事"><a href="#【关于-实体消歧-】-那些的你不知道的事" class="headerlink" title="【关于 实体消歧 】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/EntityDisambiguation/">【关于 实体消歧 】 那些的你不知道的事</a></h6><ul><li>【关于 DeepType 】 那些的你不知道的事<ul><li>论文：DeepType: Multilingual Entity Linking by Neural Type System Evolution</li><li>论文地址：<a href="https://arxiv.org/abs/1802.01021">https://arxiv.org/abs/1802.01021</a></li><li>github：<a href="https://github.com/openai/deeptype">https://github.com/openai/deeptype</a></li></ul></li></ul><h6 id="-17"><a href="#-17" class="headerlink" title=""></a></h6><h6 id="【关于KGQA-】-那些的你不知道的事"><a href="#【关于KGQA-】-那些的你不知道的事" class="headerlink" title="【关于KGQA 】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/KGQA/">【关于KGQA 】 那些的你不知道的事</a></h6><ul><li>【关于KGQA 】 那些的你不知道的事<ul><li>一、基于词典和规则的方法</li><li>二、基于信息抽取的方法</li></ul></li><li>【关于 Multi-hopComplexKBQA 】 那些你不知道的事<ul><li>论文：Lan Y, Jiang J. Query Graph Generation for Answering Multi-hop  Complex Questions from Knowledge Bases[C]//Proceedings of the 58th  Annual Meeting of the Association for Computational Linguistics. 2020:  969-974.</li><li>会议：ACL2020</li><li>链接：<a href="https://www.aclweb.org/anthology/2020.acl-main.91/">https://www.aclweb.org/anthology/2020.acl-main.91/</a></li><li>代码：<a href="https://github.com/lanyunshi/Multi-hopComplexKBQA">https://github.com/lanyunshi/Multi-hopComplexKBQA</a></li></ul></li></ul><h6 id="-18"><a href="#-18" class="headerlink" title=""></a></h6><h6 id="【关于Neo4j-】-那些的你不知道的事"><a href="#【关于Neo4j-】-那些的你不知道的事" class="headerlink" title="【关于Neo4j  】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/neo4j/">【关于Neo4j  】 那些的你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/neo4j/">【关于Neo4j】 那些的你不知道的事</a><ul><li>一、Neo4J 介绍与安装<ul><li>1.1 引言</li><li>1.2 Neo4J 怎么下载？</li><li>1.3 Neo4J 怎么安装？</li><li>1.4 Neo4J Web 界面 介绍</li><li>1.5 Cypher查询语言是什么？</li></ul></li><li>二、Neo4J 增删查改篇<ul><li>2.1 引言</li><li>2.2 Neo4j 怎么创建节点？</li><li>2.3 Neo4j 怎么创建关系？</li><li>2.4 Neo4j 怎么创建 出生地关系？</li><li>2.5 Neo4j 怎么查询？</li><li>2.6 Neo4j 怎么删除和修改？</li></ul></li><li>三、如何利用 Python 操作 Neo4j 图数据库？<ul><li>3.1 neo4j模块：执行CQL ( cypher ) 语句是什么？</li><li>3.2 py2neo模块是什么？</li></ul></li><li>四、数据导入 Neo4j 图数据库篇</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/KG_study/neo4j/index.md">【关于 Neo4j 索引】那些你不知道的事</a></li></ul><h5 id="-19"><a href="#-19" class="headerlink" title=""></a></h5><h5 id="【关于-NLP-Trick】-那些你不知道的事"><a href="#【关于-NLP-Trick】-那些你不知道的事" class="headerlink" title="【关于 NLP Trick】 那些你不知道的事"></a>【关于 NLP Trick】 那些你不知道的事</h5><h6 id="-20"><a href="#-20" class="headerlink" title=""></a></h6><h6 id="【关于-Dropout】-那些你不知道的事"><a href="#【关于-Dropout】-那些你不知道的事" class="headerlink" title="【关于 Dropout】 那些你不知道的事"></a>【关于 Dropout】 那些你不知道的事</h6><ul><li>【关于 R-Drop】 那些你不知道的事<ul><li>论文：R-Drop: Regularized Dropout for Neural Networks</li><li>论文下载地址：<a href="https://arxiv.org/abs/2106.14448">https://arxiv.org/abs/2106.14448</a></li><li>论文代码：<a href="https://github.com/dropreg/R-Drop">https://github.com/dropreg/R-Drop</a></li><li>论文动机：<ul><li>由于深度神经网络非常容易过拟合，因此 Dropout  方法采用了随机丢弃每层的部分神经元，以此来避免在训练过程中的过拟合问题。<strong>正是因为每次随机丢弃部分神经元，导致每次丢弃后产生的子模型都不一样，所以 Dropout 的操作一定程度上使得训练后的模型是一种多个子模型的组合约束。</strong>基于 Dropout  的这种特殊方式对网络带来的随机性，研究员们提出了 R-Drop 来进一步对（子模型）网络的输出预测进行了正则约束。</li></ul></li><li>论文方法：与传统作用于神经元（Dropout）或者模型参数（DropConnect）上的约束方法不同，R-Drop <strong>作用于模型的输出层</strong>，弥补了 Dropout 在训练和测试时的不一致性。简单来说就是在每个 mini-batch 中，<strong>每个数据样本过两次带有 Dropout 的同一个模型，R-Drop 再使用 KL-divergence 约束两次的输出一致</strong>。</li><li>作用：<strong>R-Drop 约束了由于 Dropout 带来的两个随机子模型的输出一致性</strong>。</li></ul></li></ul><h6 id="-21"><a href="#-21" class="headerlink" title=""></a></h6><h6 id="【关于-主动学习】-那些的你不知道的事"><a href="#【关于-主动学习】-那些的你不知道的事" class="headerlink" title="【关于 主动学习】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/active_learn_study/">【关于 主动学习】 那些的你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/DL_algorithm/active_learn_study/ProactiveLearningforNamedEntityRecognition/">【关于 Proactive Learning for Named Entity Recognition（命名实体识别的主动学习）】 那些的你不知道的事</a></li></ul><h6 id="-22"><a href="#-22" class="headerlink" title=""></a></h6><h6 id="【关于-对抗训练】-那些的你不知道的事"><a href="#【关于-对抗训练】-那些的你不知道的事" class="headerlink" title="【关于 对抗训练】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/adversarial_training_study/">【关于 对抗训练】 那些的你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/adversarial_training_study/">【关于 生成对抗网络 GAN 】 那些的你不知道的事</a></li><li>【关于 FreeLB 】 那些的你不知道的事<ul><li>论文名称: FreeLB: Enhanced Adversarial Training for Language Understanding 加强语言理解的对抗性训练</li><li>动机：对抗训练使保留标签的输入扰动的最大风险最小，对于提高语言模型的泛化能力是有效的。</li><li>方法：提出了一种新的对抗性训练算法—— freeb，它通过在字嵌入中添加对抗性的干扰，最小化输入样本周围不同区域内的对抗性风险，从而提高嵌入空间的鲁棒性和不变性。</li></ul></li></ul><h6 id="-23"><a href="#-23" class="headerlink" title=""></a></h6><h6 id="【关于-文本预处理】-那些的你不知道的事"><a href="#【关于-文本预处理】-那些的你不知道的事" class="headerlink" title="【关于 文本预处理】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/NLP_tools/pre_study/samplingStudy/">【关于 文本预处理】 那些的你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/NLP_tools/pre_study/samplingStudy/samplingStudy">【关于 过采样】 那些的你不知道的事</a></li></ul><h6 id="-24"><a href="#-24" class="headerlink" title=""></a></h6><h6 id="【关于-半监督学习】-那些的你不知道的事"><a href="#【关于-半监督学习】-那些的你不知道的事" class="headerlink" title="【关于 半监督学习】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/Unsupervised/">【关于 半监督学习】 那些的你不知道的事</a></h6><ul><li>Unsupervised Data Augmentation (UDA)<ul><li>【关于 UDA】 那些你不知道的事<ul><li>阅读理由：UDA（Unsupervised Data Augmentation 无监督数据增强）是Google在2019年提出的半监督学习算法。该算法超越了所有现有的半监督学习方法，并实现了仅使用极少量标记样本即可达到使用大量标记样本训练集的精度。</li><li>动机： 深度学习的模型训练通常依赖大量的标签数据，在只有少量数据上通常表现不好;</li><li>思路：提出了一种基于无监督数据的数据增强方式UDA（Unsupervised Data  Augmentation）。UDA方法生成无监督数据与原始无监督数据具备分布的一致性，而以前的方法通常只是应用高斯噪声和dropout噪声（无法保证一致性）。UDA方法利用了一种目前为止最优的方法生成更加“真实”的数据。</li><li>优点：使用这种数据增强方法，在极少量数据集上，六种语言任务和三种视觉任务都得到了明显的提升。</li></ul></li></ul></li><li>【关于 “脏数据”处理】那些你不知道的事<ul><li>一、动机<ul><li>1.1 何为“脏数据”？</li><li>1.2 “脏数据” 会带来什么后果？</li></ul></li><li>二、“脏数据” 处理篇<ul><li>2.1 “脏数据” 怎么处理呢？</li><li>2.2 置信学习方法篇<ul><li>2.2.1 什么是 置信学习方法？</li><li>2.2.2 置信学习方法 优点？</li><li>2.2.3 置信学习方法 怎么做？</li><li>2.2.4 置信学习方法 怎么用？有什么开源框架？</li><li>2.2.5 置信学习方法 的工作原理？</li></ul></li></ul></li></ul></li></ul><h6 id="-25"><a href="#-25" class="headerlink" title=""></a></h6><h6 id="【关于-GCN-in-NLP-】那些你不知道的事"><a href="#【关于-GCN-in-NLP-】那些你不知道的事" class="headerlink" title="【关于 GCN in NLP 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/trick/GNN/GCN2NLP/">【关于 GCN in NLP 】那些你不知道的事</a></h6><ul><li>【关于 GCN in NLP 】那些你不知道的事<ul><li>构图方法：<ul><li>句法依赖树；</li><li>TF-IDF;</li><li>PMI;</li><li>序列关系；</li><li>词典</li></ul></li></ul></li></ul><h5 id="-26"><a href="#-26" class="headerlink" title=""></a></h5><h5 id="【关于-问答系统】-那些的你不知道的事"><a href="#【关于-问答系统】-那些的你不知道的事" class="headerlink" title="【关于 问答系统】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/">【关于 问答系统】 那些的你不知道的事</a></h5><h6 id="-27"><a href="#-27" class="headerlink" title=""></a></h6><h6 id="【关于-FAQ-】那些你不知道的事"><a href="#【关于-FAQ-】那些你不知道的事" class="headerlink" title="【关于 FAQ 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/">【关于 FAQ 】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/FAQ_trick/">【关于 FAQ Trick】那些你不知道的事</a><ul><li>一、动机<ul><li>1.1 问答系统的动机？</li><li>1.2 问答系统 是什么？</li></ul></li><li>二、FAQ 检索式问答系统介绍篇<ul><li>2.1 FAQ 检索式问答系统 是 什么？</li><li>2.2 query 匹配标准 QA 的核心是什么?</li></ul></li><li>三、FAQ 检索式问答系统 方案篇<ul><li>3.1 常用 方案有哪些？</li><li>3.2 为什么 QQ 匹配比较常用？<ul><li>3.2.1 QQ 匹配的优点有哪些？</li><li>3.2.2 QQ 匹配的语义空间是什么？</li><li>3.2.3 QQ 匹配的语料的稳定性是什么？</li><li>3.2.4 QQ 匹配的业务回答与算法模型的解耦是什么？</li><li>3.2.5 QQ 匹配的新问题发现与去重是什么？</li><li>3.2.6 QQ 匹配的上线运行速度是什么？</li></ul></li><li>3.3  QQ 匹配一般处理流程是怎么样？ 【假设 标准问题库 已处理好】</li></ul></li><li>四、FAQ 标准问题库构建篇<ul><li>4.1 如何发现 FAQ 中标准问题？</li><li>4.2 FAQ 如何做拆分？</li><li>4.3 FAQ 如何做合并？</li><li>4.4 FAQ 标准库如何实时更新？</li><li>4.5 FAQ 知识库搭建原则有哪些？</li><li>4.6 FAQ 知识库应该具备哪些特点？</li><li>4.7 FAQ 知识库应该怎么从零构建？</li><li>4.8 FAQ 标准问题库答案如何优化？</li><li>4.9 FAQ 怎样发现未能解决客户的问题？</li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/Industry-scaleQAS/">【关于 Robust Industry-scale Question Answering System】 那些你不知道的事</a><ul><li>论文：Towards building a Robust Industry-scale Question Answering System -论文地址：<a href="https://www.aclweb.org/anthology/2020.coling-industry.9.pdf">https://www.aclweb.org/anthology/2020.coling-industry.9.pdf</a></li><li>会议：COLING 2020</li><li>工业规模的 NLP 系统需要两个功能。<ul><li><ol><li>鲁棒性：“零样本迁移学习”(ZSTL) 的性能值得称道；</li></ol></li><li><ol><li>效率：系统必须高效训练并即时响应。</li></ol></li></ul></li><li>论文方法：介绍了一种称为GAAMA（Go Ahead Ask Me Anything）的生产模型的发展，它具有上述两个特征：<ul><li>为了稳健性，它在最近引入的Natural Questions（NQ）数据集上进行训练。 NQ 对 SQuAD 等旧数据集提出了额外的挑战：<ul><li>(a) QA 系统需要阅读和理解整篇 Wikipedia 文章而不是一小段文章；</li><li>(b) NQ 在构建过程中不会受到观察偏差的影响，从而减少问题和问题之间的词汇重叠文章。</li></ul></li><li>GAAMA 由Attention-over-Attention、注意力头的多样性、分层迁移学习和合成数据增强组成，同时计算成本低廉。</li></ul></li><li>实验结果：<ul><li>建立在强大的 BERTQA 模型之上，GAAMA 在 F1 中比 NQ 上的行业规模最先进 (SOTA) 系统提供了 2.0% 的绝对提升；</li><li>GAAMA 将零样本转移到了看不见的现实生活和重要领域，因为它在两个基准上产生了可观的性能：BioASQ 和新引入的 CovidQA 数据集。</li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/LCNQA/">【关于 LCNQA】那些你不知道的事</a><ul><li>论文名称：Lattice CNNs for Matching Based Chinese Question Answering</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/T1_LSTM-based_for_Non-factoid_Answer_Selection/">LSTM-based Deep Learning Models for Non-factoid Answer Selection</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/T4_DenoisingDistantlySupervisedODQA/">【关于 Denoising Distantly Supervised ODQA】那些你不知道的事</a><ul><li>论文名称：Denoising Distantly Supervised Open-Domain Question Answering</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/ACM2019_faq_bert-based_query-answer_relevance/">FAQ retrieval using query-question similarity and BERT-based query-answer relevance</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/FAQ/SIGIR2020_DCBert/">【DC-BERT】 那些的你不知道的事</a><ul><li>论文名称：DC-BERT : DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING</li><li>会议：SIGIR2020</li><li>常用方法：<ul><li>遵循“检索和读取”管道，并使用基于BERT的重新排序器对检索到的文档进行筛选，</li><li>然后再将其馈送到阅读器模块中。</li><li>BERT检索器将问题和每个检索到的文档的连接作为输入。</li></ul></li><li>问题：<ul><li>无法处理传入问题的高吞吐量，每个问题都有大量检索到的文档；</li></ul></li><li>论文方法：具有双重BERT模型的解耦上下文编码框架：<ul><li>一个在线BERT，仅对问题进行一次编码；</li><li>一个正式的BERT，对所有文档进行预编码并缓存其编码；</li></ul></li></ul></li></ul><h6 id="-28"><a href="#-28" class="headerlink" title=""></a></h6><h6 id="【关于-多轮检索-】那些你不知道的事"><a href="#【关于-多轮检索-】那些你不知道的事" class="headerlink" title="【关于 多轮检索 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/mulFAQ/">【关于 多轮检索 】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E5%A4%9A%E8%BD%AE%E6%A3%80%E7%B4%A2.xmind">【关于 文本匹配和多轮检索】那些你不知道的事</a></li><li>【关于 MulFAQ】那些你不知道的事<ul><li>【关于 MSN】 那些你不知道的事<ul><li>论文名称：Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based chatbots</li><li>论文地址：<a href="https://www.aclweb.org/anthology/D19-1011.pdf">https://www.aclweb.org/anthology/D19-1011.pdf</a></li><li>论文项目：<a href="https://github.com/chunyuanY/Dialogue">https://github.com/chunyuanY/Dialogue</a></li><li>动机：<ul><li><ol><li>上下文拼接问题：将候选回复与上下文utterance在多粒度级别进行匹配，这种方式忽略了使用过多的上下文信息带来副作用。</li></ol></li><li><ol><li>根据直接，一般来说距离回复越近的utterance，越能够反应最终轮对话的意图。所以，我们首先使用最后一个utterance作为key去选择word级别和sentence级别上相关的上下文回复。然而，我们发现<strong>许多样本中最后一个utterance都是一些短句并且很多都是无效信息</strong>（比如good, ok）</li></ol></li></ul></li><li>方法：提出一种多跳选择网络（multi-hop selector network, MSN）<ul><li>s1 ：采用 多跳选择器从 上下文集 中 选取最相关的上下文 utterances，并生成 k 个 不同的上下文；</li><li>s2 : 融合 k 个 上下文 utterance ，并与候选回复做匹配；</li><li>s3 : 匹配截取，采用 CNN 提取匹配特征，并 用 GRU 学习 utterance 间的临时关系；</li></ul></li></ul></li></ul></li></ul><h6 id="-29"><a href="#-29" class="headerlink" title=""></a></h6><h6 id="【关于-KBFAQ-】那些你不知道的事"><a href="#【关于-KBFAQ-】那些你不知道的事" class="headerlink" title="【关于 KBFAQ 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/KBFAQ/">【关于 KBFAQ 】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/KBFAQ/">【关于 KBFAQ】那些你不知道的事</a></li></ul><h5 id="-30"><a href="#-30" class="headerlink" title=""></a></h5><h5 id="【关于-对话系统】-那些的你不知道的事"><a href="#【关于-对话系统】-那些的你不知道的事" class="headerlink" title="【关于 对话系统】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/">【关于 对话系统】 那些的你不知道的事</a></h5><h6 id="-31"><a href="#-31" class="headerlink" title=""></a></h6><h6 id="【关于-自然语言理解-NLU】那些你不知道的事"><a href="#【关于-自然语言理解-NLU】那些你不知道的事" class="headerlink" title="【关于 自然语言理解 NLU】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLU/contextLU/">【关于 自然语言理解 NLU】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLU/contextLU/">【关于 上下文理解 contextLU】那些你不知道的事</a></li><li>【关于 DIET】 那些你不知道的事<ul><li>论文名称：DIET：Dual Intent and Entity Transformer</li><li>作者：RASA</li><li>论文代码：<a href="https://github.com/cheesama/DIET-pytorch">https://github.com/cheesama/DIET-pytorch</a> 【韩文】</li><li>动机：<ul><li>大规模的预训练语言模型在 GLUE 和 SuperGLUE 等语言理解基准上显示出令人印象深刻的结果，与分布式表示 (GloVe) 和纯监督方法等其他预训练方法相比有了显着改善。</li></ul></li><li>论文方法：我们介绍了 the Dual Intent and Entity Transformer (DIET) (DIET) 架构（基于两个任务共享的Transformer）：<ul><li>1 实体标签序列通过Transformer后，输出序列进入顶层条件随机场（CRF）标记层预测，输出每个Token成为BIOE的概率；</li><li>2 完整话语和意图标签经过Transformer输出到单个语义向量空间中；</li><li>3 利用点积损失最大化与目标标签的相似度，最小化与负样本的相似度。</li></ul></li><li>优点：<ul><li>它是一种模块化体系结构，适合典型的软件开发工作流程；</li><li>在准确性和性能方面，能达到大规模预训练语言模型的效果；</li><li>改进了现有技术，胜过当时的SOTA，并且训练速度提高了6倍。</li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLU/IntentClassification/">【关于 Domain/Intent Classification 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLU/SlotFilling/">【关于 槽位填充 (Slot Filling)】那些你不知道的事</a></li></ul><h6 id="-32"><a href="#-32" class="headerlink" title=""></a></h6><h6 id="【关于-状态追踪（DST）】那些你不知道的事"><a href="#【关于-状态追踪（DST）】那些你不知道的事" class="headerlink" title="【关于 状态追踪（DST）】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/DST/">【关于 状态追踪（DST）】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/DST/">【关于 状态追踪（DST）】那些你不知道的事</a></li></ul><h6 id="-33"><a href="#-33" class="headerlink" title=""></a></h6><h6 id="【关于-自然语言生成NLG-】那些你不知道的事"><a href="#【关于-自然语言生成NLG-】那些你不知道的事" class="headerlink" title="【关于 自然语言生成NLG 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLG/">【关于 自然语言生成NLG 】那些你不知道的事</a></h6><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/NLG/">【关于 自然语言生成NLG 】那些你不知道的事</a></li><li>【关于 IRN 】 那些的你不知道的事<ul><li>论文：ScriptWriter: Narrative-Guided Script Generation</li><li>发表会议：ACL2020</li><li>论文地址：<a href="https://www.aclweb.org/anthology/2020.acl-main.10/">https://www.aclweb.org/anthology/2020.acl-main.10/</a></li><li>github：#</li><li>论文动机：如何将输入中对话状态的slot-value对正确的在response生成</li><li>论文方法：<ul><li>迭代网络：来不断修正生成过程不对的slot-value；</li><li>强化学习：不断更新，实验证明我们的网络生成的回复中中slot关键信息生成的正确性大大提高。</li></ul></li><li>实验结果：对多个基准数据集进行了综合研究，结果表明所提出的方法显著降低了所有强基线的时隙错误率。人类的评估也证实了它的有效性。</li></ul></li></ul><h6 id="-34"><a href="#-34" class="headerlink" title=""></a></h6><h6 id="【关于-E2E-】那些你不知道的事"><a href="#【关于-E2E-】那些你不知道的事" class="headerlink" title="【关于 E2E 】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/E2E/">【关于 E2E 】那些你不知道的事</a></h6><ul><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/E2E/TC_Bot/">【关于 TC_Bot(End-to-End Task-Completion Neural Dialogue Systems) 】那些你不知道的事</a></p></li><li><p>【关于 DF-Net 】 那些的你不知道的事</p><ul><li>论文：Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog</li><li>发表会议：ACL2020</li><li>论文地址：<a href="https://arxiv.org/abs/2004.11019">https://arxiv.org/abs/2004.11019</a></li><li>github：<a href="https://github.com/LooperXX/DF-Netmd">https://github.com/LooperXX/DF-Netmd</a></li><li>论文动机：<ol><li>依赖大量标准数据：端到端的模型依赖于大量的标注数据，这就导致了模型在一个新拓展的领域上很难利用。</li><li>对于一个新的领域，总是很难收集足够多的数据。这就使得将知识从具有充足标注数据的源领域迁移到一个只有少量标注数据的新领域成为非常重要的问题。</li></ol></li><li>前沿工作总结<ul><li>第一类：简单地结合多领域的数据集进行训练，如图 (a)<ul><li>优点：隐含地提取共享的特征</li><li>缺点：很难有效捕捉领域特有的知识</li></ul></li><li>第二类是在各个领域单独地训练模型，如图 (b)<ul><li>优点：能够很好地捕捉领域特有的知识；</li><li>缺点：却忽视了不同领域间共有的知识。</li></ul></li><li>第三类：通过建模不同领域间知识的连接来解决已有方法的局限。已有的一个简单的baseline如图 (c)，将领域共享的和领域私有的特征合并在一个共享-私有 (shared-private) 架构中。<ul><li>优点：区分了共享以及私有的知识</li><li>缺点：<ul><li>一是面对一个几乎不具备数据的新领域时，私有模块无法有效提取对应的领域知识；</li><li>二是这个架构忽略了一些领域子集间细粒度的想关性（比如和天气领域相比，导航领域和规划领域更相关）。</li></ul></li></ul></li></ul></li><li>思路：</li></ul><ol><li>shared-private 架构：学习共享的知识以及对应的领域特有特征；</li><li>动态融合网络：动态地利用所有领域间的相关性提供给下一步细粒度知识迁移；</li><li>对抗训练 (adversarial training) ：促使共享模块生成领域共享特征</li></ol></li></ul><h6 id="-35"><a href="#-35" class="headerlink" title=""></a></h6><h6 id="【关于-Rasa-】-那些的你不知道的事"><a href="#【关于-Rasa-】-那些的你不知道的事" class="headerlink" title="【关于 Rasa 】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/">【关于 Rasa 】 那些的你不知道的事</a></h6><ol><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C.md">【关于 rasa 安装 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6_%E8%A7%86%E9%A2%91%E8%AE%B2%E8%A7%A3.md">【关于 rasa 基本架构 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F.md">【关于 rasa中文对话系统】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA.md">【关于 rasa中文对话系统构建】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E7%B3%BB%E5%88%97/rasa_nlu.md">【关于 rasa-&gt;NLU 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa_core_FormAction/rasa_nlu.md">【关于 rasa -&gt; Core -&gt; FormAction 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa%E7%B3%BB%E5%88%97/rasa_core_Stories.md">【关于 rasa -&gt; Core -&gt; Stories 】那些你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/QA_study/dialogue_system_study/rasa/rasa_core_FormAction/rasa_core_Action.md">【关于 rasa -&gt; Core -&gt; Action 】那些你不知道的事</a></li></ol><h5 id="-36"><a href="#-36" class="headerlink" title=""></a></h5><h5 id="【关于-文本摘要】-那些的你不知道的事"><a href="#【关于-文本摘要】-那些的你不知道的事" class="headerlink" title="【关于 文本摘要】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/summarization_study/">【关于 文本摘要】 那些的你不知道的事</a></h5><ul><li><p>【关于 Bertsum】 那些的你不知道的事</p><p>【推荐阅读】</p><ul><li>论文名称：Fine-tune BERT for Extractive Summarization</li><li>会议：EMNLP2019</li><li>Bert 在抽取式文本摘要中的应用<ul><li>问题：<ul><li>如何获得每个句子向量？</li><li>如何将向量用于二分类问题？</li><li>如何判断每个句子的去留？</li></ul></li></ul></li><li>思路：定义文档 $d=[sent_1,sent_2,…,sent_m]$,$sent_i$ 表示  文档中的第$i$个句子，Extractive summarization 定义为  给每个$sent_i$分配一个标签$y_i∈{0,1}$，用于判断该句子是否包含于 摘要 中。<ul><li>方法介绍<ul><li>Extractive Summarization with BERT<ul><li>动机：<ul><li>Bert 作为 一个 masked-language model，输出向量基于标记而不是句子；</li><li>Bert 只包含 两个标签（sentence A or sentence B），而不是多个句子；</li></ul></li><li>方法<ul><li>Encoding Multiple Sentences：在每个句子之前插入一个[CLS] token<strong>【bert 就开头一个】</strong>，在每个句子之后插入一个[SEP] token</li><li>Interval Segment Embeddings<ul><li>我们使用区间段嵌入来区分文档中的多个句子。</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><blockquote><p>例如：对于$[sent_1，sent_2，sent_3，sent_4，sent_5]$，我们将分配[E_A，E_B，E_A，E_B，E_A] $sent_i$ 由向量 $T_i$ 表示，$T_i$是来自顶部BERT层的第$i$个[CLS]符号的向量。</p></blockquote><ul><li>【关于Pointer-Generator Networks 指针网络】 那些的你不知道的事<ul><li>论文名称：Get To The Point: Summarization with Pointer-Generator Networks</li><li>会议：ACL2017</li><li>动机：<ul><li>文本摘要类别<ul><li>extractive 抽取式<ul><li>方式：直接从原文抽取一些段落</li><li>优点：简单</li><li>问题：无法生成高质量的摘要，因为不具备一些复杂的摘要能力(如释义(paraphasing), 概括(generalization), 与现实世界知识的融合(incorporation of real-world knowledge))</li></ul></li><li>abstractive 生成式<ul><li>方式：根据长文本 生成 摘要</li><li>代表：seq2sq架构</li><li>问题：<ul><li>难以准确复述原文细节；</li><li>无法处理原文中的未登录词(OOV)；</li><li>在生成的摘要中存在一些重复的部分；</li></ul></li></ul></li></ul></li></ul></li><li>方法：<ul><li>编码器（encoder）<ul><li>方式：BiLSTM</li><li>作用：将文章中每个词的词向量编码为 隐状态 ht</li></ul></li><li>解码器（decoder）<ul><li>方式：单向 LSTM</li><li>作用：每一时刻 t，将上一时刻 生成 的 词的词向量作为输入，得到 Decoder Hidden State st，该状态被用于计算attention分布和词的预测</li></ul></li><li>Attention<ul><li>作用：每个时间步t,考虑当前序列的注意力分布，注意力分布用于生成编码器隐藏状态的加权总和，转化为上下文向量，与解码器t时刻的隐状态进行concatenated然后喂到两个线性层来计算词汇分布P（一个固定维度的向量，每个维度代表被预测词的概率，取argmax就能得到预测的单词）。</li><li>目的：告诉模型在当前步的预测过程中，原文中的哪些词更重要</li></ul></li></ul></li></ul></li></ul><h5 id="-37"><a href="#-37" class="headerlink" title=""></a></h5><h5 id="【关于-文本匹配】-那些的你不知道的事"><a href="#【关于-文本匹配】-那些的你不知道的事" class="headerlink" title="【关于 文本匹配】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/text_match_study/">【关于 文本匹配】 那些的你不知道的事</a></h5><ul><li><p>【关于 SimCSE】 那些你不知道的事</p><p>【推荐阅读】</p><ul><li>论文：SimCSE: Simple Contrastive Learning of Sentence Embeddings</li><li>会议：</li><li>论文地址：<a href="https://arxiv.org/abs/2104.08821">https://arxiv.org/abs/2104.08821</a></li><li>论文代码：<a href="https://github.com/princeton-nlp/SimCSE">https://github.com/princeton-nlp/SimCSE</a></li><li>思路：<ul><li>首先描述了一种无监督方法，它采用输入句子并在对比目标中预测自己，仅将标准 dropout 用作噪声。这种简单的方法效果出奇地好，与以前的受监督计数器部件相当。我们假设 dropout 充当最小数据增强的大小，删除它会导致表示崩溃。</li><li>然后，我们从最近从自然语言推理 (NLI) 数据集中学习句子嵌入的成功中汲取灵感，并将 NLI 数据集中的注释对合并到对比学习中，方法是使用“蕴含”对作为正例，将“矛盾”对作为硬负例。</li></ul></li><li>实验结果：<ul><li>作者评估了标准语义文本相似性（STS）任务上的 SimCSE，使用 BERT-base 的无监督和监督模型分别平均实现了 74.5％ 和 81.6％ 的 Spearman 相关性，与之前的最佳结果相比，分别提高了 7.9 和 4.6点。</li><li>作者还表明，对比学习理论上将嵌入分布得更均匀，并且在有监督信号可用时，它可以更好地对齐正样本。</li></ul></li></ul></li><li><p>【关于 BERT-flow 】那些你不知道的事</p><ul><li>论文：On the Sentence Embeddings from Pre-trained Language Models</li><li>会议：EMNLP2020</li><li>论文地址：<a href="https://arxiv.org/pdf/2011.05864.pdf">https://arxiv.org/pdf/2011.05864.pdf</a></li><li>论文代码：<a href="https://github.com/bohanli/BERT-flow">https://github.com/bohanli/BERT-flow</a></li><li>前沿：像BERT这样的经过预训练的上下文表示在自然语言处理中取得了巨大的成功；</li><li>动机：已经发现，未经微调的来自预训练语言模型的句子嵌入很难捕获句子的语义；</li><li>论文方法：在本文中，我们认为BERT嵌入中的语义信息没有得到充分利用。我们首先从理论上揭示了掩盖的语言模型预训练目标与语义相似性任务之间的理论联系，然后从经验上分析了BERT句子的嵌入。</li><li>实验结果：我们发现BERT总是诱发非光滑的各向异性语义空间，这会损害其语义相似性的表现。为解决此问题，我们建议通过将非正则化的流量标准化来将各向异性的语义嵌入分布转换为平滑的各向异性高斯分布。实验结果表明，我们提出的BERT流方法在各种语义文本相似性任务上比最先进的句子嵌入方法具有明显的性能提升。</li></ul></li><li><p>【关于 Sentence-BERT】 那些你不知道的事</p><ul><li>项目地址：<a href="https://github.com/km1994/nlp_paper_study">https://github.com/km1994/nlp_paper_study</a></li><li>论文：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</li><li>github:<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></li><li>动机：<ul><li>方法一：BERT使用交叉编码器：将两个句子传递到变压器网络，并预测目标值；<ul><li>问题： 由于太多可能的组合，此设置不适用于各种对回归任务。 在n = 10000个句子的集合中找到相似度最高的对需要BERT  n·（n-1）/ 2 = 49 995 000推理计算。 在现代V100 GPU上，这大约需要65个小时。  类似地，对于一个新问题，找到Quora的超过4,000万个现有问题中最相似的一个可以建模为与BERT的成对比较，但是，回答单个查询将需要50多个小时。</li></ul></li><li>方法二：解决聚类和语义搜索的常用方法是将每个句子映射到向量空间，以使语义相似的句子接近。 研究人员已开始将单个句子输入BERT，并得出固定大小的句子嵌入。 最常用的方法是平均BERT输出层（称为BERT嵌入）或通过使用第一个令牌的输出（[CLS]令牌）；<ul><li>问题：就像我们将要展示的那样，这种常规做法产生的句子嵌入效果很差，通常比平均GloVe嵌入效果更差。</li></ul></li></ul></li><li>论文方法：<ul><li>我们开发了SBERT。 siamese network 体系结构使得可以导出输入句子的固定大小矢量。 使用余弦相似度或Manhatten / Euclidean距离之类的相似度度量，可以找到语义上相似的句子。</li></ul></li><li>存在问题解答：<ul><li>小问题：<a href="https://github.com/km1994/nlp_paper_study/tree/master/text_match_study/sentence_bert/">在语义相似度任务中，SBERT的计算速度为什么比纯bert进行句子编码要快？</a></li></ul></li></ul></li><li><p>【关于 语义相似度匹配任务中的 BERT】 那些你不知道的事</p><p>【推荐阅读】</p><ul><li>阅读理由：BERT 在 语义相似度匹配任务 中的应用，可以由很多种方式，然而，你真的了解这些方式的区别和优缺点么？</li><li>动机：BERT 在 语义相似度匹配任务 中的应用，可以常用 Sentence Pair Classification Task：使用  [CLS]、cosine similairity、sentence/word embedding、siamese network  方法，那么哪种是最佳的方式呢？你是否考虑过呢?</li></ul></li><li><p>【关于 MPCNN】 那些你不知道的事</p><ul><li>论文：Multi-Perspective Sentence Similarity Modeling with Convolution Neural Networks</li></ul></li><li><p>【关于 RE2】 那些你不知道的事</p><ul><li>论文：Simple and Effective Text Matching with Richer Alignment Features</li><li>动机： 可以使用多个序列间比对层构建更强大的模型。 代替基于单个对准过程的比较结果进行预测，具有多个对准层的堆叠模型将保持其中间状态并逐渐完善其预测。<strong>但是，由于底层特征的传播效率低下和梯度消失，这些更深的体系结构更难训练。</strong></li><li>介绍：一种快速强大的神经体系结构，具有用于通用文本匹配的多个对齐过程。  我们对以前文献中介绍的文本匹配方法中许多慢速组件的必要性提出了质疑，包括复杂的多向对齐机制，对齐结果的大量提炼，外部句法特征或当模型深入时用于连接堆叠块的密集连接。 这些设计选择会极大地减慢模型的速度，并且可以用重量更轻且效果相同的模型代替。 同时，我们重点介绍了有效文本匹配模型的三个关键组成部分。  这些组件（名称为RE2代表）是以前的对齐特征（残差矢量），原始点向特征（嵌入矢量）和上下文特征（编码矢量）。  其余组件可能尽可能简单，以保持模型快速，同时仍能产生出色的性能。</li></ul></li><li><p>【关于 DSSM】 那些你不知道的事</p><ul><li>论文：Deep Structured Semantic Model</li><li>论文会议：CIKM2013</li><li>问题：语义相似度问题<ul><li>字面匹配体现<ul><li>召回：在召回时，传统的文本相似性如 BM25，无法有效发现语义类 Query-Doc 结果对，如”从北京到上海的机票”与”携程网”的相似性、”快递软件”与”菜鸟裹裹”的相似性</li><li>排序：在排序时，一些细微的语言变化往往带来巨大的语义变化，如”小宝宝生病怎么办”和”狗宝宝生病怎么办”、”深度学习”和”学习深度”；</li></ul></li><li>使用 LSA 类模型进行语义匹配，但是效果不好</li></ul></li><li>思路：<ul><li>利用 表示层 将 Query 和 Title 表达为低维语义向量；</li><li>通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。</li></ul></li><li>优点<ul><li>减少切词的依赖：解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题，因为在英文单词中，词的数量可能是没有限制，但是字母 n-gram 的数量通常是有限的</li><li>基于词的特征表示比较难处理新词，字母的 n-gram可以有效表示，鲁棒性较强；</li><li>传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA  的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA  都是无监督的训练，这样会给整个模型引入误差，DSSM 采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高；</li><li>省去了人工的特征工程；</li></ul></li><li>缺点<ul><li>word hashing可能造成冲突</li><li>DSSM采用了词袋模型，损失了上下文信息</li><li>在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛</li></ul></li></ul></li><li><p>【关于 ABCNN 】那些你不知道的事</p><ul><li>论文：ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</li><li>会议：TACL 2016</li><li>论文方法：采用了CNN的结构来提取特征，并用attention机制进行进一步的特征处理，作者一共提出了三种attention的建模方法</li></ul></li><li><p>【关于 ESIM 】那些你不知道的事 </p><ul><li>论文：Enhanced LSTM for Natural Language Inference</li><li>会议：TACL2017</li><li>自然语言推理（NLI: natural language inference）问题：<ul><li>即判断能否从一个前提p中推导出假设h</li><li>简单来说，就是判断给定两个句子的三种关系：蕴含、矛盾或无关</li></ul></li><li>论文方法：<ul><li>模型结构图分为左右两边：</li><li>左侧就是 ESIM，</li><li>右侧是基于句法树的 tree-LSTM，两者合在一起交 HIM (Hybrid Inference Model)。</li><li>整个模型从下往上看，分为三部分：<ul><li>input encoding；</li><li>local inference modeling；</li><li>inference composition；</li><li>Prediction</li></ul></li></ul></li></ul></li><li><p>【关于 BiMPM 】那些你不知道的事</p><ul><li>论文：Bilateral multi-perspective matching for natural language sentences</li><li>会议：IJCAI2017</li><li>方法：<ul><li>Word Representation Layer:其中词表示层使用预训练的Glove或Word2Vec词向量表示,  论文中还将每个单词中的字符喂给一个LSTM得到字符级别的字嵌入表示, 文中使用两者构造了一个dd维的词向量表示, 于是两个句子可以分别表示为  P:[p1,⋯,pm],Q:[q1,⋯,qn].</li><li>Context Representation Layer: 上下文表示层, 使用相同的双向LSTM来对两个句子进行编码. 分别得到两个句子每个时间步的输出.</li><li>Matching layer: 对两个句子PP和QQ从两个方向进行匹配, 其中⊗⊗表示某个句子的某个时间步的输出对另一个句子所有时间步的输出进行匹配的结果. 最终匹配的结果还是代表两个句子的匹配向量序列.</li><li>Aggregation Layer: 使用另一个双向LSTM模型, 将两个匹配向量序列两个方向的最后一个时间步的表示(共4个)进行拼接, 得到两个句子的聚合表示.</li></ul></li></ul></li><li><p>Prediction Layer: 对拼接后的表示, 使用全连接层, 再进行softmax得到最终每个标签的概率.</p></li><li><p>【关于 DIIN 】那些你不知道的事 </p><ul><li>论文：Densely Interactive Inference Network</li><li>会议：TACL2017</li><li>模型主要包括五层：嵌入层（Embedding Layer）、编码层（Encoding Layer）、交互层（Interaction Layer ）、特征提取层（Feature Extraction Layer）和输出层（Output Layer）</li></ul></li><li><p>【关于 DC-BERT】 那些你不知道的事</p><ul><li>论文名称：DC-BERT : DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING</li><li>阅读理由：Bert 在 QA 上面的应用</li><li>动机：Bert 无法处理传入问题的高吞吐量，每个问题都有大量检索到的文档；</li><li>论文方法：具有双重BERT模型的解耦上下文编码框架：<ul><li>一个在线BERT，仅对问题进行一次编码；</li><li>一个正式的BERT，对所有文档进行预编码并缓存其编码；</li></ul></li></ul></li><li><p>【关于 tBERT 】那些你不知道的事 </p><ul><li>论文：tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection</li><li>会议：ACL2020</li><li>论文地址：<a href="https://www.aclweb.org/anthology/2020.acl-main.630/">https://www.aclweb.org/anthology/2020.acl-main.630/</a></li><li>论文代码：<a href="https://github.com/wuningxi/tBERT">https://github.com/wuningxi/tBERT</a></li><li>动机：未存在将主题模型和BERT结合的方法。 语义相似度检测是自然语言的一项基本任务理解。添加主题信息对于以前的特征工程语义相似性模型和神经网络模型都是有用的其他任务。在那里目前还没有标准的方法将主题与预先训练的内容表示结合起来比如 BERT。</li><li>方法：我们提出了一种新颖的基于主题的基于BERT的语义相似度检测体系结构，并证明了我们的模型在不同的英语语言数据集上的性能优于强神经基线。我们发现在BERT中添加主题特别有助于解决特定领域的情况。</li></ul></li></ul><h5 id="-38"><a href="#-38" class="headerlink" title=""></a></h5><h5 id="【关于-机器翻译】-那些的你不知道的事"><a href="#【关于-机器翻译】-那些的你不知道的事" class="headerlink" title="【关于 机器翻译】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/MachineTranslation/">【关于 机器翻译】 那些的你不知道的事</a></h5><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/MachineTranslation/NeuralMachineTranslationOfRareWordsWithSubwordUnits/">Neural Machine Translation of Rare Words with Subword Units 论文学习</a></li></ul><h5 id="-39"><a href="#-39" class="headerlink" title=""></a></h5><h5 id="【关于-文本生成】-那些的你不知道的事"><a href="#【关于-文本生成】-那些的你不知道的事" class="headerlink" title="【关于 文本生成】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/text_generation/">【关于 文本生成】 那些的你不知道的事</a></h5><ul><li><p><a href="https://github.com/km1994/nlp_paper_study/tree/master/text_generation/SLCVAE/">【关于 SLCVAE 安装 】那些你不知道的事</a></p></li><li><p>【关于 ScriptWriter 】 那些的你不知道的事</p><ul><li>论文：ScriptWriter: Narrative-Guided Script Generation</li><li>发表会议：ACL2020</li><li>论文地址：<a href="https://arxiv.org/abs/2005.10331">https://arxiv.org/abs/2005.10331</a></li><li>github：<a href="https://github.com/DaoD/ScriptWriter">https://github.com/DaoD/ScriptWriter</a></li><li>论文动机：如果人为提供一些参考信息（例如情节），能否进一步提高文本生成的质量？例如，在故事生成这一问题中，现有模型要从头考虑如何生成一整个故事，难度比较大，那如果人为提供一个故事线，是否可以提升模型的性能呢？</li><li>论文方法：<ul><li>根据给定的情节和已有的台词上文生成后续台词。在这一模型中，我们设计了一个情节追踪模块，这一模块可以使模型根据已有的上文内容判断情节的表达情况，并在后续生成中更加关注未表达的情节。实验结果表明，情节的确可以帮助模型提高台词的生成质量，且相比于其他模型，ScriptWriter能够更有效地利用情节信息。</li></ul></li><li>论文思路：</li></ul><ol><li>多层注意力机制：将情节、上文、候选回复表示为向量。</li><li>情节更新机制：使情节的表示包含更多未表达部分的情节信息。</li><li>抽取了三类匹配特征：<ol><li>（1）上文-回复匹配，其能够反映回复是否与上文连贯；</li><li>（2）情节-回复匹配，其能够反映回复是否与情节相符；</li><li>（3）上文-情节匹配，其能够隐式反映哪些情节已经被上文表达。</li></ol></li><li>最后，这些匹配特征经过CNN的进一步抽取和聚集，再经过MLP得到最终的匹配得分。</li></ol></li></ul><h5 id="-40"><a href="#-40" class="headerlink" title=""></a></h5><h5 id="【关于-NLP分类任务】那些你不知道的事"><a href="#【关于-NLP分类任务】那些你不知道的事" class="headerlink" title="【关于 NLP分类任务】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/">【关于 NLP分类任务】那些你不知道的事</a></h5><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/">【关于 NLP分类任务】那些你不知道的事</a><ul><li>转载于：JayLou娄杰大佬的 <a href="https://zhuanlan.zhihu.com/p/183852900">如何解决NLP分类任务的11个关键问题：类别不平衡&amp;低耗时计算&amp;小样本&amp;鲁棒性&amp;测试检验&amp;长文本分类</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/classifier_trick/">【关于 文本分类 trick】那些你不知道的事</a><ul><li>一、数据预处理篇<ul><li>1.1 vocab 构建问题</li><li>1.2 模型输入问题</li><li>1.3 噪声数据处理问题</li><li>1.4 中文任务分词问题</li><li>1.5 停用词处理问题</li></ul></li><li>二、模型篇<ul><li>2.1 模型选择问题</li><li>2.2 词袋模型 or 词向量————词向量选择问题</li><li>2.3 字 or 词向量———— 粒度选择问题</li></ul></li><li>三、参数篇<ul><li>3.1 正则化问题</li><li>3.2 学习率问题</li></ul></li><li>四、任务篇<ul><li>4.1 二分类问题</li><li>4.2 多标签分类问题</li><li>4.3 长文本问题</li><li>4.4 鲁棒性问题</li></ul></li><li>五、标签体系构建<ul><li>5.1 标签体系构建问题</li><li>5.2 标签体系合理性评估问题</li></ul></li><li>六、策略构建篇<ul><li>6.1 算法策略构建问题</li><li>6.2 特征挖掘策略问题</li><li>6.3 数据不均衡问题<ul><li>6.3.1 重采样（re-sampling）</li><li>6.3.2 重加权（re-weighting）</li><li>6.3.3 数据增强</li></ul></li><li>6.4 预训练模型融合角度问题</li><li>6.5 灾难性遗忘问题</li><li>6.6 小模型大智慧<ul><li>6.6.1 模型蒸馏</li><li>6.6.2 数据蒸馏</li></ul></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/KG_classifier/">【关于 Knowledge in TextCNN】那些你不知道的事</a><ul><li>论文： Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</li><li>github：<a href="https://zhuanlan.zhihu.com/p/183852900">https://zhuanlan.zhihu.com/p/183852900</a></li><li>介绍：文本分类是NLP应用程序中的一项基本任务。 现有的大多数工作都依靠显式或隐式文本表示来解决此问题。</li><li>问题：虽然这些技术对句子很有效，但由于其简短和稀疏，因此无法轻松地应用于短文本。</li><li>方法：提出了一个基于卷积神经网络的框架，该框架结合了短文本的显式和隐式表示形式进行分类<ul><li>首先使用大型分类学知识库将短文本概念化为一组相关概念。</li><li>然后，通过在预训练的单词向量之上合并单词和相关概念来获得短文本的嵌入。</li><li>我们进一步将字符级功能集成到模型中，以捕获细粒度的子词信息。</li></ul></li><li>实验结果：在五个常用数据集上的实验结果表明，我们提出的方法明显优于最新方法。</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/LOTClass/">【关于 LOTClass】那些你不知道的事</a><ul><li>论文名称：《Text Classification Using Label Names Only: A Language Model Self-Training Approach》</li><li>会议：EMNLP2020</li><li>论文地址：<a href="https://arxiv.org/pdf/2010.07245.pdf">https://arxiv.org/pdf/2010.07245.pdf</a></li><li>论文源码地址：<a href="https://github.com/yumeng5/LOTClass">https://github.com/yumeng5/LOTClass</a></li><li>动机<ul><li>监督学习：标注数据昂贵</li><li>半监督学习：虽然减少了对标注数据的依赖，但还是需要领域专家手动进行标注，特别是在类别数目很大的情况下</li><li>关键词积累：关键词在不同上下文中也会代表不同类别</li></ul></li><li>方法：<ul><li>提出了一种基于预训练神经 LM 的弱监督文本分类模型 LotClass，它不需要任何标记文档，只需要每个类的标签名称。</li><li>提出了一种寻找类别指示词的方法和一个基于上下文的单词类别预测任务，该任务训练LM使用一个词的上下文来预测一个词的隐含类别。经过训练的LM很好地推广了基于未标记语料库的自训练文档级分类</li></ul></li><li>在四个分类数据集上，LOTClass明显优于各弱监督模型，并具有与强半监督和监督模型相当的性能。</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/PET/">【关于 PET 】那些你不知道的事</a><ul><li>论文名称：《exploiting cloze questions for few shot text classification and natural language inference 》</li><li>会议：EMNLP2020</li><li>论文地址：chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Faclanthology.org%2F2021.eacl-main.20.pdf#=&amp;zoom=125</li><li>论文源码地址：<a href="https://github.com/timoschick/pet">https://github.com/timoschick/pet</a></li></ul></li></ul><h6 id="-41"><a href="#-41" class="headerlink" title=""></a></h6><h6 id="【关于-细粒度情感分析】-那些的你不知道的事"><a href="#【关于-细粒度情感分析】-那些的你不知道的事" class="headerlink" title="【关于 细粒度情感分析】 那些的你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/classifier_study/ABSC_study/">【关于 细粒度情感分析】 那些的你不知道的事</a></h6><ul><li>【关于 LCF】 那些的你不知道的事<ul><li>论文名称：A Local Context Focus Mechanism for Aspect-Based Sentiment Classiﬁcation</li><li>论文动机：没有考虑情感极性和局部上下文间关系<ul><li>LCF:利用自注意力机制同时捕获局部上下文特征和全局上下文特征，以推断 targeted aspect 的情感极性</li><li>SRD:评估上下文词与 aspect 间的独立性，SRD对于弄清局部上下文具有重要意义，并且SRD阈值中的上下文单词的特征将得到保留和重点关注。</li><li>CDM 和 CDW 层：强化 LCF，使其对 特殊 aspest 的局部上下文提供 更多 注意力。CDM层通过掩盖语义相对较少的上下文词的输出表示，将重点放在局部上下文上。 CDW 层根据 SRD 削弱语义相对较少的上下文词的特征；</li></ul></li></ul></li></ul><h5 id="-42"><a href="#-42" class="headerlink" title=""></a></h5><h5 id="【关于-中文分词】那些你不知道的事"><a href="#【关于-中文分词】那些你不知道的事" class="headerlink" title="【关于 中文分词】那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/chinese_word_segmentation/">【关于 中文分词】那些你不知道的事</a></h5><ul><li>【关于 中文分词】那些你不知道的事<ul><li>一、什么是 中文分词？</li><li>二、为什么用使用 中文分词，直接用句子或字不好么？</li><li>三、中文分词 有哪些难点？</li><li>四、常用方法有哪些？<ul><li>4.1 基于词典的中文分词方法<ul><li>4.1.1 正向最大匹配法</li><li>4.1.2 负向最大匹配法</li><li>4.1.3 双向最大匹配法</li></ul></li><li>4.2 基于N-gram语言模型的分词方法</li><li>4.3 基于规则的中文分词方法</li></ul></li></ul></li><li>【关于 DAAT 】 那些的你不知道的事<ul><li>论文：Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation</li><li>发表会议：ACL2020</li><li>论文地址：h<a href="https://arxiv.org/abs/2007.08186">https://arxiv.org/abs/2007.08186</a></li><li>github：<a href="https://github.com/Alibaba-NLP/DAAT-CWS">https://github.com/Alibaba-NLP/DAAT-CWS</a></li><li>动机：完全监督的神经方法在中文分词（CWS）的任务中取得了重大进展。将监督模型应用于域外数据时，其性能往往会急剧下降。<ul><li>性能下降原因：<ul><li>跨域的分布差距</li><li>词汇不足（OOV）问题</li></ul></li></ul></li><li>方法<ul><li>Distant annotation（DA）<ul><li>目的：自动生成目标域内句子的分词结果</li><li>方法：是在不需要任何人工定义词典的情况下，自动对目标领域文本实现自动标注。</li><li>思路：<ul><li>基本分词器：使用来自源域的标注数据训练，用于识别源域和目标域中常见的单词</li><li>特定领域的单词挖掘器：旨在探索目标特定于领域的单词</li></ul></li><li>存在问题<ul><li>存在影响最终性能的标注错误问题</li></ul></li></ul></li><li>Adversarial Training（AT）<ul><li>动机：为了降低噪声数据的影响，更好地利用源域数据，</li><li>方法：在源域数据集和通过Distant annotation构造的目标领域数据集上联合进行Adversarial training的方法。</li><li>优点：Adversarial training模块可以捕获特定领域更深入的特性，和不可知领域的特性。</li></ul></li></ul></li></ul></li></ul><h5 id="-43"><a href="#-43" class="headerlink" title=""></a></h5><h5 id="【关于-搜索引擎】-那些你不知道的事"><a href="#【关于-搜索引擎】-那些你不知道的事" class="headerlink" title="【关于 搜索引擎】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/search_engine/">【关于 搜索引擎】 那些你不知道的事</a></h5><ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/search_engine">【关于 搜索引擎】 那些你不知道的事</a><ul><li>搜索系统的架构设计<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E6%90%9C%E7%B4%A2-qpquery%E7%90%86%E8%A7%A3%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1">搜索 QP（query理解）的架构设计</a></li></ul></li><li>搜索j介绍<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F-%E4%BB%8B%E7%BB%8D">搜索排序 介绍</a></li><li>Embedding 搜索<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8A%A8%E6%9C%BA">动机</a></li><li><a href="https://github.com/km1994/nlp_paper_study#embedding-%E6%90%9C%E7%B4%A2%E4%BC%98%E7%82%B9">Embedding 搜索优点</a></li><li><a href="https://github.com/km1994/nlp_paper_study#embedding%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%BD%A2%E5%BC%8F">Embedding的学习形式</a></li><li><a href="https://github.com/km1994/nlp_paper_study#embedding-%E6%90%9C%E7%B4%A2-%E6%89%80%E5%85%B3%E5%BF%83%E7%9A%84%E9%97%AE%E9%A2%98">Embedding 搜索 所关心的问题</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li></ul></li><li>Query纠错<ul><li><a href="https://github.com/km1994/nlp_paper_study#query%E7%BA%A0%E9%94%99-%E4%B9%8B--%E5%8E%9F%E7%90%86">Query纠错 之  原理</a></li><li>Query纠错 之 文本错误类型<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8A%A8%E6%9C%BA-1">动机</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%B8%B8%E8%A7%81%E7%9A%84%E9%94%99%E8%AF%AF%E7%B1%BB%E5%9E%8B">常见的错误类型</a></li></ul></li><li>Query纠错 之 纠错结果类型<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8A%A8%E6%9C%BA-2">动机</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BB%8B%E7%BB%8D">介绍</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E7%BA%A0%E9%94%99%E7%BB%93%E6%9E%9C%E7%B1%BB%E5%9E%8B">纠错结果类型</a></li></ul></li></ul></li></ul></li><li>搜索引擎两大问题<ul><li>问题一：召回<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%AC%E5%9B%9E">什么是召回？</a></li><li>基于关键词的召回方法<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BB%80%E4%B9%88%E6%98%AF-%E5%9F%BA%E4%BA%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E7%9A%84%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%B3%95-">什么是 基于关键词的召回方法 ？</a></li></ul></li><li>基于关键词的召回方法存在哪些问题？<ul><li><a href="https://github.com/km1994/nlp_paper_study#q1%E7%B4%A2%E5%BC%95%E7%B2%92%E5%BA%A6%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E9%97%AE%E9%A2%98">Q1：索引粒度如何选择问题</a></li><li><a href="https://github.com/km1994/nlp_paper_study#q2%E4%BF%9D%E8%AF%81-%E5%8F%AC%E5%9B%9E-%E6%9C%89%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3%E6%95%B0%E9%97%AE%E9%A2%98">Q2：保证 召回 有相关文档数问题</a></li><li><a href="https://github.com/km1994/nlp_paper_study#q3%E5%8F%AC%E5%9B%9E-%E5%80%99%E9%80%89-query-%E5%A4%9A%E6%A0%B7%E6%80%A7%E9%97%AE%E9%A2%98">Q3：召回 候选 query 多样性问题</a></li><li><a href="https://github.com/km1994/nlp_paper_study#q4%E5%8F%AC%E5%9B%9E-%E5%80%99%E9%80%89-query-%E6%97%A0%E8%AF%AD%E4%B9%89%E6%95%88%E6%9E%9C%E9%97%AE%E9%A2%98">Q4：召回 候选 query 无语义效果问题</a></li></ul></li><li>基于语义的召回方法<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BB%80%E4%B9%88%E6%98%AF-%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E7%9A%84%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%B3%95">什么是 基于语义的召回方法？</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E7%9A%84%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%B3%95-%E7%9A%84%E6%80%9D%E8%B7%AF">基于语义的召回方法 的思路</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E7%9A%84%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%B3%95-%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98">基于语义的召回方法 存在问题</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1">参考资料</a></li></ul></li><li>问题二：相关性<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E4%BB%80%E4%B9%88%E6%98%AF-%E7%9B%B8%E5%85%B3%E6%80%A7">什么是 相关性？</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E7%9B%B8%E5%85%B3%E6%80%A7-%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98">相关性 存在哪些问题？</a></li><li>相关性方法介绍<ul><li><a href="https://github.com/km1994/nlp_paper_study#%E8%AE%A1%E7%AE%97%E5%9C%BA%E6%99%AF%E8%A7%92%E5%BA%A6">计算场景角度</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E8%A7%92%E5%BA%A6">计算方法角度</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-2">参考资料</a></li></ul></li></ul></li><li>搜索未来新趋势<ul><li><a href="https://github.com/km1994/nlp_paper_study#1-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%90%9C%E7%B4%A2">1. 多模态搜索</a></li><li><a href="https://github.com/km1994/nlp_paper_study#2-%E6%9B%B4%E8%AF%AD%E4%B9%89%E6%90%9C%E7%B4%A2">2. 更语义搜索</a></li><li><a href="https://github.com/km1994/nlp_paper_study#3-%E5%A4%9A%E8%BD%AE%E6%90%9C%E7%B4%A2">3。 多轮搜索</a></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/search_engine/PLMbasedRankingInBaiduSearch/">【关于 GECToR】 那些你不知道的事</a><ul><li>论文：Pre-trained Language Model based Ranking in Baidu Search</li><li>论文地址：<a href="https://arxiv.org/abs/2105.11108">https://arxiv.org/abs/2105.11108</a></li><li>论文出处：KDD’21</li><li>动机：<ul><li>作为搜索引擎的核心， Ranking System 在满足用户的信息需求方面起着至关重要的作用；</li><li>基于 PLM 的 Neural Rankers 难以直接应用：<ul><li>（1）推理时延高：大规模神经 PLM 的计算成本过高，尤其是对于网络文档中的长文本，禁止将它们部署在需要极低延迟的 Online Ranking System 中；</li><li>(2) 目标不一致问题：基于 PLM 的训练目标 与 临时检索场景目标 存在不一致问题；</li><li>(3) 兼容性问题：搜索引擎通常涉及 committee of ranking components，如何 让 Fine-tuning PLM 得到的 Ranking System 与其 兼容，存在问题；</li></ul></li></ul></li><li>论文方法：在线搜索引擎系统中部署最先进的中文预训练语言模型（即 ERNIE）时，贡献了一系列成功应用的技术来解决这些暴露的问题。<ul><li>首先，阐述了一种新颖的做法，以经济高效地汇总 Web 文档，并使用廉价但功能强大的 Pyramid-ERNIE 架构将结果汇总内容与查询联系起来。</li><li>然后，赋予了一种创新范式来精细地利用大规模嘈杂和有偏见的点击后行为数据进行面向相关的预训练。</li><li>提出了一种 针对 在线排名系统 的 human-anchored 微调策略 ，旨在稳定各种在线组件的排名信号。</li></ul></li><li>实验结果：大量的离线和在线实验结果表明，所提出的技术显着提高了搜索引擎的性能。</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/search_engine/PLMforWeb-scaleRetrievalInBaiduSearch/">【关于 PLM for Web-scale Retrieval in Baidu Search 】 那些你不知道的事</a><ul><li>论文：Pre-trained Language Model for Web-scale Retrieval in Baidu Search</li><li>论文地址：<a href="https://arxiv.org/abs/2106.03373">https://arxiv.org/abs/2106.03373</a></li><li>论文出处：KDD’21</li><li>介绍： Retrieval 是网络搜索中的一个关键阶段，它从十亿规模的语料库中识别出一个与查询相关的候选集。在 retrieval 阶段发现更多语义相关的候选集 有助于 向最终用户展示更多高质量的结果。</li><li>动机：<ul><li>【语义匹配】：<strong>如何 解决 用户 query 多样化和口语化问题？</strong></li><li>【冷启动问题】：<strong>对于 大多数 第一次出现的 query 和 doc，如何让 Retrieval Models 捕获 其对应语义信息？</strong></li><li>【工程实践】：<strong>如何 将 Retrieval Models 应用于 Baidu Search？</strong></li></ul></li><li>论文方法：论文描述了作者在 Baidu Search 中开发和部署的 Retrieval Models 。<ul><li>该系统利用了最近最先进的中文预训练语言模型，即通过知识整合 (ERNIE) 的增强表示，它促进了系统的表达语义匹配。</li><li>基于 ERNIE 的 Retrieval Models 拥有：<ul><li>1）expressive Transformer-based semantic encoders：能够 帮助 Retrieval 充分捕获 query 和 doc 对应语义信息；</li><li>2）多阶段训练范式：ERNIE 预训练模型 分别采用 不同的语料数据 进行 多阶段训练，提高模型 泛化能力；</li></ul></li><li>系统工作流程：基于 ERNIE 的 Retrieval Models 结合 传统 Retrieval Models 和 Deep  Retrieval Models，并 采用  lightweight post-retrieval filtering module  引入更多的统计特征（例如，点击率、停留时间），来对上述 Retrieval Models 的 检索结果 进行 统一过滤，；</li><li>最终，该系统完全部署到生产环境中，并进行了严格的离线和在线实验。</li></ul></li><li>实验结果：<ul><li>该系统可以执行高质量的候选 retrieval ，特别是对于那些需求不常见的尾部查询。</li><li>由预训练语言模型（即 ERNIE）推动的新 retrieval system 可以在很大程度上提高我们搜索引擎的可用性和适用性。</li></ul></li></ul></li></ul><h5 id="-44"><a href="#-44" class="headerlink" title=""></a></h5><h5 id="【关于-文本纠错】-那些你不知道的事"><a href="#【关于-文本纠错】-那些你不知道的事" class="headerlink" title="【关于 文本纠错】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/text_corrector/">【关于 文本纠错】 那些你不知道的事</a></h5><ul><li><p>【关于 GECToR】 那些你不知道的事</p><ul><li><p>动机：</p><ul><li>由于 NMT-based GEC系统 的 核心是 seq2seq 结构，所以在部署的时候会遇到以下问题：</li></ul><ol><li>缓慢的推理速度；</li><li>需要大量的训练数据；</li><li>可解释性，从而使他们需要其他功能来解释更正，例如语法错误类型分类；</li></ol></li><li><p>论文方法：提出了仅使用Transformer编码器的简单有效的GEC序列标注器。</p></li><li><p>论文思路：</p><ul><li>系统在综合数据上进行了预训练；</li><li>然后分两个阶段进行了微调：<ul><li>首先是错误的语料库；</li><li>其次是有错误和无错误的平行语料库的组合。</li></ul></li><li>我们设计了自定义的字符级别转换，以将输入字符映射到纠正后的目标。</li></ul></li><li><p>效果：</p><ul><li>我们最好的单模型以及联合模型GEC标注器分别在CoNLL-2014测试集上F0.5达到65.3和66.5，在BEA-2019上F0.5达到72.4和73.6。模型的推理速度是基于Transformer的seq2seq GEC系统的10倍</li></ul></li></ul></li></ul><h5 id="-45"><a href="#-45" class="headerlink" title=""></a></h5><h5 id="【关于-Text-to-SQL】-那些你不知道的事"><a href="#【关于-Text-to-SQL】-那些你不知道的事" class="headerlink" title="【关于 Text-to-SQL】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/">【关于 Text-to-SQL】 那些你不知道的事</a></h5><ul><li>【关于 Text-to-SQL 】 那些你不知道的事<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#%E4%BB%80%E4%B9%88%E6%98%AF-text-to-sql">什么是 Text-to-SQL?</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-text-to-sql">为什么需要 Text-to-SQL?</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#text-to-sql-%E5%AE%9A%E4%B9%89">Text-to-SQL 定义?</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#text-to-sql-%E6%9C%89%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE%E9%9B%86">Text-to-SQL 有哪些数据集?</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#text-to-sql-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7">Text-to-SQL 如何评价?</a></li><li>Text-to-SQL 有哪些模型，都存在哪些优缺点?<ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#seq2seq-%E6%A8%A1%E5%9E%8B">seq2seq 模型</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#sqlnet-%E6%A8%A1%E5%9E%8B">SQLNet 模型</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#typesql-%E6%A8%A1%E5%9E%8B">TypeSQL 模型</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#syntaxsqlnet-%E6%A8%A1%E5%9E%8B">SyntaxSQLNet 模型</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#irnet-%E6%A8%A1%E5%9E%8B">IRNet 模型</a></li><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/text2sql_study/readme.md#global-gnn--rat-sql">Global-GNN &amp;&amp; RAT-SQL</a></li></ul></li></ul></li><li>【关于 LGESQL 】 那些你不知道的事<ul><li>论文：LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations</li><li>来源：ACL2021</li><li>链接：<a href="https://arxiv.org/abs/2106.01093">https://arxiv.org/abs/2106.01093</a></li><li>Github地址：<a href="https://github.com/rhythmcao/text2sql-lgesql">https://github.com/rhythmcao/text2sql-lgesql</a></li><li>动机：这项工作旨在解决 text-to-SQL 任务中具有挑战性的异构图编码问题。以前的方法通常以节点为中心，仅利用不同的权重矩阵来参数化边类型<ul><li><ol><li>忽略了边的拓扑结构中嵌入的丰富语义</li></ol></li><li><ol><li>无法区分每个节点的局部和非局部关系。</li></ol></li></ul></li><li>论文方法：提出了一种 Line Graph Enhanced Text-to-SQL (LGESQL)  模型来挖掘底层关系特征，而无需构建元路径。凭借线图，消息不仅通过节点之间的连接，而且通过有向边的拓扑更有效地传播。此外，在图迭代期间，局部和非局部关系都被独特地整合在一起。我们还设计了一个称为图修剪的辅助任务，以提高编码器的判别能力。</li><li>论文实验：在撰写本文时，我们的框架在跨域文本到 SQL 基准 Spider 上取得了最先进的结果（Glove 为 62.8%，Electra 为 72.0%）。</li></ul></li></ul><h4 id="-46"><a href="#-46" class="headerlink" title=""></a></h4><h4 id="实战篇"><a href="#实战篇" class="headerlink" title="实战篇"></a>实战篇</h4><h5 id="-47"><a href="#-47" class="headerlink" title=""></a></h5><h5 id="重点推荐篇"><a href="#重点推荐篇" class="headerlink" title="重点推荐篇"></a>重点推荐篇</h5><ul><li><a href="https://github.com/km1994/nlp_paper_study/tree/master/transformer_study/Transformer">Transfromer 源码实战</a><ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/transformer_study/Transformer/code.md">【关于 Transformer 代码实战（文本摘要任务篇）】 那些你不知道的事</a> 【<a href="https://zhuanlan.zhihu.com/p/312044432">知乎篇</a> 】</li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/">【关于 Bert 源码解析 】 那些的你不知道的事</a><ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/bertCode1_modeling.md">【关于 Bert 源码解析 之 主体篇 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/bertCode2_pretraining.md">【关于 Bert 源码解析 之 预训练篇 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/bertCode3_fineTune.md">【关于 Bert 源码解析 之 微调篇 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/bertCode4_word2embedding.md">【关于 Bert 源码解析IV 之 句向量生成篇 】 那些的你不知道的事</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/bert_study/T1_bert/bertCode5_similarity.md">【关于 Bert 源码解析V 之 文本相似度篇 】 那些的你不知道的事</a></li></ul></li></ul><h3 id="-48"><a href="#-48" class="headerlink" title=""></a></h3><h3 id="会议收集篇"><a href="#会议收集篇" class="headerlink" title="会议收集篇"></a>会议收集篇</h3><ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/other_study/meeting/ACL_study/ACL2020.md">ACL2020</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/other_study/meeting/SIGIR_stduy/readme.md">SIGIR2020</a></li><li>【关于 AAAI 】那些你不知道的事<ul><li>一、AAAI 2021<ul><li>1.1 情感分析<ul><li>【Learning Modality-Specific Representations with Self-Supervised  Multi-Task Learning for Multimodal Sentiment Analysis (Self-MM)】</li><li>【An Adaptive Hybrid Framework for Cross-Domain Aspect-Based Sentiment Analysis (AHF)】</li><li>【Bridging Towers of Multi-Task Learning with a Gating Mechanism for  Aspect-Based Sentiment Analysis and Sequential Metaphor Identification  ()】</li><li>【Human-Level Interpretable Learning for Aspect-Based Sentiment Analysis ()】</li><li>【A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis ()】</li><li>【Quantum Cognitively Motivated Decision Fusion for Video Sentiment Analysis ()】</li><li>【Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis ()】</li><li>【Segmentation of Tweets with URLs and its Applications to Sentiment Analysis ()】</li><li>【Segmentation of Tweets with URLs and its Applications to Sentiment Analysis ()】</li></ul></li><li>1.2 命名实体识别<ul><li>【Multi-Modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance (UMGF)】</li><li>【CrossNER: Evaluating Cross-Domain Named Entity Recognition ()】</li><li>【A Supervised Multi-Head Self-Attention Network for Nested Named Entity Recognition ()】</li><li>【Nested Named Entity Recognition with Partially-Observed TreeCRFs (Partially-Observed-TreeCRFs)】</li><li>【Continual Learning for Named Entity Recognition】</li><li>【Knowledge-Aware Named Entity Recognition with Alleviating Heterogeneity】</li><li>【Denoising Distantly Supervised Named Entity Recognition via a Hypergeometric Probabilistic Model】</li><li>【MTAAL: Multi-Task Adversarial Active Learning for Medical Named Entity Recognition and Normalization（MTAAL）】</li></ul></li><li>1.3 关系抽取<ul><li>【FL-MSRE: A Few-Shot Learning Based Approach to Multimodal Social Relation Extraction（L-MSRE）】</li><li>【Multi-View Inference for Relation Extraction with Uncertain Knowledge】</li><li>【GDPNet: Refining Latent Multi-View Graph for Relation Extraction（GDPNet）】</li><li>【Progressive Multi-Task Learning with Controlled information Flow for Joint Entity and Relation Extraction】</li><li>【Curriculum-Meta Learning for Order-Robust Continual Relation Extraction】</li><li>【Document-Level Relation Extraction with Reconstruction】</li><li>【Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling(ATLOP )】</li><li>【Entity Structure Within and Throughout: Modeling Mention Dependencies for Document Level Relation Extraction（SSAN）】</li><li>【Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training(  MULTICAST)】</li><li>【Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference（CTRL-PG）】</li><li>【A Unified Multi-Task Learning Framework for Joint Extraction of Entities and Relations】</li></ul></li><li>1.4 事件抽取<ul><li>【A Unified Multi-Task Learning Framework for Joint Extraction of Entities and Relations】</li><li>【What the Role Is vs. What Plays the Role: Semi-Supervised Event Argument Extraction via Dual Question Answering（DualQA）】</li><li>【Span-Based Event Coreference Resolution】</li></ul></li><li>1.5 知识图谱<ul><li>【Dual Quaternion Knowledge Graph Embeddings(DualE)】</li><li>【Type-Augmented Relation Prediction in Knowledge Graphs】</li><li>【ChronoR: Rotation Based Temporal Knowledge Graph Embedding】</li><li>【PASSLEAF: A Pool-Based Semi-Supervised Learning Framework for Uncertain Knowledge Graph Embedding】</li><li>【KG-BART: Knowledge Graph-Augmented Bart for Generative Commonsense Reasoning】</li><li>【Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders】</li><li>【其他】</li></ul></li></ul></li></ul></li></ul><h3 id="-49"><a href="#-49" class="headerlink" title=""></a></h3><h3 id="Elastrsearch-学习篇"><a href="#Elastrsearch-学习篇" class="headerlink" title="Elastrsearch 学习篇"></a>Elastrsearch 学习篇</h3><ul><li>Elastrsearch 学习<ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/[es_study/](https:/github.com/km1994/nlp_paper_study/tree/master/trick/NLP_tools/es_study/)ElasticSearch%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md">ElasticSearch架构解析与最佳实践.md</a></li></ul></li></ul><h3 id="-50"><a href="#-50" class="headerlink" title=""></a></h3><h3 id="竞赛篇"><a href="#竞赛篇" class="headerlink" title="竞赛篇"></a>竞赛篇</h3><h4 id="-51"><a href="#-51" class="headerlink" title=""></a></h4><h4 id="【关于-NLP比赛】-那些你不知道的事"><a href="#【关于-NLP比赛】-那些你不知道的事" class="headerlink" title="【关于 NLP比赛】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/nlp_game/">【关于 NLP比赛】 那些你不知道的事</a></h4><ul><li>一、问答匹配任务<ul><li>\5. 新冠疫情相似句对判定大赛 【比赛地址】<ul><li><a href="https://github.com/km1994/nlp_paper_study#51-%E8%B5%9B%E9%A2%98%E8%83%8C%E6%99%AF">5.1 赛题背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#52-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D">5.2 数据集介绍</a></li><li><a href="https://github.com/km1994/nlp_paper_study#53-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88%E6%94%B6%E9%9B%86">5.3 比赛方案收集</a></li></ul></li><li>\4. 2021搜狐校园文本匹配算法大赛 【比赛地址】<ul><li><a href="https://github.com/km1994/nlp_paper_study#41-%E8%B5%9B%E9%A2%98%E8%83%8C%E6%99%AF">4.1 赛题背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#42-%E6%AF%94%E8%B5%9B%E4%BB%BB%E5%8A%A1">4.2 比赛任务</a></li><li><a href="https://github.com/km1994/nlp_paper_study#43-%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E-%E5%9C%B0%E5%9D%80">4.3 数据说明 【地址】</a></li><li><a href="https://github.com/km1994/nlp_paper_study#44-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88%E6%94%B6%E9%9B%86">4.4 比赛方案收集</a></li></ul></li><li>\3. CCF2020问答匹配比赛<ul><li><a href="https://github.com/km1994/nlp_paper_study#31-%E6%AF%94%E8%B5%9B%E8%83%8C%E6%99%AF">3.1 比赛背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#32-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88%E6%94%B6%E9%9B%86">3.2 比赛方案收集</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#2-%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%97%AE%E9%A2%98%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E7%AC%AC%E4%B8%89%E5%B1%8A%E9%AD%94%E9%95%9C%E6%9D%AF%E5%A4%A7%E8%B5%9B">2. 智能客服问题相似度算法设计——第三届魔镜杯大赛</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1-2018cikm-analyticup--%E9%98%BF%E9%87%8C%E5%B0%8F%E8%9C%9C%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%B7%A8%E8%AF%AD%E8%A8%80%E7%9F%AD%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B">1. 2018CIKM AnalytiCup – 阿里小蜜机器人跨语言短文本匹配算法竞赛</a></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%85%B6%E4%BB%96">其他</a></li></ul></li><li>二、对话<ul><li>\1. 2020 CCF BDCI《千言：多技能对话》<ul><li><a href="https://github.com/km1994/nlp_paper_study#11-%E8%B5%9B%E9%A2%98%E7%AE%80%E4%BB%8B">1.1 赛题简介</a></li><li><a href="https://github.com/km1994/nlp_paper_study#12-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88%E6%94%B6%E9%9B%86">1.2 比赛方案收集</a></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#2-2018jd-dialog-challenge-%E4%BB%BB%E5%8A%A1%E5%AF%BC%E5%90%91%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E6%8C%91%E6%88%98%E8%B5%9B">2. 2018JD Dialog Challenge 任务导向型对话系统挑战赛</a></li></ul></li><li>三、文本分类<ul><li><a href="https://github.com/km1994/nlp_paper_study#1-2018-dc%E8%BE%BE%E8%A7%82-%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98">1. 2018 DC达观-文本智能处理挑战</a></li><li><a href="https://github.com/km1994/nlp_paper_study#2-%E8%B7%AF%E9%80%8F%E7%A4%BE%E6%96%B0%E9%97%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90">2. 路透社新闻数据集“深度”探索性分析(词向量/情感分析)</a></li><li><a href="https://github.com/km1994/nlp_paper_study#3-%E7%9F%A5%E4%B9%8E%E7%9C%8B%E5%B1%B1%E6%9D%AF">3. 知乎看山杯</a></li><li><a href="https://github.com/km1994/nlp_paper_study#4-2018-ccl-%E5%AE%A2%E6%9C%8D%E9%A2%86%E5%9F%9F%E7%94%A8%E6%88%B7%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB%E8%AF%84%E6%B5%8B">4. 2018 CCL 客服领域用户意图分类评测</a></li><li><a href="https://github.com/km1994/nlp_paper_study#5-2018-kaggle-quora-insincere-questions-classification">5. 2018 kaggle quora insincere questions classification</a></li></ul></li><li>四、 关键词提取<ul><li><a href="https://github.com/km1994/nlp_paper_study#1-%E7%A5%9E%E7%AD%96%E6%9D%AF2018%E9%AB%98%E6%A0%A1%E7%AE%97%E6%B3%95%E5%A4%A7%E5%B8%88%E8%B5%9B%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96">1. “神策杯”2018高校算法大师赛(关键词提取)</a></li></ul></li><li>五、内容识别<ul><li><a href="https://github.com/km1994/nlp_paper_study#1-%E7%AC%AC%E4%BA%8C%E5%B1%8A%E6%90%9C%E7%8B%90%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%E5%A4%A7%E8%B5%9B">1. 第二届搜狐内容识别大赛</a></li></ul></li><li>六、观点主题<ul><li><a href="https://github.com/km1994/nlp_paper_study#1-%E6%B1%BD%E8%BD%A6%E8%A1%8C%E4%B8%9A%E7%94%A8%E6%88%B7%E8%A7%82%E7%82%B9%E4%B8%BB%E9%A2%98%E5%8F%8A%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB">1. 汽车行业用户观点主题及情感识别</a></li></ul></li><li>七、实体链指<ul><li><a href="https://github.com/km1994/nlp_paper_study#71-ccks2019%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E7%9A%84%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87">7.1. CCKS&amp;2019中文短文本的实体链指</a></li><li><a href="https://github.com/km1994/nlp_paper_study#72-ccks-2020%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87%E6%AF%94%E8%B5%9B">7.2. CCKS 2020实体链指比赛</a></li></ul></li><li>八、命名实体识别<ul><li>8.1 天池中药说明书实体识别<ul><li><a href="https://github.com/km1994/nlp_paper_study#811-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0">8.1.1 任务描述</a></li><li><a href="https://github.com/km1994/nlp_paper_study#812-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">8.1.2 比赛方案</a></li></ul></li><li>8.2 CCF BDCI 中文命名实体识别算法鲁棒性评测<ul><li><a href="https://github.com/km1994/nlp_paper_study#821-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0">8.2.1 任务描述</a></li></ul></li></ul></li><li>九、事件抽取<ul><li>9.1 CCKS 2020：面向金融领域的小样本跨类迁移事件抽取<ul><li><a href="https://github.com/km1994/nlp_paper_study#911-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0">9.1.1 任务描述</a></li><li><a href="https://github.com/km1994/nlp_paper_study#912-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">9.1.2 比赛方案</a></li></ul></li><li>9.2 CCKS2019_EventEntityExtraction<ul><li><a href="https://github.com/km1994/nlp_paper_study#911-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0-1">9.1.1 任务描述</a></li><li><a href="https://github.com/km1994/nlp_paper_study#912-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88-1">9.1.2 比赛方案</a></li></ul></li><li>9.3 2020 科大讯飞事件抽取挑战赛<ul><li><a href="https://github.com/km1994/nlp_paper_study#931-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0">9.3.1 任务描述</a></li><li><a href="https://github.com/km1994/nlp_paper_study#932-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">9.3.2 比赛方案</a></li></ul></li></ul></li><li>十、阅读理解<ul><li>10.1 2021海华AI挑战赛·中文阅读理解·技术组<ul><li><a href="https://github.com/km1994/nlp_paper_study#1011-%E8%B5%9B%E9%A2%98%E8%83%8C%E6%99%AF">10.1.1 赛题背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1012-%E6%AF%94%E8%B5%9B%E4%BB%BB%E5%8A%A1">10.1.2 比赛任务</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1013-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">10.1.3 比赛方案</a></li></ul></li></ul></li><li>十一、关系抽取<ul><li>11.1 2020语言与智能技术竞赛：关系抽取任务 【比赛链接】<ul><li><a href="https://github.com/km1994/nlp_paper_study#1111-%E8%B5%9B%E9%A2%98%E8%83%8C%E6%99%AF">11.1.1 赛题背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1112-%E8%B5%9B%E9%A2%98%E8%AF%B4%E6%98%8E">11.1.2 赛题说明</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1113-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D">11.1.3 数据集介绍</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1114-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">11.1.4 比赛方案</a></li></ul></li></ul></li><li>十二、中文文本纠错<ul><li>12.1 专业赛：自然语言处理技术创新大赛——中文文本纠错比赛<ul><li><a href="https://github.com/km1994/nlp_paper_study#1211-%E7%AB%9E%E8%B5%9B%E8%83%8C%E6%99%AF">12.1.1 竞赛背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1212-%E8%B5%9B%E9%A2%98%E6%8F%8F%E8%BF%B0">12.1.2 赛题描述</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1213-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">12.1.3 模型训练</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1214-%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D">12.1.4 训练集数据介绍</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1215-%E6%AF%94%E8%B5%9B%E6%96%B9%E6%A1%88">12.1.5 比赛方案</a></li></ul></li></ul></li><li>十三、智能人机交互自然语言理解<ul><li>13.1 CCF BDCI 智能人机交互自然语言理解<ul><li><a href="https://github.com/km1994/nlp_paper_study#1311-%E7%AB%9E%E8%B5%9B%E8%83%8C%E6%99%AF">13.1.1 竞赛背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1312-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D">13.1.2 数据集介绍</a></li></ul></li></ul></li><li>十四、智能人机交互自然语言理解<ul><li>14.1 CCF BDCI 预训练模型知识量度量<ul><li><a href="https://github.com/km1994/nlp_paper_study#1411-%E7%AB%9E%E8%B5%9B%E8%83%8C%E6%99%AF">14.1.1 竞赛背景</a></li><li><a href="https://github.com/km1994/nlp_paper_study#1412-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D">14.1.2 数据集介绍</a></li></ul></li></ul></li><li><a href="https://github.com/km1994/nlp_paper_study#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li></ul><h4 id="-52"><a href="#-52" class="headerlink" title=""></a></h4><h4 id="【关于-NLP-比赛方案学习】-那些你不知道的事"><a href="#【关于-NLP-比赛方案学习】-那些你不知道的事" class="headerlink" title="【关于 NLP 比赛方案学习】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/nlp_game/game_study/">【关于 NLP 比赛方案学习】 那些你不知道的事</a></h4><h5 id="-53"><a href="#-53" class="headerlink" title=""></a></h5><h5 id="【关于-NLP-比赛方案学习】-那些你不知道的事-1"><a href="#【关于-NLP-比赛方案学习】-那些你不知道的事-1" class="headerlink" title="【关于 NLP 比赛方案学习】 那些你不知道的事"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/nlp_game/game_study/">【关于 NLP 比赛方案学习】 那些你不知道的事</a></h5><ul><li>实体链指<ul><li>CCKS&amp;2019中文短文本的实体链指<ul><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_game/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87/CCKS&2019%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E7%9A%84%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87/CCKS2019%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87%E6%AF%94%E8%B5%9B%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A5%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">CCKS2019中文短文本实体链指比赛技术创新奖解决方案</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_game/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87/CCKS&2019%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E7%9A%84%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87/CCKS2020%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87%E6%AF%94%E8%B5%9B%E5%B0%8F%E7%B1%B3KG%E5%86%A0%E5%86%9B%E6%96%B9%E6%A1%88">CCKS2020实体链指比赛小米KG冠军方案</a></li></ul></li></ul></li></ul><h3 id="-54"><a href="#-54" class="headerlink" title=""></a></h3><h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a><a href="https://github.com/km1994/nlp_paper_study/blob/master">学习资源</a></h3><ul><li>知识图谱<ul><li><a href="https://github.com/npubird/KnowledgeGraphCourse">东南大学《知识图谱》研究生课程</a></li><li><a href="https://github.com/codeants2012/FinancialKnowledgeGraph">基于知识图谱的金融资讯推荐</a></li><li><a href="https://github.com/memect/kg-beijing">北京知识图谱学习小组</a></li><li>美团技术团队文章<ul><li><a href="https://mp.weixin.qq.com/s/FFkcu5K1oZnzX8Rg72WHqQ">领域应用 | 常识性概念图谱建设以及在美团场景中的应用</a></li><li><a href="https://mp.weixin.qq.com/s/itAj4jvL1lR4CfbL2rkl_w">【实践】多业务建模在美团搜索排序中的实践</a></li><li><a href="https://mp.weixin.qq.com/s/JX9xUgxcniNLlmKDR7AAGA">美团外卖美食知识图谱的迭代及应用</a></li></ul></li></ul></li><li>文本摘要<ul><li><a href="https://github.com/bifeng/nlp_paper_notes/blob/75cf64a7eb244814fccf241d5990e23526352ab3/Summarization.md">Summarization.</a></li><li><a href="https://github.com/liucongg/GPT2-NewsTitle">GPT2-NewsTitle</a></li></ul></li><li><a href="https://github.com/CLUEbenchmark/CLUEDatasetSearch">CLUEDatasetSearch</a>【中英文NLP数据集】</li></ul><h3 id="-55"><a href="#-55" class="headerlink" title=""></a></h3><h3 id="NLP-数据集"><a href="#NLP-数据集" class="headerlink" title="NLP 数据集"></a><a href="https://github.com/km1994/nlp_paper_study/tree/master/nlp_corpus/">NLP 数据集</a></h3><ul><li>【关于 NLP 语料】那些你不知道的事<ul><li>一、命名实体识别<ol><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/boson">boson数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/cluener_public">clue细粒度实体识别数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/MSRA">微软实体识别数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/people_daily">人民网实体识别数据集（98年）</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/tianchi_yiyao">中药说明书实体识别数据集（“万创杯”中医药天池大数据竞赛）</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/video_music_book_datasets">视频_音乐_图书数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nnlp_corpus/er_data/weibo">微博数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/ResumeNER">简历 数据集</a></li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/ner_data/2020_ccks_ner">2020_ccks_ner 中文医学文本命名实体识别</a></li></ol></li><li>二、抽取式文本摘要<ol><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/chinese_abstractive_corpus/education">教育培训行业抽象式自动摘要中文语料库</a>【中文】</li><li><a href="https://pan.baidu.com/share/init?surl=szq0Wa60AS5ISpM_SNPcbA">哈工大-新浪微博短文本摘要</a>【密码：ayn6】【中文】</li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/chinese_abstractive_corpus/education">教育培训行业抽象式自动摘要中文语料库</a>【中文】</li><li><a href="https://pan.baidu.com/s/1NWe6K33GMTp4Wk7CwaGotA">个人-新浪微博</a>【密码：4k12】【中文】</li><li><a href="https://github.com/km1994/nlp_paper_study/blob/master/nlp_corpus/chinese_abstractive_corpus/%E6%B8%AF%E5%A4%A7%E5%A4%9A%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81">港大多文本摘要</a> 【英文】</li><li><a href="https://github.com/mahnazkoupaee/WikiHow-Dataset">WikiHow: A Large Scale Text Summarization Dataset</a> 【英文】</li><li><a href="https://github.com/abisee/cnn-dailymail">The CNN / Daily Mail dataset (non-anonymized)</a> 【英文】</li><li><a href="https://github.com/WING-NUS/scisumm-corpus">Scientific Document Summarization Corpus and Annotations from the WING NUS group</a> 【英文】【Data and code for the AAAI 2019 paper cisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization  with Citation Networks】</li><li><a href="https://github.com/Alex-Fabbri/Multi-News">Multi-News</a>  【英文】【介绍：Data and code for the ACL 2019 paper Multi-News: a Large-Scale  Multi-Document Summarization Dataset and Abstractive Hierarchical  Model.】</li></ol></li></ul></li></ul><h3 id="-56"><a href="#-56" class="headerlink" title=""></a></h3><h3 id="GCN-study学习篇"><a href="#GCN-study学习篇" class="headerlink" title="GCN_study学习篇"></a><a href="https://github.com/km1994/GCN_study">GCN_study学习篇</a></h3><ul><li>GCN 介绍篇<ul><li><a href="https://github.com/km1994/GCN_study/blob/master/graph_introduction/graph_introduction.md">Graph 介绍</a></li><li><a href="https://github.com/km1994/GCN_study/blob/master/WL/WL_conclusion.md">Weisfeiler-Leman 算法介绍</a></li></ul></li><li>GCN 三剑客<ul><li><a href="https://github.com/km1994/GCN_study/blob/master/CNNonGraph_Defferrard_2016/CNNonGraph_Defferrard_2016_%E6%80%BB%E7%BB%93.md">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a></li><li>[SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS](<a href="https://github.com/km1994/GCN_study/blob/master/GCN/SEMI-SUPERVISED">https://github.com/km1994/GCN_study/blob/master/GCN/SEMI-SUPERVISED</a> CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS.pdf)</li><li><a href="https://github.com/km1994/GCN_study/blob/master/Attention_models/Attention_models.md">Attention Models in Graphs: A Survey</a></li></ul></li><li>经典篇<ul><li><a href="https://github.com/km1994/GCN_study/tree/master/CanGNNGoOnline">Can GNN go “online”?an analysis of pretraining and inference</a></li><li><a href="https://github.com/km1994/GCN_study/tree/master/GCNforTextClassification">Graph Convolutional Networks for Text Classification</a></li><li>[HOW POWERFUL ARE GRAPH NEURAL NETWORKS](<a href="https://github.com/km1994/GCN_study/blob/master/HowPowerGCN/HOW">https://github.com/km1994/GCN_study/blob/master/HowPowerGCN/HOW</a> POWERFUL ARE GRAPH NEURAL NETWORKS.pdf)</li><li>[Graph Convolutional Matrix Completion](<a href="https://github.com/km1994/GCN_study/blob/master/GCMC/Graph">https://github.com/km1994/GCN_study/blob/master/GCMC/Graph</a> Convolutional Matrix Completion.pdf)</li><li><a href="https://github.com/km1994/GCN_study/blob/master/RepresentationLearningForAttributedMultiplexHeterogeneousNetwork/RepresentationLearningForAttributedMultiplexHeterogeneousNetwork.md">Representation Learning For Attributed Multiplex Heterogeneous Network</a></li></ul></li><li>预训练篇<ul><li><a href="https://github.com/km1994/GCN_study/tree/master/pretrainingGCN_1">GNN 教程：GCN 的无监督预训练</a></li><li><a href="https://github.com/km1994/GCN_study/blob/master/pretrainingGCN_2/pretrainingGCN.md">Pre-training Graph Neural Networks</a></li></ul></li><li>实战篇<ul><li><a href="https://github.com/km1994/dgl">DGL</a></li><li><a href="https://github.com/km1994/GCN_study/blob/master/DGL_study/DGL_introduction.md">DGL 入门</a></li><li><a href="https://github.com/km1994/GCN_study/blob/master/DGL_study/DGL_GCN_introduction.md">DGL 入门 —— GCN 实现</a></li></ul></li></ul><h2 id="-57"><a href="#-57" class="headerlink" title=""></a></h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.pianshen.com/article/14251297031/">【ACL2020放榜!】事件抽取、关系抽取、NER、Few-Shot 相关论文整理</a></li><li><a href="https://www.zhihu.com/question/385259014">第58届国际计算语言学协会会议（ACL 2020）有哪些值得关注的论文？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目前研一，在nlp课题组，只想毕业。请问一下nlp中哪个方向比较好发论文？</title>
      <link href="/2021/12/02/%E7%9B%AE%E5%89%8D%E7%A0%94%E4%B8%80%EF%BC%8C%E5%9C%A8nlp%E8%AF%BE%E9%A2%98%E7%BB%84%EF%BC%8C%E5%8F%AA%E6%83%B3%E6%AF%95%E4%B8%9A%E3%80%82%E8%AF%B7%E9%97%AE%E4%B8%80%E4%B8%8Bnlp%E4%B8%AD%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91%E6%AF%94%E8%BE%83%E5%A5%BD%E5%8F%91%E8%AE%BA%E6%96%87%EF%BC%9F/"/>
      <url>/2021/12/02/%E7%9B%AE%E5%89%8D%E7%A0%94%E4%B8%80%EF%BC%8C%E5%9C%A8nlp%E8%AF%BE%E9%A2%98%E7%BB%84%EF%BC%8C%E5%8F%AA%E6%83%B3%E6%AF%95%E4%B8%9A%E3%80%82%E8%AF%B7%E9%97%AE%E4%B8%80%E4%B8%8Bnlp%E4%B8%AD%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91%E6%AF%94%E8%BE%83%E5%A5%BD%E5%8F%91%E8%AE%BA%E6%96%87%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>教题主一个方法：统计nlp领域四大顶会：ACL、EMNLP、COLING、NAACL近三年所有录用长文的研究方向。</p><p>前两年没什么研究，最近一年开始猛增的方向说明这个方向处在研究热点的早期，能填的坑还比较多，较容易发论文。</p><p>前三年都有很多研究论文说明该方向是研究热点，能不能填坑非常考验水平。</p><p>三年来论文数逐渐减少，且最近一年论文个数非常少，说明这个方向的坑填的差不多的，不太好发论文。</p><blockquote><p>作者：阿良<br>链接：<a href="https://www.zhihu.com/question/454906063/answer/1842052693">https://www.zhihu.com/question/454906063/answer/1842052693</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何研读一篇论文？</title>
      <link href="/2021/12/02/%E5%A6%82%E4%BD%95%E7%A0%94%E8%AF%BB%E4%B8%80%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%9F/"/>
      <url>/2021/12/02/%E5%A6%82%E4%BD%95%E7%A0%94%E8%AF%BB%E4%B8%80%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：李rumor<br>链接：<a href="https://www.zhihu.com/question/21083751/answer/2200220849">https://www.zhihu.com/question/21083751/answer/2200220849</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>在我最开始读论文的时候，必须要把所有文章都打印出来，从头读到尾，用笔去各种<a href="https://www.zhihu.com/search?q=%E5%88%92%E7%BA%BF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2200220849%7D">划线</a>做笔记。只有脱离屏幕，我才能静下心来把那些东西装进脑子里。直到后来我打印的论文越来越多，才不得已在电脑上看。再后来因为写知乎博客，在那段BERT变体疯狂出世的时期，我读论文的速度越来越快，总想第一个把新研究分享出来，<strong>当知乎NLP前沿最快的女人</strong>。到了现在，我似乎又比之前提高了一些，大家有时会惊讶于我出文的速度，如果文章不太复杂的话，<strong>我其实读了标题、摘要、图表之后就开始打草稿了</strong>。。。今天就顺着沐神的视频「<strong>如何读论文</strong>」，<strong>也分享一些我的个人私货</strong>。</p><p>论文通常分为6个部分：Title、Abstract、Introduction (包含Related works)、Method、Experiments、Conclusion。</p><p>沐神建议大家分三遍来读一篇论文。</p><p><strong>第一遍，目的是大概知道论文在讲什么，适不适合自己</strong>。先读一下标题和摘要，了解论文在做什么，再直接看结论，虽然大部分都是重复的，但也会把摘要里提到的问题用实际结论再证明一下。再去瞄一眼关键的图和表。这一遍大概要十几分钟，如果确定适合自己，就可以开始第二遍。</p><p><strong>第二遍，通读一遍文章，目的是了解文章解决了什么问题</strong>，用了什么方法，搞清楚重要的图和表都在干什么，相比别人的方法有什么优点。读的时候可以把有用的东西圈出来，比如重要的参考文献、不懂的句子。这一遍不用特别细节，一些公式和证明可以略过。这遍读完后，如果觉得这份工作对自己有价值，可以继续第三遍的精读。</p><p><strong>第三遍，知道每一段每一句都在干什么，把自己放到作者的位置</strong>，开始思考自己如何去解决这个问题，如何设计实验，能不能做得更好，或者哪些地方可以继续深入。这样以后基于这份工作扩展，或者使用这个方法的时候就会更自如。</p><p>以上就是沐神分享的方法，我的整体差不多，只是不同场景会有diff。一类场景是某类论文的集中调研，可能是项目要落地，也可能是要做research，总之会有个大概的领域和任务限定；另一类场景是随便读读，毕竟周围的人都读，不进则退，被迫好学。下面就分这两个场景介绍一下我的方法。</p><h2 id="有方向的调研"><a href="#有方向的调研" class="headerlink" title="有方向的调研"></a><strong>有方向的调研</strong></h2><p>首先是<strong>怎么找论文</strong>，这是我后台经常收到的问题。在做research之前一定要调研充分，否则论文都写了一半发现跟之前的撞了，或者审稿人直接给你丢过来一篇就很尴尬。找论文可以从以下四方面：</p><ol><li>先去搜索最近的该领域综述，综述读下来就对该领域有初步理解，知道了历史SOTA</li><li>最近一两年的顶会（NLP比如ACL、NAACL、EMNLP、AAAI、ICLR等），去他们公开的paper list按关键词搜索。怎么找关键词？从之前看的综述里面找</li><li>在Google Scholar等搜索引擎搜关键词，顶会毕竟有几个月的延迟，可能人家先挂出来了</li><li>用上面的方法读了几篇之后，去看他们Related Works、实验对比里的其他方法，基本都是该领域有代表性的</li></ol><p>之后是<strong>怎么筛选论文，优先看知名会议、机构、作者的文章，一般来说质量还是有保障的</strong>。其他的就用沐神的方法先快速扫一下，没价值就不看了。</p><p>最后就是读论文了，沐神讲的方法很好，我略微补充几点：</p><ol><li><strong>既然是有方向的调研，就要调研出体系，这个领域有几种问题，每种问题都有什么解决办法，对比起来有什么优缺点</strong></li><li>如果是自己要follow的方法，实验分析环节要仔细的看，怎么处理的数据，用了什么超参数等</li><li>如果要发自己的文章，就要更精细地看一下其他paper都是怎么讲的motivation和contribution</li></ol><p>最最后，怎么样想新的idea？就像沐神说的那样，<strong>用作者思维去思考</strong>，读个一二十篇就能有些感觉。想第一次就发一篇惊世巨作太难了，更多的人还是先follow前人的工作，从其他领域借鉴一些改动，并学习英文的写作方式。所以<strong>不要自己干想</strong>。</p><h2 id="随便看看"><a href="#随便看看" class="headerlink" title="随便看看"></a><strong>随便看看</strong></h2><p>我现在日常就是随便看看，碰到我感兴趣的工作就分享给大家。</p><p>首先还是<strong>怎么找论文</strong>：</p><ol><li>Arxiv网站每天都会更新大家上传的论文，推荐我之前参与的项目：<a href="https://link.zhihu.com/?target=https://arxiv.xixiaoyao.cn/">https://arxiv.xixiaoyao.cn/</a>，我们把论文的作者机构和发表的会议单独筛了出来，方便快速筛选</li><li>顶会paper list也可以刷刷</li></ol><p>之后读论文的方法就差不多了。熟能生巧，当对特定领域的论文+偶尔思考超过一定数量之后（感觉自己也就读了不到100篇吧），速度一下就能上去。<strong>我速读时的attention主要在</strong>：</p><ol><li>Abstract的重点（也就两三句话）</li><li>Introduction中作者提出motivation和contribution那里</li><li>Related works里列的方法</li><li>Method里面的图表和说明（不太复杂的方法就那么几句话，数据集也都是常见那些）</li><li>Experiments里面的主实验、<a href="https://www.zhihu.com/search?q=%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2200220849%7D">消融实验</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研究生真的应该研一大量读文献吗？</title>
      <link href="/2021/12/02/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9C%9F%E7%9A%84%E5%BA%94%E8%AF%A5%E7%A0%94%E4%B8%80%E5%A4%A7%E9%87%8F%E8%AF%BB%E6%96%87%E7%8C%AE%E5%90%97%EF%BC%9F/"/>
      <url>/2021/12/02/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9C%9F%E7%9A%84%E5%BA%94%E8%AF%A5%E7%A0%94%E4%B8%80%E5%A4%A7%E9%87%8F%E8%AF%BB%E6%96%87%E7%8C%AE%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>答案是肯定的，最好是在研一开始就大量读文献，我跟你说下面几点原因你就清楚了。</p><p>早点开始读文献有很多好处，先来说下第一个好处：</p><p><strong>1.可以早点了解研究领域</strong></p><p>早读文献就可以早打基础，了解研究领域是需要一定时间的，不是一两个星期，至少是一两个月。如果研一不早点读文献，等你选题时再了解就晚了，到时候你会各种焦虑。</p><p><strong>2.为选题提早准备</strong></p><p>研一下学期末或者研二上学期初，通常是要进行开题的，如果研一不看文献，等到研二再看时间会非常紧，关键是你完全是没思路的，如果导师不罩着你，根本不知道怎么选题，特别是被放养的同学，绝对是一脸懵。</p><p>如果研一就读了大量文献，就会对文献的基本框架、领域的专业术语、领域的背景和现状都有所了解，再选题时就会轻松很多。</p><p>3.早点出成果</p><p>早读文献的好处还有就是一点，可以早点出成果，无论是文科还是<a href="https://www.zhihu.com/search?q=%E7%90%86%E5%B7%A5%E7%A7%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2249176443%7D">理工科</a>，文献是一切的基础，没有看过文献，就像发小论文，根本不可能。</p><p>所以，研一如果多看看文献，就会清楚小论文要怎么写，应该怎么写，很多研一开始做课题的人都发过小论文。</p><p>早出成果的好处很多，比如凭奖学金，<a href="https://www.zhihu.com/search?q=%E5%9B%BD%E5%A5%96&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2249176443%7D">国奖</a>和达到毕业要求，研一有成果，是最容易获得国奖的，很多学校毕业要求都要有小论文。如果研一就发小论文了，研二就会轻松很多。</p><p>总结：给大家罗列出了这三点是最重要的，还有很多的好处就不一一细说了。</p><p><strong>建议研一的学弟学妹尽早看论文，可以制定计划：</strong></p><ol><li><p>固定时间去看文献；</p></li><li><p>每周保持1～3篇文献阅读；</p></li><li><p>提前把检索文献，文献笔记，文献管理的方法技巧学会；</p></li><li><p>争取研一能够发表一篇小论文，哪怕是综述类文献也可；</p></li></ol><p>其实不会花费太多时间，这四件事在研一能够坚持下来，对你研二开展课题会轻松很多。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何高效管理文献？</title>
      <link href="/2021/12/02/%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E7%AE%A1%E7%90%86%E6%96%87%E7%8C%AE%EF%BC%9F/"/>
      <url>/2021/12/02/%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E7%AE%A1%E7%90%86%E6%96%87%E7%8C%AE%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：黄少<br>链接：<a href="https://www.zhihu.com/question/26857521/answer/34329827">https://www.zhihu.com/question/26857521/answer/34329827</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>我用Zotero，同学们在我的带动下好多也抛弃了endnote，开始使用zotero,都说好。阳志平写过几篇介绍zotero的文章</p><p><a href="https://link.zhihu.com/?target=http://www.yangzhiping.com/tech/zotero1.html">Zotero（1）：文献管理软件Zotero基础及进阶示范</a></p><p>，我就是看了这个决定用的。说一下我觉得zotero好的地方。</p><ul><li> 一键下载题录和原文。endnote的最大问题就是题录和原文无法同时下载，需要下载题录信息，再粘贴原文。这样很不方便。zotero有chrome和火狐的插件，利用这个插件，只要是你单位已经购买的文献，就可以一键下载。</li><li>标签和文件夹共存的模式。其他软件都是只能用文件夹来管理文献，一个文献只能放在一个文件夹里。而zotero在文件夹之外增加了标签，可以给一篇文献增加好几个标签。这样，你可以用按主题给文件夹分类，同时用另外的分类方式来给文献加标签。</li><li>安装新的引文格式很方便。这是与endnote相比，在安装新的引文格式时，endnote需要下载完后手动将该格式文件放到指定文件夹，而zotero只需在软件中加载即可。</li><li>zotero能够双击打开全文。endnote需要选中，再点击软件上方的工具栏，这种反人类的操作方式很让人受不了。每每想到这个缺点我都不能理解为什么还有人用endnote。</li></ul><p>另外，再提一句，文献管理软件里边可能会放很多文献。我平常也是觉得有用的就简单归个类放进去。这个文献管理软件的作用大概就是让你知道自己下过哪些文献，不会“费了好大劲找到了才发现自己之前下载过”。 但阅读文献，并最终引用到文章中的过程，就需要做笔记了。我一般是在onenote里列出这个主题的重要文献（从zotero里找），然后记录每篇文献的目的，方法，结果等等。虽然zotero也可以做笔记，但没有那么一目了然。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 - 文献管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>想不出创新点，感觉现有文献已经做得很完善了，研究生课题做不出来，内心焦虑不知道咋办才好?</title>
      <link href="/2021/12/02/%E6%83%B3%E4%B8%8D%E5%87%BA%E5%88%9B%E6%96%B0%E7%82%B9%EF%BC%8C%E6%84%9F%E8%A7%89%E7%8E%B0%E6%9C%89%E6%96%87%E7%8C%AE%E5%B7%B2%E7%BB%8F%E5%81%9A%E5%BE%97%E5%BE%88%E5%AE%8C%E5%96%84%E4%BA%86%EF%BC%8C%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E9%A2%98%E5%81%9A%E4%B8%8D%E5%87%BA%E6%9D%A5%EF%BC%8C%E5%86%85%E5%BF%83%E7%84%A6%E8%99%91%E4%B8%8D%E7%9F%A5%E9%81%93%E5%92%8B%E5%8A%9E%E6%89%8D%E5%A5%BD/"/>
      <url>/2021/12/02/%E6%83%B3%E4%B8%8D%E5%87%BA%E5%88%9B%E6%96%B0%E7%82%B9%EF%BC%8C%E6%84%9F%E8%A7%89%E7%8E%B0%E6%9C%89%E6%96%87%E7%8C%AE%E5%B7%B2%E7%BB%8F%E5%81%9A%E5%BE%97%E5%BE%88%E5%AE%8C%E5%96%84%E4%BA%86%EF%BC%8C%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E9%A2%98%E5%81%9A%E4%B8%8D%E5%87%BA%E6%9D%A5%EF%BC%8C%E5%86%85%E5%BF%83%E7%84%A6%E8%99%91%E4%B8%8D%E7%9F%A5%E9%81%93%E5%92%8B%E5%8A%9E%E6%89%8D%E5%A5%BD/</url>
      
        <content type="html"><![CDATA[<p>说几个自己用着还不错的土方法。</p><p>之前读研时，导师是学校领导，所以基本没时间指导，但要求又高，最后还是一个课题搞了三篇sci出来（还有一堆其他ei，cscd什么的，以及各种奖项），当然肯定不全是在校期间发的。具体步骤如下:</p><p><strong>第一步:直接抄技术</strong></p><p>万事开头难，做研究也是，在一个课题的初始阶段，如果没有头绪，完全可以直接把觉得有价值文献里的技术，原封不动的抄过来走一遍，先让研究思维动起来，解决“有没有”的问题。</p><p>在“抄”技术的过程中，当已经能够熟练掌握这套方法了，再结合自己课题的特征，通过大量复盘和跑盘，不断优化模型，调整参数，最终得到一个适合于自身的优化模型或者方法，初步的创新点就这么成型了。</p><p>就比如小白做吐司面包，通过照着教程一顿努力，终于做出一个普通的<a href="https://www.zhihu.com/search?q=%E5%A5%B6%E7%B2%89%E5%90%90%E5%8F%B8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">奶粉吐司</a>，那是不是可以根据自身口味，把奶粉这个参数优化替换成<a href="https://www.zhihu.com/search?q=%E6%8A%B9%E8%8C%B6%E7%B2%89&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">抹茶粉</a>，巧克力粉，椰子粉，创新出更多口味的面包。</p><p><strong>第二步:算法创新</strong></p><p>这个也是大多数论文常用的套路。大多<a href="https://www.zhihu.com/search?q=%E5%9F%BA%E7%A1%80%E7%A0%94%E7%A9%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">基础研究</a>，其实就是寻找世间客观存在的规律（简称找规律），也就是找实验数据和事物表象之间的一个联系，当出现什么什么参数特征时，就能推测出一个怎样怎样的事实结果。这种时候，就可以学点大数据分析的方法，什么<a href="https://www.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">神经网络</a>，支撑向量，模糊数学，灰色关键，多元回归，<a href="https://www.zhihu.com/search?q=%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">层次分析</a>。用这些算法来搭建实验数据和事物特征之间的关系（当然了，前提是二者间真的有联系）。把上述关系搞好了，创新点不就又出来了么。</p><p><strong>第三步: 融合式创新</strong></p><p>读文献不局限本行业的，可以尝试把不同研究领域的结论融合起来，就好比“相机＋手机＝拍照手机，AI＋家具＝智能家具，包括最近常听到的互联网＋”等等。有时候，把跨界的研究成果拿过来用，会有意想不到的创新点。比如<a href="https://www.zhihu.com/search?q=%E9%9B%B7%E8%BE%BE%E6%84%9F%E5%BA%94%E7%B3%BB%E7%BB%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">雷达感应系统</a>，和扫地机器人结合叫<a href="https://www.zhihu.com/search?q=%E7%B2%BE%E5%87%86%E4%BF%9D%E6%B4%81&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">精准保洁</a>，和汽车结合就叫主动刹车/巡航系统/智能驾驶，和手机结合又叫高精度手势操控，等等。</p><p><strong>第四步:横向运用型创新</strong></p><p>如果已经得到了一个运用于A领域，比较成熟的技术，完全可以再试一试拓展它的运用区间，拓展运用到A＋，或A－等横向领域，得到若干运用型创新点。就比如已经研发出一款药，已知它的功能是杀菌，那这款药有没有抗病毒，降血压，甚至治疗脱发等功能，都可以尝试下。大名鼎鼎的“<a href="https://www.zhihu.com/search?q=%E4%BC%9F%E5%93%A5&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">伟哥</a>”“米诺地尔”等神药，就是这么开发出来的。不过这个对长期在研究机构的研究员，特别是学生来说比较困难，需要有丰富的一线实践经历。</p><p><strong>第五步:<a href="https://www.zhihu.com/search?q=%E7%BA%B5%E5%90%91%E5%8E%9F%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">纵向原理</a>式创新</strong></p><p>能达到这个阶段的，大多都是大牛了（反正在自己短暂的研究生涯中，没达到）。能够基于自身广阔的学识和经历，通过大量理论和实践研究，形成一套独立自主的理论和应用体系。应该就是常说的“<a href="https://www.zhihu.com/search?q=%E5%AE%8C%E5%85%A8%E8%87%AA%E4%B8%BB%E5%88%9B%E6%96%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">完全自主创新</a>”吧。</p><p>不过，普通<a href="https://www.zhihu.com/search?q=%E7%A0%94%E7%A9%B6%E7%94%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1693371471%7D">研究生</a>，或者追求一般的研究人员，就用前四甚至前三个步骤已经足以，足够毕业或者混混职称了。</p><blockquote><p>作者：jerry.K<br>链接：<a href="https://www.zhihu.com/question/392686675/answer/1693371471">https://www.zhihu.com/question/392686675/answer/1693371471</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何对科研保持新鲜感?</title>
      <link href="/2021/12/02/%E5%A6%82%E4%BD%95%E5%AF%B9%E7%A7%91%E7%A0%94%E4%BF%9D%E6%8C%81%E6%96%B0%E9%B2%9C%E6%84%9F/"/>
      <url>/2021/12/02/%E5%A6%82%E4%BD%95%E5%AF%B9%E7%A7%91%E7%A0%94%E4%BF%9D%E6%8C%81%E6%96%B0%E9%B2%9C%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：自律向上的菲比<br>链接：<a href="https://www.zhihu.com/question/470047139/answer/2181080346">https://www.zhihu.com/question/470047139/answer/2181080346</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>读到一篇文章，是作者博一一年的感想。感同身受、深受触动，像是当头一击敲醒了我：科研不是生活的全部。<br><strong>（1）我这一个月</strong><br>9月5日-10月21日，博士开学一个月了。<br>或许是读硕时研一放浪一年，后续虽然跟上，依旧羞愧当时浪费了一年时光，未博士入学就下定决心，读博上来就要投入、要好好读文献、赶紧定方向、抓紧出成果。<br>于是，急于求成，和老师讨论了几次，说的最多的是不要急，慢慢来。刚听到时，觉得老师在鼓励，但听到的多了，自信心大受打击，内心开始反问：怎么想法总是和老师会有出入、达不到老师想要的结果、抓不到老师所说的方向？<br>昨天再次自信满满地和老师讨论，想着这次总能得到认可了。老师：回去再多看看文章，先把要解决什么问题定下来、如何解决，不要急着转模型出成果，慢慢来。<br>谈完话，心态是崩的，还是没有得到老师的认可、我可能不适合做科研。<br><strong>（2）负反馈</strong><br>开学这段时间，已经好久没有的痘痘，开始集中式爆发，一狠心咬咬牙买神仙水的想法闪过无数次。但内心也清醒的知道，爆痘和近段时间的焦虑、缺乏锻炼有关。心态和体力没调整好，外用的护肤水不会从根本上解决掉问题。<br>我知道，要解决和调整现在的状态，最适合自己的是运动。<br>但是因为科研，内心有个小人在说，等这次汇报结束、等周二和老师讨论完、等老师对自己的进展有了实际性的肯定，而不是鼓励，再去锻炼和放松，到时要好好睡两天、好好出出汗。<br>就这样，一个月内，无数次陷入上述的自我安慰式焦灼。要有进展、要有方案、要有结果像百米冲刺一样，搞得我心态紧绷。<br>但是，结果好吗？<br>不好，因为想要短时间内得到进展和任何，当老师一次次说多读文献、慢慢来时，我开始怀疑自己是不是不适合科研，而这进一步影响我的心态，看着看着会分心、越看越焦虑……<br><strong>（3）有了一些反思：科研不是百米冲刺</strong><br>今早自习室坐着，看到那篇文章，我才猛然惊觉，近段时间我把科研当做百米冲刺，一心只想要成果。<br>但是如果下定决心做科研，是要做好与科研长时间并肩的准备。<br>长时间是多久？读博的3年、4年，也有可能是未来的30年、40年，甚至一辈子。<br>科研是生活的一部分。一段时间只有科研，为了科研把其它事情抛开的想法本就是不对的。<br>慢下来，沉下心。要运动的时候去运动，该看文献的时候读文献。<br>有感于2021年10月21日。</p><hr><blockquote><p>作者：Kelsey夏至<br>链接：<a href="https://www.zhihu.com/question/470047139/answer/2042518160">https://www.zhihu.com/question/470047139/answer/2042518160</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>把research gate当成朋友圈来刷，把读文献当成刷知乎，影响因子当成知乎点赞，把发文献的大佬当成知乎大v。</p><p>大v引用的文章当然要点开看看，知乎高赞当然也要点开看看，发现了意想不到的工艺，可以点个赞，发现了未曾涉猎的干货内容，自然要收藏，发现了有深度的研究，自然要细品。</p><p>有人发了个菜谱，你大概看了下发现你的实验室也可以炒来试试，然后一开始你发现你手残做不出来，不服气，多做几次，终于做的和原博主一样好吃，但是距离合你口味还差点意思，于是你不停试验，嗯，你喜欢吃口味重的，那就加大调料的比例，你喜欢吃辣，那放点辣椒面会如何呢，研发黑暗料理的过程，是你想象不到的有趣，终于，你发现你多放了勺盐，菜更好吃了，而少放点醋改放辣椒面儿，又是意想不到的风味，你尝到那道菜的时候内心激动，忍不住想跟全世界的网友分享，让大家都知道这个菜还有很多不一样的吃法。</p><p>这个世界上有那么多菜，有那么多调料，不同的工艺和排列组合自然也是有不同的口感和味道，这个世界上有很多人在自己的实验室试菜，有人研究菜本身，有人研究调料，有人研究工艺，有的人致力于口感更好，有的人致力于味道更好，有的人致力于菜品繁复精致，有的人致力于普通的菜也可以绽放不一样的光彩。</p><p>于是你发现了新世界，你也有好多方案想要尝试，你很期待出锅后菜品的色泽，味道，口感会是什么样。</p><p>于是就不枯燥了。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理有哪些方向适合独立研究？</title>
      <link href="/2021/11/28/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E5%90%91%E9%80%82%E5%90%88%E7%8B%AC%E7%AB%8B%E7%A0%94%E7%A9%B6%EF%BC%9F/"/>
      <url>/2021/11/28/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E5%90%91%E9%80%82%E5%90%88%E7%8B%AC%E7%AB%8B%E7%A0%94%E7%A9%B6%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>我来猜一猜motivation叭。题主是不是正在转行做NLP，资源有限，想找一个方向做一定的深度，最好能独立水水paper或者打打比赛做点项目，进而找一份NLP相关的工作？(￣∇￣)</p><p>如果是的话，就要兼顾研究难度和工业界缺口了。</p><p>从入门难度，对计算资源的需求以及工业界缺口的角度看，个人比较推荐的路线是：</p><p><strong>先入门分类，后熟悉匹配，进而根据个人兴趣去做对话/问答/推荐。</strong></p><p>原因很简单，文本分类是NLP里最简单的任务，无论理论还是工程实现难度。而从分类到匹配又是十分平滑的，而且分类和匹配任务都有小中大不同规模的公开数据集，也有小中大不同复杂程度的代表模型，预处理相对容易很多，理论也不难，开源资料、博客之类的又很多，工业界还需求超大（对话、问答、检索、推荐、反作弊等场景都有一堆分类和匹配需求）。自己花点时间多看多写，基础不是太差的话半年到一年就可以独立炼丹，刷刷榜单了。</p><p>有了分类和匹配的基础，做对话、问答和推荐等方向非常容易。如果数据预处理做的比较熟，对数据挖掘兴趣更高一些，可以重点了解下推荐、反作弊/风控等场景下的NLP问题；如果对检索系统或问答对话更感兴趣，可以重点了解下检索式问答（检索+排序的框架就好，不用着急碰机器阅读理解问题）、检索式闲聊机器人、FAQ系统怎么做；精力旺盛的还可以简单了解一下任务型对话系统和机器阅读理解，不过这两个问题不建议一个人瞎搞。</p><p>然后说下分类和匹配可以打卡的几个点：</p><p>文本分类来说，熟悉一下bow、ngram、tfidf这类基本概念，了解下NLP里常用的特征，配合NB、SVM就能轻松体验特征工程解决文本分类的基本套路；熟悉深度学习框架后，textcnn、fasttext、adasent、dpcnn甚至bert都可以拿来跑下，研究下代码；此外，文本分类数据集大多结构简单，还可以顺便学点基本的正则语法做做简单的文本预处理（可能都用不到正则）。</p><p>入门分类后，做匹配任务就是多了一个输入的文本分类，主要就是额外理解一下花式attention，以及区分下pointwise，pairwise甚至listwise的训练方式，提高一下写复杂网络和调参的能力。尤其可以跑实验感受一下query-query匹配与query-response匹配有什么不同，思考一下为什么，加深对NLP和深度学习理论的理解，从无脑炼丹走向有理有据。这时候该怎么继续往下走也不用我多说啦</p><p>匹配任务也可以搞定了之后，就要多多关注其他方向的东西啦，比如序列标注问题里的CRF是什么，机器翻译里提出的transformer的设计细节，语义表示相关的ELMo、BERT等为什么work，生成相关的VAE、copy机制等在解决什么问题，以及对抗样本、多任务学习、迁移学习、FSL、NAS、灾难性遗忘与持续学习等ML问题等。当然啦，有条件熟悉一下多机多卡训练更好了，这都是在工业界做NLP必备的工程能力。</p><p>最后，加油嗷(^_−)−☆</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何系统性地学习NLP 自然语言处理？</title>
      <link href="/2021/11/28/%E5%A6%82%E4%BD%95%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%9C%B0%E5%AD%A6%E4%B9%A0NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%9F/"/>
      <url>/2021/11/28/%E5%A6%82%E4%BD%95%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%9C%B0%E5%AD%A6%E4%B9%A0NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p><strong>如何系统性地学习NLP 自然语言处理？当前NLP技术发展非常迅速，但是缺少系统性的学习资料</strong></p><hr><blockquote><p>作者：杨夕<br>链接：<a href="https://www.zhihu.com/question/27529154/answer/1810995812">https://www.zhihu.com/question/27529154/answer/1810995812</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>分享一个高 star 的 github 项目，该项目目录如下：</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-nlp-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 NLP】 那些你不知道的事</a></p></li><li><ul><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E7%9B%AE%E5%BD%95">目录</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E4%BB%8B%E7%BB%8D">介绍</a></p></li><li><ul><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E8%AE%BA%E6%96%87%E5%B7%A5%E5%85%B7%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 论文工具】那些你不知道的事</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E4%BC%9A%E8%AE%AE%E6%94%B6%E9%9B%86%E7%AF%87">会议收集篇</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23nlp-%E5%AD%A6%E4%B9%A0%E7%AF%87">NLP 学习篇</a></p></li><li><ul><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E7%BB%8F%E5%85%B8%E4%BC%9A%E8%AE%AE%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E7%AF%87">经典会议论文研读篇</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AF%87">理论学习篇</a></p></li><li><ul><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%E7%AF%87">经典论文研读篇</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-transformer--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 transformer 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 预训练模型】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 信息抽取】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体关系联合抽取】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 命名实体识别】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 关系抽取】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%96%87%E6%A1%A3%E7%BA%A7%E5%88%AB%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文档级别关系抽取】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 知识图谱 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8C%87%E7%AF%87-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体链指篇】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%AE%9E%E4%BD%93%E6%B6%88%E6%AD%A7--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 实体消歧 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8Ekgqa--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于KGQA 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8Eneo4j---%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于Neo4j 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E7%BB%86%E7%B2%92%E5%BA%A6%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 细粒度情感分析】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 主动学习】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 对抗训练】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-gcn-in-nlp-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 GCN in NLP 】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本预处理】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于问答系统】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本摘要】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本匹配】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 机器翻译】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 文本生成】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 对话系统】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90nlg-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 自然语言生成NLG 】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-e2e-%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 E2E 】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-rasa--%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 Rasa 】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E9%82%A3%E4%BA%9B%E7%9A%84%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 半监督学习】 那些的你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-nlp%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 NLP分类任务】那些你不知道的事</a></li><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%85%B3%E4%BA%8E-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E9%82%A3%E4%BA%9B%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B">【关于 中文分词】那些你不知道的事</a></li></ul></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E5%AE%9E%E6%88%98%E7%AF%87">实战篇</a></p></li><li><ul><li><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E9%87%8D%E7%82%B9%E6%8E%A8%E8%8D%90%E7%AF%87">重点推荐篇</a></li></ul></li></ul></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23elastrsearch-%E5%AD%A6%E4%B9%A0%E7%AF%87">Elastrsearch 学习篇</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%AD%A6%E4%B9%A0%E7%AF%87">推荐系统 学习篇</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23%E7%AB%9E%E8%B5%9B%E7%AF%87">竞赛篇</a></p></li><li><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study%23gcn_study%E5%AD%A6%E4%B9%A0%E7%AF%87">GCN_study学习篇</a></p></li></ul></li></ul></li></ul><p>针对每一篇论文都有特别好的介绍：</p><p><img src="https://pic3.zhimg.com/50/v2-be7718b5c1823e25648f3ad4f2a03d69_720w.jpg?source=1940ef5c" alt="img"></p><p>项目地址：</p><p><a href="https://link.zhihu.com/?target=https://github.com/km1994/nlp_paper_study">https://github.com/km1994/nlp_paper_study</a></p><hr><blockquote><p>作者：李rumor<br>链接：<a href="https://www.zhihu.com/question/27529154/answer/1643865710">https://www.zhihu.com/question/27529154/answer/1643865710</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>分享一下我的学习路径，<strong>正常的理工科大学生大概三四个月可以入门（学过C语言、线性代数、概率论），独立解决NLP问题，搭建baseline并适当优化效果</strong></p><p><strong>注：文中脑图的下载地址、NLP学习群进入的方式都放在文末啦～</strong></p><p>机器学习是一门既重理论又重实践的学科，想一口吃下这个老虎是不可能的，因此学习应该是个<strong>循环且逐渐细化</strong>的过程。</p><p>首先要有个全局印象，知道minimum的情况下要学哪些知识点：</p><p><img src="https://pic1.zhimg.com/50/v2-a59ca4108d9acdb449a01599d208ca47_720w.jpg?source=1940ef5c" alt="img"></p><p>之后就可以开始逐个击破，但也不用死磕，控制好目标难度，先用三个月时间进行第一轮学习：</p><ol><li>读懂机器学习、深度学习原理，不要求手推公式</li><li>了解经典任务的baseline，动手实践，看懂代码</li><li>深入一个应用场景，尝试自己修改模型，提升效果</li></ol><p>迈过了上面这道坎后，就可以重新回归理论，提高对自己的要求，比如<strong>手推公式、盲写模型、拿到比赛Top</strong>等。</p><h3 id="Step1-基础原理"><a href="#Step1-基础原理" class="headerlink" title="Step1: 基础原理"></a><strong>Step1: 基础原理</strong></h3><p>机器学习最初入门时对数学的要求不是很高，掌握基础的线性代数、概率论就可以了，正常读下来的理工科大学生以上应该都没问题，可以直接开始学，碰到不清楚的概念再去复习。</p><p>统计机器学习部分，建议初学者先看懂<strong>线性分类、SVM、树模型和图模型</strong>，这里推荐李航的「统计学习方法」，薄薄的摸起来没有很大压力，背着也方便，我那本已经翻四五遍了。喜欢视频课程的话可以看吴恩达的「CS229公开课」或者林田轩的「机器学习基石」。但不管哪个教程，都不必要求一口气看完吃透，第一轮先重点看懂以下知识点就够了：</p><p><img src="https://pic2.zhimg.com/50/v2-f642e796654dd6dc2d6f4bc03a2e0e04_720w.jpg?source=1940ef5c" alt="img"></p><p>深度学习部分，推荐吴恩达的「深度学习」网课、李宏毅的「深度学习」网课或者邱锡鹏的「神经网络与深度学习」教材。先弄懂神经网络的反向传播推导，然后去了解词向量和其他的编码器的核心思想、前向反向过程：</p><p><img src="https://pic3.zhimg.com/50/v2-6f681ffa5ba0ba68118f0c6a3df27e2e_720w.jpg?source=1940ef5c" alt="img"></p><h3 id="Step2-经典模型与技巧"><a href="#Step2-经典模型与技巧" class="headerlink" title="Step2: 经典模型与技巧"></a><strong>Step2: 经典模型与技巧</strong></h3><p>有了上述的基础后，应该就能看懂模型结构和论文里的各种名词公式了。接下来就是了解NLP各个经典任务的baseline，并看懂源码。对于TF和Pytorch的问题不用太纠结，接口都差不多，找到什么就看什么，自己写的话建议Pytorch。</p><p>快速了解经典任务脉络可以看综述，建议先了解一两个该任务的经典模型再去看，否则容易云里雾里：</p><ul><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2008.00364v2.pdf">2020 A Survey on Text Classification: From Shallow to Deep Learning</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2011.06727">2020 A Survey on Recent Advances in Sequence Labeling from Deep Learning Models</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2004.13820">2020 Evolution of Semantic Similarity - A Survey</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1711.09534">2017 Neural text generation: A practical guide</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1803.07133.pdf">2018 Neural Text Generation: Past, Present and Beyond</a></li><li><a href="https://link.zhihu.com/?target=https://www.sciencedirect.com/science/article/pii/S1319157820303360">2019 The survey: Text generation models in deep learning</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2009.06732">2020 Efficient Transformers: A Survey</a></li></ul><h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a><strong>文本分类</strong></h3><p>文本分类是NLP应用最多且入门必备的任务，<strong>TextCNN</strong>堪称第一baseline，往后的发展就是<strong>加RNN、加Attention、用Transformer、用GNN</strong>了。第一轮不用看得太细，每类编码器都找个代码看一下即可，顺便也为其他任务打下基础。</p><p><img src="https://pica.zhimg.com/50/v2-356c3269de020c2eb15fe934617cb2c4_720w.jpg?source=1940ef5c" alt="img"></p><p>但如果要做具体任务的话，建议倒序去看SOTA论文，了解各种技巧，同时善用知乎，可以查到不少提分方法。</p><h3 id="文本匹配"><a href="#文本匹配" class="headerlink" title="文本匹配"></a><strong>文本匹配</strong></h3><p>文本匹配会稍微复杂些，它有双塔和匹配两种任务范式。双塔模型可以先看<strong>SiamCNN</strong>，了解完结构后，再深入优化编码器的各种方法；基于匹配的方式则在于句子表示间的交互，了解BERT那种TextA+TextB拼接的做法之后，可以再看看阿里的<strong>RE2</strong>这种轻量级模型的做法：</p><p><img src="https://pica.zhimg.com/50/v2-4d273391f8370b2fd1ed13ba61e1a281_720w.jpg?source=1940ef5c" alt="img"></p><h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a><strong>序列标注</strong></h3><p>序列标注主要是对Embedding、编码器、结果推理三个模块进行优化，可以先读懂<strong>Bi-LSTM+CRF</strong>这种经典方案的源码，再去根据需要读论文改进。</p><p><img src="https://pic2.zhimg.com/50/v2-ca71074d0e8f701ea5370c5cbdd9d7b9_720w.jpg?source=1940ef5c" alt="img"></p><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a><strong>文本生成</strong></h3><p>文本生成是最复杂的，具体的SOTA模型我还没梳理完，可以先了解Seq2Seq的经典实现，比如基于<strong>LSTM的编码解码+Attention、纯Transformer、GPT2以及T5</strong>，再根据兴趣学习VAE、GAN、RL等。</p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a><strong>语言模型</strong></h3><p>语言模型虽然很早就有了，但18年BERT崛起之后才越来越被重视，成为NLP不可或缺的一个任务。了解BERT肯定是必须的，有时间的话再多看看后续改进，很经典的如<strong>XLNet、ALBERT、ELECTRA</strong>还是不容错过的。</p><p><img src="https://pic3.zhimg.com/50/v2-4a36d06e9c38380945ee0c8591dff984_720w.jpg?source=1940ef5c" alt="img"></p><h3 id="Step3-实践优化"><a href="#Step3-实践优化" class="headerlink" title="Step3: 实践优化"></a><strong>Step3: 实践优化</strong></h3><p>上述任务都了解并且看了一些源码后，就该真正去当炼丹师了。千万别满足于跑通别人的github代码，最好去参加一次Kaggle、天池、Biendata等平台的比赛，享受优化模型的摧残。</p><p>Kaggle的优点是有各种kernel可以学习，国内比赛的优点是中文数据方便看case。建议把两者的优点结合，比如参加一个国内的文本匹配比赛，就去kaggle找相同任务的kernel看，学习别人的trick。同时多看些顶会论文并复现，争取做完一个任务后就把这个任务技巧摸清。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>其实自己刚开始学的时候只到了minimum的水平，在后续的实践、面试中才逐渐把知识点补全，并归到自己的体系里。刚入门的同学们也不要气馁，先懵懵懂懂地看一看原理，哆哆嗦嗦地跑一跑代码，时间会给你答案。<strong>怀疑自己的时候，就算算自己到底学了多久，没到一万小时之前都还来得及</strong>。</p><p>-–</p><p>欢迎初入NLP领域的小伙伴们加入rumor建立的「<strong>NLP卷王养成群</strong>」一起学习，添加微信「<strong>leerumorrr」</strong>备注<strong>知乎+NLP</strong>即可，群里的讨论氛围非常好～</p><p>-–</p><p>入门路线和各任务详解都在这里下载啦～</p><p><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/zrziuFLRvbG8axG48QpFvg">NLP快速入门路线及任务详解下载</a></p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习线路推荐</title>
      <link href="/2021/11/28/NLP%E5%AD%A6%E4%B9%A0%E7%BA%BF%E8%B7%AF%E6%8E%A8%E8%8D%90/"/>
      <url>/2021/11/28/NLP%E5%AD%A6%E4%B9%A0%E7%BA%BF%E8%B7%AF%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>导读</strong>：大家好，我是机智的叉烧，这是我NLP.TM系列下的第23篇文章(部分文章还未更新到知乎中，微信公众号下有)。本文是自己在有一定经验，结合时代变迁，资料逐步完善后，回头总结的NLP学习线路。<br>更多文章欢迎关注：<br>我的专栏：数学·数据·计算机<br>我的公众号：CS的陋室</p></blockquote><p>之前写过一篇和这个非常接近的内容：</p><p><a href="https://zhuanlan.zhihu.com/p/64026730">机智的叉烧：我的NLP学习之路648 赞同 · 96 评论文章</a></p><p>开始谈之前，还是先说明一下我下面内容的思路和一些看法。</p><ul><li>内容在精不在多，我不会堆砌大量资料，而是精心选择一些适合各个阶段的一个或者几个。我发现大部分时候，收藏了大量资料你也不见得会学，那意义可就不大了。</li><li>和打游戏一样有简单模式和困难模式，简单模式玩起来舒服，但是收获会少，困难模式玩起来难受，但是收获会多，想要收获多没毛病，但是也要分析好自己的能力，量力而行，最重要的是，游戏要通关了，才会有最终的收获。</li><li>基础打好再来走下一步，否则寸步难行。</li><li>基础内容现在可以开始看中文了，本身基础不好看英文会很吃力，理解效果也不好。但是，英文欠的债迟早要还，前沿的内容很多都是英文了。</li><li>基础不好，根基不稳，迟早会出问题。</li><li>特定营销号制造的焦虑，没必要。</li><li>NLP领域，我感觉还没有非常完善的书本和课程，很多书也浅尝辄止，博客知识有比较零散不成体系。所以打开这里的注意一下，我关心的是你要具备什么和怎么具备，而不是简单的给你推材料，毕竟你的目标是学到东西而不是积累材料对吧。</li><li>理论和开发，两个都不能少。</li></ul><p>要是想看体系化的东西，我只推荐下面几个。</p><ul><li>CS224n课程，这应该是我看的NLP最为完善，理论和技术兼具的课程。笔记的话可以看看这个：<a href="https://link.zhihu.com/?target=https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/</a></li><li>统计自然语言处理、文本数据挖掘、基于深度学习的自然语言处理。理论综述，里面会有基础知识，但更多是论文，给了超级多的论文。</li><li>《tensorflow与自然语言处理应用》，这本书除了序列标注这块不是很完善，别的都还好。</li></ul><p>下面我来和大家谈谈，NLP领域的学习线路。</p><h2 id="先修"><a href="#先修" class="headerlink" title="先修"></a>先修</h2><p>下面的内容，默认你已经学会。</p><ul><li>基础的数学知识吧，高数线代概率统计。</li><li>统计学习方法内的知识已经覆盖。（包括条件随机场，nlp里面条件随机场很重要）</li><li>深度学习基础知识都没问题，全连接，CNN，RNN，GRU，LSTM等。</li><li>编程能力没太大问题。（可以理解为，你有个严谨的想法，要翻译为代码的能力，具体可以体现在你做算法题的能力）</li></ul><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>我的理解，满足下面条件，你就算入门了：</p><ul><li><p>知道自然语言处理是什么，具体任务有哪些。</p></li><li><p>文本的一些基础预处理，例如分词等。</p></li><li><p>基础简单的任务的基础解决方案及其理论基础。</p></li><li><ul><li>基础的语言模型。词袋模型（onehot、TFIDF）word2vector必须理解完全，GloVe选学。</li><li>文本分类。基于词袋模型的文本分类，fasttext（最简单快捷的文本分类模型）、TextCNN。</li><li>序列标注，命名实体识别。CRF、LSTM-CRF、BiLSTM-CRF。</li><li>匹配问题。BM25、DSSM。</li></ul></li><li><p>有关的技术实现。个人还是推荐tensorflow，优先搞1。</p></li></ul><p>例如Transformer、bert之类的东西，都不算基础，主要有几个原因：</p><ul><li>基础知识的理解对后续解决问题能力的提升非常重要，让你不关注于模型本身，而是解决问题，甚至是特定场景下的问题。</li><li>这个点非常重要，类似word2vector之类的玩意，现在还被广泛使用，未被淘汰，transformer之类的使用门槛太高，无论是时间代价还是空间代价都很高，实现成本也不低，因此这些都不是你的第一选择。</li></ul><p>有关NLP是什么，建议大家看看百度百科，另外是一些NLP的教材和专著，例如统计自然语言处理，基于深度学习的自然语言处理，看一下书的第一章和最后一章，还有目录，你就会大概理解这个概念，对一个学科有了“边界”的概念。书因为具有一定的门槛，所以乱说的会少一些，</p><p>预处理处理，主要是针对中文，我列举出来有这些，这些可以通过百度技术博客快速习得：</p><ul><li>标点符号处理——这个很基础，正则表达式即可处理。</li><li>繁体转简体。看这个：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/wds2006sdo/article/details/53583367">https://blog.csdn.net/wds2006sdo/article/details/53583367</a></li><li>分词，最简单的还是jieba，适配的环境也比较多，主流的c++、python、java都需要会。在现实应用中，尤其是大项目，分词由于属于很上游且对下游影响重大的任务，因此选定后一般不换。（了解jieba的回答我一下，jieba里面的分词用的是那种模型？）</li><li>英文的话补充一点点，NLTK基本能覆盖所有功能了。</li></ul><p>好了终于要开始讲模型了，希望大家能看下去吧。</p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型的突破性进展几乎都是NLP发展的里程碑，毕竟语义理解和抽象化都来源于上游的语言模型。</p><p>首先是词袋模型，one hot和TF-IDF是比较常见的，这块难度不是很高，百度和知乎找博客学就好了。</p><p>word2vector这块，首先是基本版的大家最好能理解，一方面理论明白，对于具体的细节，大家可以看看word2vector的源码，有关H树、负采样之类的。这方面要看详情，我很建议大家看看上面CS224N里面涉及word2vector的章节，就在前几章，中文笔记也有的。具体实现可以使用gensim，自己做试验还是够用的。</p><p><a href="https://zhuanlan.zhihu.com/p/70606283">机智的叉烧：NLP.TM | 再看word2vector10 赞同 · 0 评论文章</a></p><p>GloVe我之前就写过，看这里。</p><p><a href="https://zhuanlan.zhihu.com/p/60392680">机智的叉烧：NLP.TM | GloVe模型的原理和实现8 赞同 · 4 评论文章</a></p><p>需要补充得是，主题模型，其实这块我也理解为一种文本表示的方法，如LDA等，理论可参考一些博客，另外是gensim包和LDA之类的。</p><h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><p>文本分类应该是NLP里面比较基础的任务了，如果要完成工程任务，非常推荐fasttext，但是建议还是能从词袋模型+机器学习的模式学到点东西。这块比较简单，到了这里词袋模型到这步大家应该了解，加上机器学习是先修，大家已经明白，至于实现上，大家可以看看网上垃圾邮件分类的例子，是一个词袋模型+朴素贝叶斯的例子。</p><p>fasttxt有两个实现方法，一个是fasttext的原生工具包，另一个是gensim。说原理，原理和word2vector非常接近，如果你word2vector能看懂，其实这个也不会很难，给个博客吧：<a href="https://link.zhihu.com/?target=http://cnblogs.com/huangyc/p/9768872.html">http://cnblogs.com/huangyc/p/9768872.html</a>。</p><p>说起这个gensim，出镜率不低，他其实是一个和NLP有关的工具包，里面含盖了很多工具，希望大家可以花大概1天左右看看里面的API文档，知道里面有什么。（相似的还有sklearn）</p><h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>序列标注其实是一个文本分类的升级版问题，难度也随之提升，这块其实我的公众号谈的比较多了，分享给大家吧。</p><p>这块的学习目标如下：</p><ul><li>知道序列标注的问题的实质以及核心解决思路，即能不能把具体问题抽象出来。</li><li>常见模型。从CRF开始到BILSTM-CRF等，深度学习能理解的话，其实这些也不会太难。</li><li>这里挺考验大家得是tensorflow的使用吧。</li></ul><h3 id="匹配问题"><a href="#匹配问题" class="headerlink" title="匹配问题"></a>匹配问题</h3><p>匹配问题其实并不常见，但是我想提到的一个核心点在于多输入的问题，另外有关相似度衡量的问题，也能通过学习产生理解。这里面的模型就建议大家看看DSSM即可，这块比较建议大家直接看论文，网络上比较多博客对DSSM的解读存在问题。</p><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><p>这里的进阶，只是让自己能更好地往前沿走，具体的个人成长应该从这块完成后开始。这一块我只想大家对BERT带来的新模式有一个比较完整的理解，有条件的能自己启起来玩，没有条件会比较尴尬吧。</p><p>涉及下面内容。</p><ul><li>Transformer。来自论文Attention is all you need。可能需要一些seq2seq的知识，但是只需要知道他在干什么就好，核心还是要知道transformer是什么。（不是变形金刚！！）这里的辅助材料建议看知乎。</li><li>不建议直接学bert，建议从EMLO开始，语言模型整套整一遍，这整理了一整套，阅读量不大，额，我的理解可能写的有点深入，大家可以以哪个为提纲作为学习参考，这里就包含了可能很多人想学的bert，大家可以体验一下一开始就学bert和现在再来接触的感觉有什么不同。</li><li>bert建议看一遍源码。</li></ul><p>由于我感觉大家学到这里已经基本能有一定的代码能力和自学能力，能找到适合自己的材料，所以这里的材料不单独列啦，百度知乎谷歌github走起来。</p><h3 id="自由发展之路"><a href="#自由发展之路" class="headerlink" title="自由发展之路"></a>自由发展之路</h3><p>进入工作后好多前辈和我聊，对我的建议更多是有一个方向能有更加深刻的了解，所以建议大家也是找一个方向，做点有深入的学习和尝试。具体会有这些方向吧。大方向是分为NLU（理解）和NLG（生成）。</p><ul><li><p>NLU其实上面大家了解了很多基础了，这里就是一些更加有深度的模型了。</p></li><li><ul><li>文本分类。</li><li>序列标注。</li><li>语义匹配。 </li></ul></li><li><p>NLG重在生成，这是比较前沿的方向了，但是现实应用目前看的并不多。</p></li><li><ul><li>自动摘要。</li><li>阅读理解。</li><li>机器翻译。</li></ul></li></ul><p>具体怎么学习呢，首先是内容上，建议大家走这个路线。</p><ul><li>bert之类的，尽可能尝试用上去，例如序列标注，可以试试bert+bilstm+crf。</li><li>阅读论文和专利，不要局限在近几年的热文，也要多看看13年后的文章（具体数是个人经验吧），看看别人是怎么做的，可以从综述入手找论文来看。</li><li>有一个固定的数据集，也可以是比赛，自己尝试尽可能多的模型。</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>内容会比较零散，主要以阐述知识点为主，从上面我的学习经历也可以看到，要是你真的想走在前面，那其实真没有很多很好的材料能覆盖，这就注定是一个需要自己独立完成的过程，困难和弯路固然是有，但是那些没打到我的终究让我变得强大，有问题可以随时沟通，各位共勉。</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP打怪升级路线图：基础概念、常用方法、最佳模型，知识点全覆盖 | 萌新成长必备</title>
      <link href="/2021/11/28/NLP%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7%E8%B7%AF%E7%BA%BF%E5%9B%BE%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E3%80%81%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E3%80%81%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A8%E8%A6%86%E7%9B%96-%E8%90%8C%E6%96%B0%E6%88%90%E9%95%BF%E5%BF%85%E5%A4%87/"/>
      <url>/2021/11/28/NLP%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7%E8%B7%AF%E7%BA%BF%E5%9B%BE%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E3%80%81%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E3%80%81%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A8%E8%A6%86%E7%9B%96-%E8%90%8C%E6%96%B0%E6%88%90%E9%95%BF%E5%BF%85%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>栗子 发自 凹非寺<br>量子位 报道 | 公众号 QbitAI</p></blockquote><p>一只萌新，想把自己修炼成一个成熟的<strong>NLP</strong>研究人员，要经过一条怎样的路？</p><p>有个名叫Tae-Hwan Jung的韩国小伙伴，做了一份完整的<strong>思维导图</strong>，从基础概念开始，到NLP的常用方法和著名算法，知识点全面覆盖。</p><p>可以说，从0到1，你需要的都在这里了：</p><p><img src="https://pic4.zhimg.com/v2-2714295fbadec169724762866d3289bf_b.jpg" alt="img"></p><p><video class="ztext-gif GifPlayer-gif2mp4" src="https://vdn1.vzuu.com/SD/f3aa6576-232f-11eb-899c-5aa0cfc3f9ec.mp4?disable_local_cache=1&amp;auth_key=1638033654-0-0-99f295570340f6a4575a8207e9ff4394&amp;f=mp4&amp;bu=pico&amp;expiration=1638033654&amp;v=hw" data-thumbnail="https://pic4.zhimg.com/v2-2714295fbadec169724762866d3289bf_b.jpg" poster="https://pic4.zhimg.com/v2-2714295fbadec169724762866d3289bf_b.jpg" data-size="normal" preload="metadata" loop="" playsinline=""></video></p><p>这份精致的资源刚刚上线，不到一天Reddit热度就超过<strong>400</strong>，获得了连篇的赞美和谢意：</p><p>“肥肠感谢。”“我需要的就是这个！”“哇，真好啊！”</p><p><img src="https://pic2.zhimg.com/80/v2-e564720bd7a69a791c4836ddac125e81_1440w.jpg" alt="img"></p><p>所以，这套丰盛的思维导图，都包含了哪些内容？</p><h2 id="四大版块"><a href="#四大版块" class="headerlink" title="四大版块"></a><strong>四大版块</strong></h2><p>就算你从前什么都不知道，也可以从第一个版块开始入门：</p><h3 id="1-概率-amp-统计"><a href="#1-概率-amp-统计" class="headerlink" title="1 概率&amp;统计"></a><strong>1 概率&amp;统计</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-13038060492317d128dcbe2dd38ae97d_1440w.jpg" alt="img"></p><p>从中间的灰色方块，发散出5个方面：</p><p><strong>基础</strong> (Basic) ，<strong>采样</strong> (Sampling) 、<strong>信息理论</strong> (Information Theory) 、<strong>模型</strong> (Model) ，以及<strong>贝叶斯</strong> (Bayesian) 。</p><p>每个方面，都有许多知识点和方法，需要你去掌握。</p><p>毕竟，有了概率统计的基础，才能昂首挺胸进入第二个板块。</p><h3 id="2-机器学习"><a href="#2-机器学习" class="headerlink" title="2 机器学习"></a><strong>2 机器学习</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-fc184e3f81bb0bde116d5e2c04d7ada2_1440w.jpg" alt="img"></p><p>这个版块，一共有7个分支：</p><p><strong>线性回归</strong> (Linear Regression) 、<strong>逻辑回归</strong> (Logistic Regression) 、<strong>正则化</strong> (Regularization) 、<strong>非概率</strong> (Non-Probabilistic) 、<strong>聚类</strong> (Clustering) 、<strong>降维</strong> (Dimensionality Reduction) ，以及<strong>训练</strong> (Training) 。</p><p>掌握了机器学习的基础知识和常用方法，再正式向NLP进发。</p><h3 id="3-文本挖掘"><a href="#3-文本挖掘" class="headerlink" title="3 文本挖掘"></a><strong>3 文本挖掘</strong></h3><p>文本挖掘，是用来从文本里获得高质量信息的方法。</p><p><img src="https://pic2.zhimg.com/80/v2-77aec069bb9cc43138018b4a899223cd_1440w.jpg" alt="img"></p><p>图上有6个分支：</p><p><strong>基本流程</strong> (Basic Procedure) 、<strong>图</strong> (Graph) 、<strong>文档</strong> (Document) 、<strong>词嵌入</strong> (Word Embedding)、<strong>序列标注</strong> (Sequential Labeling) ，以及<strong>NLP基本假设</strong> (NLP Basic Hypothesis)。</p><p>汇集了NLP路上的各种必备工具。</p><h3 id="4-自然语言处理"><a href="#4-自然语言处理" class="headerlink" title="4 自然语言处理"></a><strong>4 自然语言处理</strong></h3><p>装备齐了，就该实践了。这也是最后一张图的中心思想：</p><p><img src="https://pic1.zhimg.com/80/v2-6a660a77c3b096b49cddc8060cad19ec_1440w.jpg" alt="img"></p><p>虽然只有4个分支，但内容丰盛。</p><p>一是<strong>基础</strong> (Basic) ，详细梳理了NLP常用的几类网络：循环模型、卷积模型和递归模型。</p><p>二是<strong>语言模型</strong> (Language Model) ，包含了<strong>编码器-解码器模型</strong>，以及<strong>词表征到上下文表征</strong> (Word Representation to Contextual Representation) 这两部分。许多著名模型，比如<strong>BERT</strong>和<strong>XLNet</strong>，都是在这里得到了充分拆解，也是你需要努力学习的内容。</p><p>三是<strong>分布式表征</strong> (Distributed Representation) ，许多常用的词嵌入方法都在这里，包括<strong>GloVe</strong>和<strong>Word2Vec</strong>，它们会一个个变成你的好朋友。</p><p>四是<strong>任务</strong> (Task) ，机器翻译、问答、阅读理解、情绪分析……你已经是合格的NLP研究人员了，有什么需求，就调教AI做些什么吧。</p><p>看完脑图，有人问了：是不是要把各种技术都实现一下？</p><p>韩国少年说：</p><blockquote><p>不不，你不用把这些全实现一遍。找一些感觉有趣的，实现一波就好了。</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-9f129582579cd4c44c233b16db36eaff_1440w.jpg" alt="img">△ 作者Tae-Hwan Jung，来自庆熙大学</p><h2 id="One-More-Thing"><a href="#One-More-Thing" class="headerlink" title="One More Thing"></a><strong>One More Thing</strong></h2><p>Reddit楼下，许多小伙伴对这套脑图表示膜拜，并且想知道是用什么做的。</p><p>韩国少年说，<strong>Balsamiq Mockups</strong>。</p><p><img src="https://pic3.zhimg.com/80/v2-98508f1e95506ab45c7e535e6ae23486_1440w.jpg" alt="img"></p><p>GitHub传送门：</p><p><a href="https://link.zhihu.com/?target=https://github.com/graykode/nlp-roadmap">graykode/nlp-roadmap</a></p><p><a href="https://link.zhihu.com/?target=https://github.com/graykode/nlp-roadmap">github.com/graykode/nlp-roadmap<img src="https://pic3.zhimg.com/v2-866e32ed251eb1776a478472e1c69d1a_ipico.jpg" alt="img"></a></p><p>Reddit传送门：</p><p><a href="https://link.zhihu.com/?target=https://www.reddit.com/r/MachineLearning/comments/d8jheo/p_natural_language_processing_roadmap_and_keyword/">https://www.reddit.com/r/MachineLearning/comments/d8jheo/p_natural_language_processing_roadmap_and_keyword/</a></p><p><a href="https://link.zhihu.com/?target=https://www.reddit.com/r/MachineLearning/comments/d8jheo/p_natural_language_processing_roadmap_and_keyword/">www.reddit.com/r/MachineLearning/comments/d8jheo/p_natural_language_processing_roadmap_and_keyword/</a></p><p>— <strong>完</strong> —</p><p>量子位 · QbitAI</p><p>վ’ᴗ’ ի 追踪AI技术和产品新动态</p><p>戳右上角「+关注」获取最新资讯↗↗</p><p>如果喜欢，请分享or点赞吧~比心❤</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研究推荐系统要对NLP很了解吗？</title>
      <link href="/2021/11/28/%E7%A0%94%E7%A9%B6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A6%81%E5%AF%B9NLP%E5%BE%88%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
      <url>/2021/11/28/%E7%A0%94%E7%A9%B6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A6%81%E5%AF%B9NLP%E5%BE%88%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：蘑菇先生<br>链接：<a href="https://www.zhihu.com/question/317441966/answer/2186924692">https://www.zhihu.com/question/317441966/answer/2186924692</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>最近在读Recsys2021上的paper：<strong>Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation</strong>，文章也聊到了NLP发展和推荐系统发展的关系，可以说推荐系统是在NLP的肩膀上前进的。结论是，<strong>关注不同领域，尤其是NLP领域的发展，对于推荐系统的研究和工作很可能都会有帮助。</strong></p><p>我们不妨以这篇论文为例，来看看作者的研究动机以及推荐系统和NLP发展的关系。</p><h2 id="1-研究动机"><a href="#1-研究动机" class="headerlink" title="1.研究动机"></a><strong>1.研究动机</strong></h2><p>近年来，<strong>序列/会话推荐</strong>上的进展很多都源自于<strong>NLP模型和预训练技术</strong>的发展，可以说<strong>推荐系统是在NLP的肩膀上前进的</strong>。尤其是<strong>Transformers架构</strong>，在BERT等预训练模型中广泛应用，也在序列推荐中初露端倪。然而，<strong>推荐系统的发展实际上是滞后于NLP的</strong>。NLP中花式Transformers架构层出不穷，比如：邱锡鹏组的survey[2]集中展示了Transformer架构的演进；NLP中的开源社区也十分活跃，比如HuggingFace的<strong>开源库Transformers</strong>[4]涵盖了大部分主流的Transformer实现，总之，Transformers在NLP研究中正如火如荼地开展着。</p><p>然而，在<strong>推荐系统</strong>中的应用很多只停留在最原始的Transformer[3]。很大程度上是缺乏一个类似HuggingFace的统一轮子，研究者想在此基础上做迭代实际上相比于NLP会<strong>困难不少</strong>。</p><p>为了弥补这种发展鸿沟，作者开源了一个基于HuggingFace开源库[4]的序列推荐包<strong>Transformers4Rec</strong>，目的是希望推荐系统社区能够更快地follow到NLP社区在<strong>Transformers中的进展</strong>，并在序列/会话推荐任务中实现<strong>开箱即用</strong>。完整的代码开源在了<a href="https://link.zhihu.com/?target=https://github.com/NVIDIA-Merlin/Transformers4Rec/">github</a>。</p><h2 id="2-推荐系统和NLP的发展关系"><a href="#2-推荐系统和NLP的发展关系" class="headerlink" title="2. 推荐系统和NLP的发展关系"></a><strong>2. 推荐系统和NLP的发展关系</strong></h2><p>近年来，关于序列推荐的工作也是层出不穷，survey[5]集中展示了<strong>基于深度学习的序列推荐</strong>研究进展。在大部分的序列推荐场景中，可能都只用了用户最新的交互行为数据，或者由于用户是匿名的，所以只能用到<strong>当前会话session下的序列行为</strong>，这也就是典型的<strong>session会话推荐场景</strong>，属于序列推荐中的一种。</p><h3 id="2-1-推荐以NLP的发展为基础的原因"><a href="#2-1-推荐以NLP的发展为基础的原因" class="headerlink" title="2.1 推荐以NLP的发展为基础的原因"></a><strong>2.1 推荐以NLP的发展为基础的原因</strong></h3><p>在最近10年内，大部分序列推荐的工作是在NLP发展的基础上开展的，个人认为主要是因为三方面：</p><ul><li><strong>a. 推荐问题</strong>和<strong>NLP问题</strong>的<strong>抽象形式非常相似</strong>[6]，如付鹏大佬所言， 推荐系统(尤其是序列推荐)的基本问题可以抽象成求解 <img src="https://www.zhihu.com/equation?tex=p(%5Ctext%7Bitem_i%7D+%7C+%5Ctext%7Buser%7D,+%5Ctext%7Bhistory_item_i%7D,+%5Ctext%7Bhistory_item_2%7D,...,+%5Ctext%7Bhistory_item_k%7D)" alt="[公式]">，即求解指定<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bitem%7D" alt="[公式]">在指定<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Buser%7D" alt="[公式]">的历史行为记录下可能产生行为的联合条件概率。而NLP的<strong>语言模型</strong>中，第<img src="https://www.zhihu.com/equation?tex=i" alt="[公式]">个word的概率也正是类似的形式：<img src="https://www.zhihu.com/equation?tex=p(w_i%7Cw_%7Bi-1%7D,+w_%7Bi-2%7D,+...,+w_1)" alt="[公式]">。  当省略<img src="https://www.zhihu.com/equation?tex=user" alt="[公式]">的信息，只使用历史行为记录的时候，推荐系统和NLP的问题变得<strong>惊人相似</strong>。</li><li><strong>b. 文本是推荐系统重要的side information</strong>。推荐系统中的side information多种多样，文本是其中最重要的来源之一。因此，基于NLP技术对文本做特征提取并输入到推荐系统中进行建模，就是一种很自然的想法。这样的前提下，利用最新的nlp进展来提升<strong>文本的表达能力</strong>，从而提升推荐系统的性能，也是一种很自然的动机。</li><li><strong>c. NLP的研究起步远早于推荐系统</strong>。NLP最早可以追溯到20世纪40年代和50年代[7]，经历了符号、规则、统计、RNN、word2vec、表征学习到预训练模型等。而推荐系统最早能追溯到20世纪90年代[8]，由哥伦比亚大学的Jussi Karlgren教授在一份报告中提出，最早的协同过滤推荐也是在90年代才被提出的，经历了从协同过滤、基于内容的推荐、矩阵分解SVD、分解机FM、Facebook提出的GBDT+LR、Youtube提出的DNN、花式DNN、Graph Embedding，GNN等等。</li></ul><p><strong>NLP发展始终快推荐系统一步</strong>，而二者的<strong>抽象问题又非常相似</strong>，且文本是推荐系统中重要的<strong>side information</strong>，因此很容易出现<strong>推荐系统社区</strong>研究人员在<strong>NLP社区</strong>的研究进展的基础上，直接应用或改进后用到推荐系统中。</p><h3 id="2-2-早期非深度学习时代"><a href="#2-2-早期非深度学习时代" class="headerlink" title="2.2 早期非深度学习时代"></a><strong>2.2 早期非深度学习时代</strong></h3><p>早期<strong>非深度学习时代</strong>，推荐系统受NLP的影响工作例如：</p><ul><li><strong>TF-IDF</strong>，推荐中的item frequency和NLP中的word frequency存在同样的头部或长尾特点，因此TF-IDF在NLP中可以用于提取关键词，在推荐系统中也能用来推荐Item。  </li><li><strong>SVD,LSA</strong>，隐语义分析最早被拿来抽取NLP的词向量，在推荐系统中，矩阵分解同样适用于user或者item的向量表示，在Netflix比赛中SVD和进阶版SVD++夺得了冠军。  </li><li><strong>LDA</strong>，LDA最早用于NLP中的主题发现或关键词提取[31]，后来也应用到推荐系统领域做基于内容的推荐或可解释推荐，这部分工作在深度学习流行起来之前非常多，各种基于概率图模型的推荐和自编码器等，都是受到LDA的影响。比如：荣获<strong>KDD2021时间检验研究奖</strong>的工作：<strong>协同主题回归</strong>[30]，探讨了传统基于矩阵分解的协同过滤方法和主题模型LDA的融合，能够提供非常好的解释性和为用户建立画像标签。  </li></ul><h3 id="2-3-深度学习时代"><a href="#2-3-深度学习时代" class="headerlink" title="2.3 深度学习时代"></a><strong>2.3 深度学习时代</strong></h3><p>进入深度学习时代后，NLP对推荐系统的影响就更加深远了，先看一张图：</p><p><img src="https://pic2.zhimg.com/50/v2-2cfbef9100d7b2d75a80657bc1083d1c_720w.jpg?source=1940ef5c" alt="img"></p><ul><li><strong>word2vec</strong>，这个影响就非常深远了，Mikolov大佬的作品，最早用于做词向量表征。也广泛影响着推荐系统的研究： </li><li><strong>item2vec</strong>，直接把word sequence替换成用户的历史交互行为序列item sequence，就能取得很好的效果，如电商领域的Prod2vec[14]。此外在各种各样的比赛中，基本上也都会有用。核心是利用了item-item之间的共现性。</li><li><strong>DeepWalk</strong>，稍微进阶一点的，DeepWalk，除了预处理在图上游走形成<strong>序列之外</strong>，其余的都是照搬word2vec那一套，包括训练使用skip-gram也是源自word2vec的训练方法。话说回来，基于概率的随机游走生成序列的高效实现方法<strong>Alias Table</strong>，在<strong>LDA</strong>中也被<strong>玩过了</strong>。</li><li><strong>负采样</strong>，目前推荐系统中非常注重样本负采样，关于负采样策略的”鼻祖”，我觉得<strong>word2vec</strong>论文绝对是其中之一。如基于item频次分布构造层次哈夫曼树；基于item频次的unigram分布做随机负采样等。尤其在<strong>推荐系统</strong>的召回阶段广泛应用。</li><li><strong>RNN</strong>，NLP中常用的LSTM, GRU等也被广泛地应用在推荐系统序列推荐或者用作序列特征提取器中。经典的工作比如GRU4Rec等，还比如，推荐系统中引入文本side information时，经常使用LSTM/GRU来做特征提取器，并作为DNN的输入。</li><li><strong>Attention</strong>。说起Attention，最早的起源实际上源于cv领域，当人类观察外界事物的时候，一般不会把事物当成一个整体去看，往往倾向于根据需要选择性的去获取被观察事物的某些重要部分。在seq2seq流行之后，就广泛地应用在了<strong>NLP领域</strong>，最早的应用比如Bengio大佬的机器翻译工作[10]，在Decoder端用注意力机制引入Encoder的上下文信息，让Decoder自适应的选择合适的信息进行建模。在推荐系统中，最早的attention工作如：NARM[15] (Neural Attentive Recommendation Machine)和Attentional FM[16]。而影响比较深远、且更经典的Attention应用是<strong>DIN</strong> [11] (Deep Interest Networks)，使用目标item对历史行为序列做attention。这些工作开辟了attention在推荐系统中应用的浪潮。</li><li><strong>Transformers</strong>。Transformers中最经典的结构非self-attention莫属。在推荐系统中，不仅Transformers本身应用广泛[13,17]，包括核心结构self-attention机制、多头机制等也被广泛应用，如经典的SR-GNN[12]就用了self-attention。</li><li><strong>BERT</strong>。BERT对推荐系统的影响在近两年来也产生了不少工作，例如BERT4Rec[18]，引入了双向Self-Attention进行序列建模，还比如：UBERT[19]，借鉴BERT的思想提升用户表征的表达能力。</li></ul><p>除了上述研究之外，还有不少最新<strong>前沿的推荐系统研究</strong>实际上也是深受NLP领域的影响，有一些还有着推荐系统领域”<strong>独特的味道</strong>“。</p><ul><li><strong>表征学习</strong>：最早的表征学习源自于NLP中做词向量。目前也广泛地在推荐系统研究领域开展着，且有着更明显的特点。推荐系统的稀疏性问题比NLP领域更严重：NLP领域的词表大小有限，但是推荐系统领域则面临着海量的用户和物品。因此，如何得到一个更合理的表征向量，对推荐系统来说非常重要。这方面工作例如KDD21上Google[20]和华为[21]的工作。</li><li><strong>图神经网络</strong>：图神经网络可能是极少数NLP和推荐系统齐头并进的领域，或者说在NLP和推荐系统上的发展有各自的特色。在NLP中，通常更注重图的构造，如何基于word、token、entity、句法树、依存树甚至引入知识图谱等，实际上更重要，这部分工作可以参考KDD21的tutorials：Deep Learning on Graphs for Natural Language Processing[23]，介绍的很仔细。在推荐系统领域，图的构造也很重要，但通常最优雅最自然的graph就是推荐系统天然的user-item二分图，因此推荐系统在GNN方面的研究则更注重模型结构上的改造、如何解决过平滑问题、可解释性、异构图建模等问题，包括GC-MC、LightGCN、SR-GNN等。</li><li><strong>知识图谱</strong>：知识图谱源于NLP的概念，在推荐系统中也早有应用。这部分研究最经典的如TransE[32]。近年来基于知识图谱的推荐主要和图网络结合在一起做，例如KGAT[33]等，这部分工作可以Follow斯坦福王鸿伟大佬的工作[34]。</li><li><strong>对比学习</strong>：对比学习可能最早要追溯到cv领域。后来在NLP中也广泛应用，例如SimCSE[24]。再后来又影响了推荐系统，目前基于对比学习的自监督推荐系统工作也非常多[22]，今年还涌现了不少引入对比学习做纠偏的工作[25]或抗噪的工作[28]。</li><li><strong>知识蒸馏</strong>：这方面工作实际上最早也是源于cv，出自Hinton大佬之手[26]。然后近年来随着预训练/BERT的兴起，对BERT如何做蒸馏成为了热点。而推荐系统领域应用知识蒸馏，最主流的方式是<strong>大模型蒸馏小模型</strong>或者<strong>后置跨链路蒸馏前置链路</strong>[27]。归根结底都是性能和准确性的权衡。</li><li><strong>预训练/迁移学习</strong>：也是得益于BERT的发展，目前推荐系统领域应用预训练/迁移学习的场景也变的非常多。例如：刘知远老师组的综述：<strong>基于预训练的推荐系统知识迁移综述</strong>[29]，能够有效解决推荐系统中的数据稀疏问题。</li></ul><p><strong>Transfromers4Rec</strong>工作实际上也是受到Transfomers、BERT以及NLP开源社区的影响所产生的。不同领域之间互相影响当然也是件好事，例如NLP领域也经常借鉴CV领域的成果，例如残差网络、CNN等等。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>从上面的介绍，我们可以看到NLP研究进展如何深远的影响着推荐系统的研究，可以说推荐系统的发展很大程度上是在NLP的肩膀上前进的。</p><p>到目前为止，我们看到的大多是单向影响，即：<strong>起步较早</strong>的领域深度影响着<strong>新兴领域</strong>，那么是否存在反哺的现象呢？也算是本篇文章的一个遗留问题，例如：<strong>推荐系统社区是否有反哺NLP社区呢？</strong> 这个问题可能也等价于推荐系统是否有一些独创/开创性的研究，是已经或有可能潜在影响其他领域的研究？(可能是个知乎好问题)。</p><p>这个问题也很大，需要花一些时间去学习和积累才能感受到。以读者目前的浅薄认知而言，我觉得有，至少我认为推荐系统在工程架构方面的进展是前沿的，是领先其他领域的。比如：实时推理引擎如TF Serving那一套，最早是服务于大规模实时推荐系统的，但对NLP尤其是预训练模型做实时serving还是有帮助的，小规模场景开箱即用，大规模场景做TF Serving优化后，也能使用。还有比如推荐系统中算首创的工作协同过滤或特征交互的研究进展，经典的FM等，也能一定程度上启发NLP研究。还有比如搜索/推荐场景常用的多阶段pipeline，召回+粗排+精排，也经常用在对话系统中。更多的以后有机会可以一起探讨探讨。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a><strong>参考</strong></h2><p>[1] de Souza Pereira Moreira G, Rabhi S, Lee J M, et al. <strong>Transformers4Rec: Bridging the Gap between NLP and Sequential/Session-Based Recommendation</strong>[C]//Fifteenth ACM Conference on Recommender Systems. 2021: 143-153.</p><p>[2] Lin T, Wang Y, Liu X, et al. <strong>A Survey of Transformers</strong>[J]. arXiv preprint arXiv:2106.04554, 2021.</p><p>[3] Vaswani A, Shazeer N, Parmar N, et al. <strong>Attention is all you need</strong>[C]//Advances in neural information processing systems. 2017: 5998-6008.</p><p>[4] <strong>State-of-the-art Natural Language Processing</strong> for Jax, PyTorch and TensorFlow: <a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p><p>[5] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. <strong>Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations</strong>. ACM Transactions on Information Systems (TOIS) 39, 1 (2020), 1–42.</p><p>[6] <strong>研究推荐系统要对NLP很了解吗？付鹏的回答：</strong><a href="https://www.zhihu.com/question/317441966/answer/633112869">https://www.zhihu.com/question/317441966/answer/633112869</a></p><p>[7] <strong>自然语言处理 NLP 的百年发展史</strong>, <a href="https://link.zhihu.com/?target=http://imgtec.eetrend.com/blog/2020/100052025.html">http://imgtec.eetrend.com/blog/2020/100052025.html</a></p><p>[8] <strong>一文尽览推荐系统模型演变史</strong>, <a href="https://link.zhihu.com/?target=https://cloud.tencent.com/developer/article/1652169">https://cloud.tencent.com/developer/article/1652169</a></p><p>[9] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. <strong>Session-based recommendations with recurrent neural networks</strong>. arXiv preprint arXiv:1511.06939 (2015).</p><p>[10] Bahdanau D, Cho K, Bengio Y. <strong>Neural machine translation by jointly learning to align and translate</strong>[J]. arXiv preprint arXiv:1409.0473, 2014.</p><p>[11] Zhou G, Zhu X, Song C, et al. <strong>Deep interest network for click-through rate prediction</strong>[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2018: 1059-1068.</p><p>[12] Wu S, Tang Y, Zhu Y, et al. <strong>Session-based recommendation with graph neural networks</strong>[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 346-353.</p><p>[13] <strong>Transformer 在美团搜索排序中的实践</strong>: <a href="https://link.zhihu.com/?target=https://tech.meituan.com/2020/04/16/transformer-in-meituan.html">https://tech.meituan.com/2020/04/16/transformer-in-meituan.html</a></p><p>[14] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati, Jaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. <strong>E-commerce in your inbox: Product recommendations at scale</strong>. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 1809–1818.</p><p>[15] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, and Jun Ma. 2017. <strong>Neural Attentive Session-based Recommendation</strong>. arXiv:1711.04725 [cs.IR]</p><p>[16] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. <strong>Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks</strong>. arXiv:1708.04617 [cs.LG]</p><p>[17] Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wilfred Ng. 2019. <strong>SDM: Sequential deep matching model for online large-scale recommender system</strong>. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2635–2643.</p><p>[18] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. <strong>BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer</strong>. In Proceedings of the 28th ACM international conference on information and knowledge management. 1441–1450.</p><p>[19] Qiu Z, Wu X, Gao J, et al. <strong>U-BERT: Pre-training User Representations for Improved Recommendation</strong>[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(5): 4320-4327.</p><p>[20] Kang W C, Cheng D Z, Yao T, et al. <strong>Learning to Embed Categorical Features without Embedding Tables for Recommendation</strong>[C]//Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. 2021: 840-850.</p><p>[21] Guo H, Chen B, Tang R, et al. <strong>An Embedding Learning Framework for Numerical Features in CTR Prediction</strong>[J]. arXiv preprint arXiv:2012.08986, 2020.</p><p>[22] Wu J, Wang X, Feng F, et al. <strong>Self-supervised graph learning for recommendation</strong>[C]//Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021: 726-735.</p><p>[23] KDD2021, <strong>Deep Learning on Graphs for Natural Language Processing</strong>: <a href="https://link.zhihu.com/?target=https://dlg4nlp.github.io/">https://dlg4nlp.github.io/</a></p><p>[24] Gao T, Yao X, Chen D. <strong>SimCSE: Simple Contrastive Learning of Sentence Embeddings</strong>[J]. arXiv preprint arXiv:2104.08821, 2021.</p><p>[25] Zhou C, Ma J, Zhang J, et al. <strong>Contrastive learning for debiased candidate generation in large-scale recommender systems</strong>[C]//Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. 2021: 3985-3995.</p><p>[26] Hinton G, Vinyals O, Dean J. <strong>Distilling the knowledge in a neural network</strong>[J]. arXiv preprint arXiv:1503.02531, 2015.</p><p>[27] <strong>张俊林，知识蒸馏在推荐系统的应用</strong>：<a href="https://zhuanlan.zhihu.com/p/143155437">https://zhuanlan.zhihu.com/p/143155437</a></p><p>[28] <strong>张俊林，利用Contrastive Learning对抗数据噪声：对比学习在微博场景的实践</strong>，<a href="https://zhuanlan.zhihu.com/p/370782081">https://zhuanlan.zhihu.com/p/370782081</a></p><p>[29] Zeng Z, Xiao C, Yao Y, et al. <strong>Knowledge transfer via pre-training for recommendation: A review and prospect</strong>[J]. Frontiers in big Data, 2021, 4</p><p>[30] Wang C, Blei D M. <strong>Collaborative topic modeling for recommending scientific articles</strong>[C]//Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011: 448-456.</p><p>[31] Blei D M, Ng A Y, Jordan M I. <strong>Latent dirichlet allocation</strong>[J]. the Journal of machine Learning research, 2003, 3: 993-1022.</p><p>[32] Bordes A, Usunier N, Garcia-Duran A, et al. <strong>Translating embeddings for modeling multi-relational dat</strong>a[J]. Advances in neural information processing systems, 2013, 26.</p><p>[33] Wang X, He X, Cao Y, et al. Kgat: <strong>Knowledge graph attention network for recommendation</strong>[C]//Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2019: 950-958.</p><p>[34] Hongwei Wang, <a href="https://link.zhihu.com/?target=https://scholar.google.com/citations?user=3C__4wsAAAAJ&hl=zh-CN">https://scholar.google.com/citations?user=3C__4wsAAAAJ&amp;hl=zh-CN</a></p><p>原文也可参见我的知乎专栏：<a href="https://www.zhihu.com/column/mgxs-note">蘑菇先生学习记</a>。</p><hr><blockquote><p>作者：付鹏<br>链接：<a href="https://www.zhihu.com/question/317441966/answer/633112869">https://www.zhihu.com/question/317441966/answer/633112869</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>照理来说是不需要的。但是对NLP有所涉猎对推荐系统大有裨益。</p><p><strong>一方面，许多推荐系统产品形态离不开NLP。</strong></p><ul><li>视频推荐有标题、作者、评论</li><li>电商推荐有标题、介绍、评论</li><li>新闻推荐有文章、评论</li></ul><p>目前的互联网服务形态，不管是UGC还是PGC还是什么样的产品，用户的参与都是极为重要的，只要有用户参与，就会有文本贡献，而这些强用户行为，是对推荐非常重要的。</p><p><strong>另一方面，推荐系统的问题抽象之后和NLP问题非常相似。</strong></p><p>这一点很重要，但是很多人都忽略掉了。</p><p>推荐系统的基本问题可以抽象成求解 <img src="https://www.zhihu.com/equation?tex=p(item_i%7Cuser,+history%5C_item_1,+history%5C_item_2,+...+history%5C_item_k)" alt="[公式]"> ，即求解指定 item在指定user的历史行为记录下可能产生行为的联合条件概率，其中 <img src="https://www.zhihu.com/equation?tex=item,+history%5C_item" alt="[公式]"> 的形式都是item id。</p><p>而NLP的language model中，第k个word的概率也正是类似的形式： <img src="https://www.zhihu.com/equation?tex=p(w_i%7Cw_%7Bi-1%7D,+w_%7Bi-2%7D,+...+w_1)" alt="[公式]"> 。</p><p>当我们省略user的信息，只使用历史记录的时候，推荐系统和NLP的基本问题变得惊人相似。</p><p>这当然不是巧合，有这个结果的原因就是，<strong>从某种角度，推荐系统和NLP问题都可以看做是在解决序列上的问题。</strong></p><p>所以，你可以看到许许多多的NLP技术都能被应用于推荐系统，并且大放异彩：</p><ul><li>idf是很好的NLP的权重方法，在推荐系统同样适用</li><li>SVD最早可以被拿来抽取NLP的词向量，后来在Netflix比赛中SVD和进阶版SVD++夺得了冠军</li><li>word2vec的模型，直接把word替换成用户的history，就能取得很好的效果，形成了推荐系统的item2vec算法</li><li>后期神经网络RNN/Attention能够被用来处理NLP的一系列问题，再过一两年，推荐系统上也开始大规模使用RNN/Attention</li><li>甚至于CNN被从CV领域借鉴过来，发现可以抽取文本特征之后，推荐系统也能够用CNN来抽取序列特征</li><li>…</li></ul><p>尤其以attention mechanism 和 item2vec 最为典型，如果没有NLP上的这些发现，很难说推荐系统会不会发展出同样的技术。</p><p>而推荐系统和其他的领域是没有这样高的契合度的。可以说NLP就是推荐系统的一个宝库也不为过，所以我推荐所有学习和从事推荐系统的同学都去学学NLP。</p><p><strong>如果你在做一个事情，身边有另一个事情有大把的经验可以直接借鉴，那为什么不去学呢？</strong></p><hr><blockquote><p>作者：大葱<br>链接：<a href="https://www.zhihu.com/question/317441966/answer/632942028">https://www.zhihu.com/question/317441966/answer/632942028</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>我个人觉得做推荐但对其他相关领域例如cv和nlp等有一定深度的涉猎是有很大帮助的。主要有几点吧，个人意见，仅供参考：</p><ol><li><p>众所周知，推荐领域经过这么多年的发展，已经不是仅仅依赖用户购买商品的交互数据来进行推荐了(一般我们认为这种方法是协同过滤)。往往在交互数据的基础上加入很多新信息(side information)是对推荐效果有一定提升的。而目前这类的side information主要形式也不外乎图片，文字这两种主要形式。所以怎么从这些新数据源提取有价值的信息并融入到推荐模型中，是需要cv和nlp的知识；</p></li><li><p>现在推荐无论工业界还是学术界基本都已经进入深度学习时代。虽然这么说显得技术含量不高，但是我个人愚见基于应用的深度学习算法里，模型架构很重要。由于一些场景的类似，在cv和nlp中很多已经成熟的架构用在推荐中也是有很直接的效果的。比如说在nlp中很火的skip-gram模型，在推荐中就被很多团队应用起来甚至做了基于应用场景的微调后取得了很不错的效果。还有cv界的attention机制，现在在推荐里也是司空见惯了；</p></li><li><p>最后想说回相似性，比如nlp中，词这个概念就和推荐中商品这个概念有很多接近的属性。比如说都有热门/冷门的区别，对一个session中用户行为的建模也经常参考nlp中对句子建模的方式。在模型训练中不是每个batch都能训练到所有的word/item。nlp领域怎么解决这些问题，都值得借鉴，反之亦然；</p></li></ol><p>等等这些理由，我觉得推荐现在要做好，确实是个很综合性的考量，不是说要你样样都精，但能做到对不同领域一专多能总是好的。另外，我觉得推荐还是一个对数据非常非常敏感的任务，重中之重应该是对数据的分析，清洗和研究。</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机专业想好好搞科研 读博士 建议平时刷leetcode吗 会不会浪费时间？</title>
      <link href="/2021/11/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E6%83%B3%E5%A5%BD%E5%A5%BD%E6%90%9E%E7%A7%91%E7%A0%94-%E8%AF%BB%E5%8D%9A%E5%A3%AB-%E5%BB%BA%E8%AE%AE%E5%B9%B3%E6%97%B6%E5%88%B7leetcode%E5%90%97-%E4%BC%9A%E4%B8%8D%E4%BC%9A%E6%B5%AA%E8%B4%B9%E6%97%B6%E9%97%B4%EF%BC%9F/"/>
      <url>/2021/11/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E6%83%B3%E5%A5%BD%E5%A5%BD%E6%90%9E%E7%A7%91%E7%A0%94-%E8%AF%BB%E5%8D%9A%E5%A3%AB-%E5%BB%BA%E8%AE%AE%E5%B9%B3%E6%97%B6%E5%88%B7leetcode%E5%90%97-%E4%BC%9A%E4%B8%8D%E4%BC%9A%E6%B5%AA%E8%B4%B9%E6%97%B6%E9%97%B4%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>不会。相反还是一种有效的对冲方式。</p><p>一个是对冲掉未来找不到工作的风险。题刷的好，至少大厂入场券是有了，而且面试官会觉得你聪明，会提起兴趣看你的研究成果。而且某些厂也不太看你做的啥，上来就是medium/hard，实现快速刷人。刷leetcode还是不要漫无目的，掌握核心知识点套路，核心数据结构算法，会更有条理。刷题嘛，为了挣钱不寒碜。</p><blockquote><p>作者：Yulong<br>链接：<a href="https://www.zhihu.com/question/501818272/answer/2244654809">https://www.zhihu.com/question/501818272/answer/2244654809</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><hr><blockquote><p>作者：杰尼小子<br>链接：<a href="https://www.zhihu.com/question/501818272/answer/2245141495">https://www.zhihu.com/question/501818272/answer/2245141495</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>先说结论，我认为现在是博士的话，就不要带着功利心去刷题了，<strong>这里的功利心指的是去背一些面试常考题</strong>。 </p><p>其实无论是工作还是做科研，最重要的技能之一还是代码能力， 可能只说代码能力觉得比较宽泛，我认为可以理解为：<strong>有了一个初步的idea之后， 如何从0到1完整的实现，同时这过程中会有很多跟预期不一致的地方，如何去解决这些问题，给出一个优美，简洁，复用性强的代码。</strong>只不过现在做科研的成本太低了，有很多开源代码，pytorch与TensorFlow封装的也都很好，把一些顶会的论文进行A+B+C的拼凑在一些情况就可以刷出SOTA(甚至都不用自己写几行代码)，就可以斐波那契投递会议了。</p><p>做算法题本身就是一个锻炼自己代码能力以及逻辑思维的很好方式，我想很多刚开始做题的同学都有这种情况： <strong>这个题我知道思路，但是真的实现起来就会遇到很多问题，而且更多情况是，完全不知道怎么实现。</strong> 作为一个计算机学生，这个可以说是看家本领，就算是做科研，如果你真的有绝好的idea，良好的代码能力也会让你的”SOTA“速度更快一点。</p><p>其实从目前业界的面试方式就可以看出来，外企(gg,ms, hulu等)每一轮面试都会有大量的算法题，因为这是可以快速看出你思维能力，逻辑缜密度的一个方式。所以外企面试会有几轮专门的算法面试，但是跟国内不一样的是，他们通常真的是考你的思维逻辑，如果你没有做过这一道题，他们反而会更高兴，跟你进行讨论的形式进行面试，比如30-40 min可能只做2道题，大部分时间都在讨论如何去思考的，如果条件变了会怎样，也就是算法题真正的初衷：<strong>看你是不是一个”聪明人“</strong>。</p><p>不得不吐槽一下，国内很多公司的面试官则截然相反，很多面试官只关注面试者能否快速给出一个正确的代码，丝毫不关注面试者的思考方式，思考动机，以及是否”背题目“。 因此国内很多面试你可以发现几乎就是： <strong>面试官给一道题目，应试者敲出代码大体一说在干嘛，面试官运行一下正确即可。而且有相当数量的面试官自己也不是很了解这道题，因此当代码有问题的时候，面评可能就只写了一句”代码能力一般“。</strong>可能他自己都不知道假设应试者没有见过这一题，大部分人无法在10min给出一个完全正确的结果。同时应试者如果没有”短时间“会这一题，可能面试官就直接换题目了，而不是去了解目前他思考到什么程度。</p><p>题主目前是博士，当然第一要义就是丰富自己的pub，但是要明白算法，数据结构等是一个计算机工作者的基础之一，<strong>建议题主把做题当做一个锻炼自己思维能力以及代码能力的方式，而不是自己之后找工作的一种手段</strong>(当然，本科生硕士生因为大环境的问题，还是要多刷题的)。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱+nlp，有什么适合硕士独自研究的方向？</title>
      <link href="/2021/11/28/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-nlp%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E9%80%82%E5%90%88%E7%A1%95%E5%A3%AB%E7%8B%AC%E8%87%AA%E7%A0%94%E7%A9%B6%E7%9A%84%E6%96%B9%E5%90%91%EF%BC%9F/"/>
      <url>/2021/11/28/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-nlp%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E9%80%82%E5%90%88%E7%A1%95%E5%A3%AB%E7%8B%AC%E8%87%AA%E7%A0%94%E7%A9%B6%E7%9A%84%E6%96%B9%E5%90%91%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p><strong>问：</strong>知识图谱+nlp，有什么适合硕士独自研究的方向？</p><p>目前有看关于knowledge graph  embedding，completion还有在QA的应用以及最近一些在nlp模型引入外部知识的论文，希望能确定一个小方向，适合一个人去做的，目前自己还在入门学习阶段。。。因为实验室没什么人做，想选一个没有那么难的方向呀</p><p><strong>答：</strong>做知识图谱的独立研究之前，还是建议将整个知识图谱的流程走一遍。</p><p>因为知识图谱的构建流程是一个很脏的活，尤其是你面临的数据是非结构化的文本文档、扫描版的PDF、手写文档、半结构化数据库等。这里面又牵扯到很多的NLP的方法如NER、关系抽取、事件抽取等等。</p><p>所以，从0到1构建一个简单的知识图谱，是将自己NLP和KG的知识融会贯通的最好的方法。</p><p>这时候，大概率的，你会从中找到一个感兴趣的点，同时通过读论文也会发现一些别人还没做好的点，比如说如何通过语义解析做自动化查询表达式生成？如何做多模态KGQA（比如说嵌入视频和声音特征）？当然，也可以做一些比较热门的小领域比如KGE。</p><p>找好一个方向，just do it</p><blockquote><p>作者：邵浩博士<br>链接：<a href="https://www.zhihu.com/question/351260481/answer/922688741">https://www.zhihu.com/question/351260481/answer/922688741</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><hr><blockquote><p>作者：徐波<br>链接：<a href="https://www.zhihu.com/question/351260481/answer/1070879155">https://www.zhihu.com/question/351260481/answer/1070879155</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>IEEE Fellow Philip S. Yu等人2020年新出了一篇知识图谱综述【1】，刚好可以给大家做一个参考。</p><p>一图以蔽之，每个小点（灰框中的每一行）都可以作为一个研究方向。</p><p><img src="https://pica.zhimg.com/50/v2-91ac6c3a1f4a9277781fd88833813dc4_720w.jpg?source=1940ef5c" alt="img"></p><p>【1】Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip S. Yu. A Survey on Knowledge Graphs: Representation, Acquisition and Applications. </p><hr><blockquote><p>作者：SnailDM<br>链接：<a href="https://www.zhihu.com/question/351260481/answer/946121957">https://www.zhihu.com/question/351260481/answer/946121957</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>知识图谱的知识抽取如果说命名实体识别的话现在应该比较成熟，比如人名，地名，机构名等，但是有些知识标签并不就是某个词，它可能是通过某句话来反映，比如“对财务数据进行统计分析”抽取为财务报表分析，也可能需要结合篇章的上下文来反映，比如简历中的“银行相关工作经验。balabala（此处省略1000字主要讲述具体的流程，没有提到银行），进行相关风险管理”，能通过远处的“银行”识别到“银行风险管理”吗？目前来说很难。这种细层级的知识抽取应该是有研究价值的。</p><p>​       再说关系抽取，现在学术界的关系抽取用了远程监督等算法确实提升了一个高度，但是公开的标准数据集的结果还难以达到工业界一些需要做精准关系抽取的场合，这些场合目前还往往用专家规则来抽取，所以网上很多牛人在介绍自家知识图谱的关系抽取那一块很多都免不了加上规则那一块。</p><p>​       现在也有很多在研究知识图谱的多模态表示了，比如将知识图谱和图像，语音等相结合来实现认知智能。还有用知识图谱来可解释人工智能，很容易想到的就是用知识图谱来推理因果关系，这些都是比较有价值的方向。</p><p>​       从应用场景上来说，目前知识图谱主要应用在互联网金融，互联网医疗等领域。但是其实很多机械电力工业场景也需要知识图谱，举个例子，大型工业设备的检测就需要用到大量专家知识来推理，而这些专家知识的推理可以通过构建知识图谱来解决。你构建一个汽车故障的知识图谱，从知识图谱中你就可以找到汽车某一故障发生的原因，而这也能做到工业设备的智能问答场景中。所以工业智能场景的知识图谱应用也是很有价值的。</p><p>​      还有知识表示方法，是否可以对头实体、尾实体加上不同的权重来表示，这些都是值得研究的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>是什么剥夺了我们的专注力？</title>
      <link href="/2021/11/27/%E6%98%AF%E4%BB%80%E4%B9%88%E5%89%A5%E5%A4%BA%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%B8%93%E6%B3%A8%E5%8A%9B%EF%BC%9F/"/>
      <url>/2021/11/27/%E6%98%AF%E4%BB%80%E4%B9%88%E5%89%A5%E5%A4%BA%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%B8%93%E6%B3%A8%E5%8A%9B%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h2 id="一、是什么剥夺了专注力？"><a href="#一、是什么剥夺了专注力？" class="headerlink" title="一、是什么剥夺了专注力？"></a>一、是什么剥夺了专注力？</h2><p>​    1.注意力碎片化：获取碎片化信息不用动脑，大数据精准推送，注意力被切割得稀碎。<br>​    2.注意力慢不下来：对于获取信息失去耐心，爱走捷径，沉迷于倍速，或者X分钟看完XXX系列。<br>​    3.思考力停留在表面：X分钟看完XXX系列，表面上让人一天博览群书，但不是自己整理内化的过程，没有经过自己的深度思考，因此看完也很难记住。</p><h2 id="二、低幼信息带来什么结果？"><a href="#二、低幼信息带来什么结果？" class="headerlink" title="二、低幼信息带来什么结果？"></a>二、低幼信息带来什么结果？</h2><p>​    1.思考空间被挤压：人脑有两种机制，【快思维】反应敏捷、情绪化，掌管情感和经验；【慢思维】睿智但反应慢，掌管理性认知。获取即时快乐时，快思维掌管大脑，慢思维的空间便被挤压，大脑只会接收信息而不会理性思考了。<br>​    2.认知资源被消耗：大脑某区——【默认模式网络】在人专注时休眠，在注意力分散时活跃，其功能是胡思乱想。沉迷刷手机时，会高速地循环“激活警觉网络→启动定向网络→启动执行网络”步骤，不停地消耗认知资源。<br>​    3.对即时快乐成瘾：越刷越控制不住自己，越刷越想刷，快感阈值提高，越陷越深。<br>​    4.专注力不断降低：每天接受的信息灌输远超大脑的可承受范围，超出部分称为“噪音”，噪音过多使注意力分散。</p><h2 id="三、怎样才能提高专注力？"><a href="#三、怎样才能提高专注力？" class="headerlink" title="三、怎样才能提高专注力？"></a>三、怎样才能提高专注力？</h2><p>​    1.杜绝一切干扰：关闭一切消息提醒，借用工具检测自己使用手机的时长和频率，循序渐进，设定任务。<br>​    2.寻找内啡肽快乐：【多巴胺快乐】即即时快乐，易沉溺和乏味；【内啡肽快乐】需要付出，前期疲惫，过后积极充实，如慢跑、学习等。<br>​    3.训练超强专注力：《如何像爱因斯坦一样专注》中的训练方法——共12步（好多），自己看7：30</p><h2 id="四、培养深度思考力。"><a href="#四、培养深度思考力。" class="headerlink" title="四、培养深度思考力。"></a>四、培养深度思考力。</h2><p>​    1.触发慢思维的前提是保持专注力，深度思考力建立在认知水平上。要从看到、听到转变为真正学到，需要同化有价值的信息，再形成知识体系，如输入再输出。<br>​    2.坚持用慢思维武装自己，还可带来内啡肽快乐。</p><p>最后，毁掉我们的不是我们憎恨的东西，往往是我们热爱的东西。互联网服务于我们，我们也毫无防备地成为其中的流量，被抢夺注意力，降低自控力，挤压思考空间。但造成不好影响的并非电子产品本身，而是人们的使用方法和习惯。所以我们一定要养成良好健康的使用习惯。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自律 </tag>
            
            <tag> B站 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>准研，nlp 有哪些坑?</title>
      <link href="/2021/11/27/%E5%87%86%E7%A0%94%EF%BC%8Cnlp-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91/"/>
      <url>/2021/11/27/%E5%87%86%E7%A0%94%EF%BC%8Cnlp-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：机智的叉烧<br>链接：<a href="https://www.zhihu.com/question/320804186/answer/667462035">https://www.zhihu.com/question/320804186/answer/667462035</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>背景：本科信息与计算科学。研二统计学硕士，方向：NLP+时间序列</p><p>我最近正好回顾了我学习的全过程，链接在这里：</p><p><a href="https://zhuanlan.zhihu.com/p/64026730">机智的叉烧：我的NLP学习之路648 赞同 · 96 评论文章</a></p><p>不算是根正苗红的NLP出身，但是也做过NLP的研究工作，说说我的想法吧。</p><p>首先是，既然准研，有点时间拿来打打基础，准备点预备知识非常合适，而且能快人一步，何乐不为。</p><h2 id="确认你是做应用问题还是做理论问题"><a href="#确认你是做应用问题还是做理论问题" class="headerlink" title="确认你是做应用问题还是做理论问题"></a>确认你是做应用问题还是做理论问题</h2><p>如果是应用问题，关注的是问题本身，例如我做时间序列预测问题，那我的核心目标应该在于时间序列上，NLP只是我在解决时间序列预测上的一个工具，我更应该关注我预测的准不准，而不是NLP方面的性能（也要关注，但是你有你的核心目标）。另外，你应该知道，有关时间序列预测的主要方法有哪些，NLP又是怎么用到里面的，你才会找得到创新点，换句话，你要同时知道你的应用领域和NLP本身的研究现状，毕竟NLP本身可能已经发展到一定程度了，但是很多领域可能只停留在TF-IDF层面的研究。</p><p>如果是理论，那就请你把你这个方向的前沿、标杆、经典、高引论文都好好看一遍，甚至是自己重现一遍，要做理论和模型的创新，必须对原有方法有深入的了解，如何了解？来源于你对论文之类的把握。</p><h2 id="数学、编程、理论基础"><a href="#数学、编程、理论基础" class="headerlink" title="数学、编程、理论基础"></a>数学、编程、理论基础</h2><p>你做的是科研，要的是前沿，否则怎么来创新点。所以，你的基础必须非常扎实，否则非常难在这块有所提升，数学系的我出来看到很多人的数学基础不好，导数求不对，矩阵不会推，非常痛苦，准研的你要是有时间，一定要好好补一补。</p><p>这里没有提开发，是因为从科研的角度出发，要是你要做技术开发，那一些大数据工具也要懂，但是做科研，大部分用不上，实验室甚至没这个条件对吧。科研，建议python一定会，另外一些深度学习框架，现在看来tensorflow和pytorch会一个吧，我会的是前面那个，至于keras，我的建议是慎用，里面大量细节你无法得知，例如矩阵的对齐等等，因为他封装的太好了，其实不太适用于对模型的理解。</p><p>尽管BERT、elmo等新模型层出不穷，但是不代表word2vector、fasttext、GloVe就一点用都没有，这是前辈留下十分经典的东西，甚至能有不错的基线，打比赛、科研啥的都很有用，为什么就放掉呢。</p><p>说点扎心的，基础都打不好，怎么创新。</p><h2 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a>读论文</h2><p>这里也有其他大佬提到过的，读论文是有技巧的。</p><p>首先是区分文章精读还是粗读，当你心里有目标的时候，看完摘要其实就知道这篇文章是否符合自己的心意，有没有用。对于和你研究接近的，多读读，深入读甚至是我前面说到的重现；如果是关系不大的，就下一篇即可。</p><p>第二是做笔记。不要学小熊掰棒子，只要你看过，都建议你记下来，加上自己的想法，逐渐建立自己的论文库，到时候写引言问下综述非常有用。</p><p>第三，注意一下和你很有关的论文都是发表在那里的，什么期刊、会议之类的，到时候发论文选期刊你就知道了。</p><p>第四，在了解研究现状，或者不知道具体现状如何时，看综述文章，一般都会带有“review”、“survey”之类的词，看完之后，拿他的参考文献也来看看，收获无敌大。</p><p>Last but not least，坚持读，多读。很多人老说自己没想法，我个人深有体会，<strong>所有的灵感不是拍脑袋想出来的</strong>，而是看论文积累而来的，所有的思想都来源于生活，所有的输出都来源于你的输入，我现在快毕业了，后面也快工作了，都坚持每天2个小时自学，看论文也好，学东西也罢。</p><h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><p>个人感觉，不是说书都不好，而是说现在没有特别适合初学者一撸到底的NLP书籍，所以入门，最好不要看一本书一撸到底（具体怎么入门我后面说）。</p><p>宗成庆老师的《统计自然语言处理》非常全面，我也经常放在我的旁边，但是只适合做工具书，或者是一本闲书，初学者看起来收获不大，但是对老司机来说还是有点味道的。</p><p>最近出的《基于深度学习的自然语言处理》，好书，绝对是好书，但是缺代码和实现，所以新手看起来可能有些吃力，强行看非常不错，也是一本完整的文献综述。</p><p>另外有一些带有“python”+“自然语言处理”or“文本分析”之类的文章，多半是和你讲NLTK之类的使用方法，能教你做一些基础的工作，例如词性标注之类的，也是当做工具书吧，但是里面的技能还是得会。</p><h2 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h2><p>要入门，我觉得还是上课靠谱，淘宝买也好，一些课程网站学也好，简单过一遍，不求学懂，只求知道整个框架如何吧，仔细想想，如果靠几百块上个几天的课就会了，为啥那么多人觉得找工作、科研困难？我是这么学来的，效果不错，所以建议大家也可以这么做，避免打广告的嫌疑此处不列太多啦。</p><p>先说这么多，继续赶毕业论文去了T T</p><h2 id="我的NLP学习之路"><a href="#我的NLP学习之路" class="headerlink" title="我的NLP学习之路"></a>我的NLP学习之路</h2><p>应好友邀约，写一篇有关我在NLP上学习的历程，以供大家作参考。点开的朋友，如果真的想了解这个过程，也想知道一些具体的信息，并从中有所收获，真心希望大家能认真看下去，尤其是一些近期和我在聊的新人，我虽不算大神，但是自认为还是有一定经验的，如果信得过我，请认真看吧。如果有什么想法欢迎和我多聊~</p><p>懒人目录</p><ul><li>学习历程</li><li>学习路径</li><li>学习建议</li><li>其他建议</li></ul><h3 id="学习历程"><a href="#学习历程" class="headerlink" title="学习历程"></a>学习历程</h3><h4 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h4><p>为了更好地让大家去理解我的历程，我先说说我当时的背景。</p><p>时间起点就从大四开始吧，信息与计算科学（提醒一下，这是个数学类专业，不是计算机的）专业，数学理论上还是比较自信的，计算机会点皮毛吧，matlab会点，写过点Java和前端三剑客，python也是大四中后期才自学的，蹩脚得很。</p><p>就在这种情况下，开始机器学习和NLP方面的学习。</p><h4 id="启蒙阶段"><a href="#启蒙阶段" class="headerlink" title="启蒙阶段"></a>启蒙阶段</h4><p>大四期间（16-17学年）因为机缘巧合接触了<strong>李航老师的《统计学习方法》</strong>，并在团队合作的情况下通关了里面的内容，回想起来还是满满的求生欲。由于只是过了书上的内容，所以只是对理论有了比较完整的了解，但是具体怎么用，怎么实现，基本是不知道的。</p><p>这段时间，基本就是拼的自己的脑子在攻克这本书，实际上，对于数学专业来说，这本书相比微分方程、实分析之类的还是简单很多，所以数学出身的同学还是不用太担心，但是<strong>对于高数、线代、概率论不好的同学，吃力是毋庸置疑的</strong>。</p><p>当然的，书终究只是书，有时候书真的看不懂，我会结合着网上的别的材料材料一起看，博客之类的，可以帮助理解，当然有些会有些错误，包括书本身和博客，需要自己甄别，好在不太多，不至于有三观级别的查错。</p><p>很多人因为这本书的困难而放弃，而且很多人会非常追捧周志华老师的《机器学习》，诚然这是本好书，但是如果你因为《统计学习方法》看不懂而去看周志华老师的《机器学习》，那你后面的提升会被严重限制，例如你只知道HMM的三个问题却不知道这三个问题怎么解决。（因为难而躲避，你终究会为你的躲避付出代价，要想多赚点真不是说有就有的）。</p><p>有关方法实现，我的学习基本来自于百度，例如“python SVM”，这样就会出大量的文章来讲，选一个试试你就会了，这已经是非常简单的方法了。</p><p>懒人记录：</p><ul><li>《统计学习方法》不仅在于里面的方法，还有里面的计算思想，正则化，极大似然估计，梯度下降等，所以理论搞懂，会为你以后的进步打下基础，欠的账迟早要还。</li><li>注意除了学方法，还要学实现，我当时欠了这块的账，在后续才补了回来，<strong>自己找个数据集玩玩（鸢尾花）</strong>之类的，做一遍就好</li><li>Python是这块的基操，没有不会的理由。</li></ul><h4 id="入门阶段"><a href="#入门阶段" class="headerlink" title="入门阶段"></a>入门阶段</h4><p>机器学习想必是有些基础了，但是如何用，在哪用，其实非常苦恼，因此自己结合自己的研究生方向与当前社会现状，和老师沟通了一条线路，NLP，主要课题是研究如何通过NLP与网络信息实现对金融市场的预测，这也就奠定了整个研究生阶段的主要内容了。可惜的是，老师其实对NLP的了解不深，只能从金融市场有关内容中给我建议，但是NLP既然要做，也注定是一条孤独艰难的路。</p><p>17年9月入学，至12月，我完成了第一篇有关研究内容的初稿，从什么都不知道，到有论文吧，这段时间除了偶尔看看LPL的比赛，基本就在学校理学楼221里面（学校一间几乎被废弃，桌子除了我的位置基本都是尘的教室里）自习，严格而言都不算实验室，只是一个可供自习落脚的地方吧，这么一个环境学习下来的。</p><p>当时完全不清楚NLP是什么，也不知道这东西怎么做，只能通过看论文、看博客慢慢去学，当时甚至连淘宝和网课都没有，前期每天只有两个工作，<strong>刷论文和写爬虫</strong>。爬虫那时候技术不是很强，用的自己比较熟悉的Ajax去网络抓包，<strong>论文方面是每看一篇就整理成笔记，笔记多少和文章摘要作用之类的有关，觉得有用了才回去深看</strong>，后来<strong>一篇综述成了一个转折点</strong>，这篇综述我至今仍然能口述，简直是救命的神药。论文虽然没有说具体的研究理论和方法，但是既然是综述，也就提到了很多名词，我也知道了要学什么，要做什么，目前的研究现状如何，因为这篇文章，我感觉我的瓶颈都解决了。</p><p>于是就开始<strong>按图索骥，查找有关方法和理论</strong>，慢慢把技术学起来，另外我还百度了一本大型著作——<strong>《统计自然语言处理》</strong>。从基本的语言模型，从简单的开始，慢慢开展（<strong>我的第一个文本分类的基线就是从TFIDF+SVM开始的</strong>），然后是，用着自己蹩脚的，现在看来简直是垃圾的代码技术，<strong>想一行，搜一行地完成了这块完整的代码</strong>，并通过各种实验最终完成了自己的第一稿论文（创新点啥的肯定都是有的），那时就是12月。</p><p>懒人记录：</p><ul><li>别老说没灵感，艺术家如此，科研也如此，<strong>灵感来源于你的积累，积累从哪里来，论文</strong>。</li><li>动手查，动手做，多查资料，百度、论文、github，道理都懂，但是<strong>为啥还有很多人会问我百度就能知道的问题呢？</strong></li><li>不是宣扬独立战斗，团队应该有团队的样子，但是你应该有独立解决问题的能力，否则团队要你干吗？会求助是好事，但是<strong>要问到别人手把手教你的层次，那就是个巨婴了</strong>。</li><li><strong>综述是个好东西</strong>，带survey、review的多半是，看看会对研究现状有清晰的了解，近期的关键论文基本都被引在里面了，你也省去很多找论文和鉴别论文的时间。</li><li>《统计自然语言处理》最好看看，虽然里面谈的方法不多，但是引文很全，甚至建议引文都看看，<strong>简单过一遍至少你知道文本分析和自然语言处理的差别在哪里，别老是炒概念</strong>。</li><li>从最简单的开始，先做一个初步的，然后开始慢慢加大难度，这应该是一个正常的学习历程。</li></ul><h4 id="进阶阶段"><a href="#进阶阶段" class="headerlink" title="进阶阶段"></a>进阶阶段</h4><p>说到进阶，大概就是从18年上半年开始吧。当时面临的一个大问题是怎么找工作，当然还有论文的推进，到了工业界，技能要求更完善，因此这块也是我自学的集中期，论文在坚持看但是重在基础的完善（<strong>光看论文知识很难系统化、完善化</strong>）。</p><p>深度学习方面，自己的深度学习是平时积累的，BP是本科的课程就已经接触，CNN之类的也是在平时找个1天左右看博客看会的，RNN是后来自己用到了才临时看的，包括后续的LSTM之类的，真没有集中专项学习。我目前会tensorflow和keras，前者是淘宝花了4块钱买的教程，keras是看文档自学的，后来买了一本紫色封面的书边看边学会的。</p><p>自己在文本分类方面看过不少论文，也有重现，所以有一些理解。建议大家还是把文本分类能自己动手把几个常见模型都看一遍（博客上、公众号上经常有人提到的，就叫做常见），毕竟文本分类是最最简单的自然语言处理任务，也是门槛最低的，这个都不会复杂的任务更加做不了对吧。</p><p>另外，为了自己对NLP目前现状有个更好地了解，我自己投资了一下，买了一门网课吧，选的是深蓝学院的课，其实几百块，每周2次，真不指望能学到什么（这么短时间就能学会为啥还有人感慨学不会呢？），但是能让我知道我有哪些是应该知道但是不知道的，然后开始学起来。</p><p>NLP的自学其实到这块就已经进入这个行业了，进入到行业里也就有了很多信息，过程的话我就写到这里吧。</p><h3 id="学习路径"><a href="#学习路径" class="headerlink" title="学习路径"></a>学习路径</h3><p>上面说了很多，现在来总结一下我的学习路径，当时算是刀耕火种的时代，下面这个只能做参考。</p><ul><li>《统计学习方法》</li><li>刷有关领域的论文，因为论文体现发展前沿的重要方式</li><li>《统计自然语言处理》</li><li>博客+论文级别的深度学习基础get</li><li>淘宝买的tensorflow视频，keras文档学keras</li><li>自学gensim等NLP工具+动手实践</li></ul><p>由于现在资源比较丰富了，所以还是推荐大家看看下面的材料</p><ul><li>百度、淘宝、github、论文，最快捷、扎实的学习路径</li><li>机器学习：《统计学习方法》、雷明的《机器学习与应用》，塞巴斯蒂安的《python机器学习》，sklearn的API文档建议自己多看看查查</li><li>深度学习：黄文坚《tensorflow实战》、tensorflow技术解析与实战</li><li>自然语言处理：《统计自然语言处理》、刘兵的《情感分析》、《基于深度学习的自然语言处理》。个人还是建议多看看博客、公众号等，经常提到的模型才是最好的，目前大部分书都只是浮于表面，这只与NLP这块的发展现状有关，博客多看自己也多动手</li><li>其他：《数据挖掘导论》，《机器学习实践指南》、《python数据分析与挖掘实战》、《精通数据科学》</li></ul><p>从我的视角，这些书能够看完并且能够理解，超过60%-70%的人没问题，关键就看你能不能看下来。</p><h3 id="学习建议-1"><a href="#学习建议-1" class="headerlink" title="学习建议"></a>学习建议</h3><ul><li>扎实基础，无论是数学还是编程开发，不要想着避开，出来混总是要还的</li><li>不要老是抱怨自己没有资料，只是你没找过而已，甚至是百度淘宝都是有的</li><li>不一定要系统学习，前沿知识很难有体系，所以一方面平时要接受信息，另一方面坚持去查资料看</li><li>多阅读工具给的文档，英文？翻译软件能帮到你</li></ul><h3 id="其他建议"><a href="#其他建议" class="headerlink" title="其他建议"></a>其他建议</h3><ul><li>这个领域对数学和技术都具有一定要求，门槛其实很高，当然赚的不少，但是有没有媒体说的那么好呢，我的答案是否，所以大家要谨慎，别就顾着炒概念和涌入，泡沫破了代价很大。</li><li>技术扎实是安身立命之本，裁员也裁水平不够的对吧</li><li>多！看！论！文！没灵感了看论文，实验不会做了参考论文，文章格式不会拉的时候看论文。</li><li>尽量尝试自己把问题解决，问别人之前多问问自己，百度了没，谷歌了没，看过论文没，都是怎么说的，我还没解决的点在那里，绝对比问一个人“在吗”然后抖着腿等高效多了</li></ul><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>没想过自己会通过这个基于回顾到整个自己自学的历程，研究生历程远不止如此，但是NLP的学习历程基本能覆盖，但是NLP的学习和进步历程远不止这些，由于自己的工作方向等原因，近期我还在看短文本分了等方面的内容，有新的进展会和大家分享，敬请期待。</p><p>发布于 2019-04-27 22:47</p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何不痛苦地早起？</title>
      <link href="/2021/11/27/%E5%A6%82%E4%BD%95%E4%B8%8D%E7%97%9B%E8%8B%A6%E5%9C%B0%E6%97%A9%E8%B5%B7%EF%BC%9F/"/>
      <url>/2021/11/27/%E5%A6%82%E4%BD%95%E4%B8%8D%E7%97%9B%E8%8B%A6%E5%9C%B0%E6%97%A9%E8%B5%B7%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.zhihu.com/people/gao-leng-leng-61">高冷冷</a></p><p>哈工大化学本科丨北大中文硕士</p></blockquote><p>我……<strong>我是个几乎十年如一日，晚11点睡、早5点自然醒的存（qi）在（pa）</strong>……习惯是需要用科学方法养成的，祭出我的早起大法。接下来几天，不妨在起床时试一下~</p><h2 id="一、-早起的时间要精准，动作一定要利索"><a href="#一、-早起的时间要精准，动作一定要利索" class="headerlink" title="一、 早起的时间要精准，动作一定要利索"></a><strong>一、 早起的时间要精准，动作一定要利索</strong></h2><p>对于养成习惯来说，时间的确切性和行为的精确性是最重要的。着重说一下早起行为的精确性：</p><h3 id="a-打破惯性"><a href="#a-打破惯性" class="headerlink" title="a. 打破惯性"></a><strong>a. 打破惯性</strong></h3><p>从床上的懒散状态转变成起床的行动状态，是对惯性的打破，对每个人来说，都会造成不适（只是程度不同，我的不适感几乎为零，除非冬天特别冷的时候）。但<strong>这种不适其实只会持续不到1分钟，只要你利利索索地坐起来，下床，裹上衣服，就可以顺利消除这种不适了。</strong>大家真的不妨尝试一下，只要你从床上滚出来了，你就不那么难受了。</p><p>诀窍是：动作要快、姿势要帅。<strong>具体来说：早上醒来时，可以大喊一声（或者怒目圆睁，在大脑中想象自己喊这么一句）：“谁踏马都别想拦着我起床！”/“1、2、3，老子要变身了！”然后起来，打破之前懒散的惯性。<br>啊啊啊虽然很中二，但真的管用真的管用！这种想法，</strong>像是一种开关，一种对大脑和身体的暗示。<strong>我的感觉是，这种口号，可以把注意力从对早起和赖床的纠结中，转移到一个不那么让人感到痛苦的、有点无聊的事情上（比如拯救地球啊），这个时候我自己会忍不住默默笑自己一下“…真的好神经病啊”，但是就会觉得轻松愉快、清醒多了，然后可以较为顺利地开启起床模式。之前有胖友说他喊完变身之后，没有用。</strong>这是因为，身体没有行动。<strong>用意念，是无法开启机器的（哈哈哈哈哈，想起了谢耳朵两手放在脑袋旁啊哈哈哈哈哈）。只有按下开关，让机器艰难地运转起来，和意念保持一致，才能真正行动起来。</strong>最重要是身体和想法要一致，让身体也投入到所做的事情当中。行为和想法一致时，人的身心才会觉得舒适，这是哈佛大学公开课“积极心理学”Tal讲的啦。</p><h3 id="b-思维集中于行为如何具体落实"><a href="#b-思维集中于行为如何具体落实" class="headerlink" title="b.思维集中于行为如何具体落实"></a>b.思维集中于行为如何具体落实</h3><p><strong>不用想那么多早起的终极奥义、人生价值，</strong>直接想“我需要做什么”就可以了**。想象早起是如何做到的，在大脑中想象自己在做这件事。坐起来，走到地上去，穿衣服。闭着眼睛也能做到。大脑喜欢内外的一致，想法和实际的一致。当你不做，却在想的时候，会焦虑，立刻跳起来行动，就可以了。（这个观念似乎是来自《精力管理》<a href="https://link.zhihu.com/?target=https://book.douban.com/subject/26606009/">精力管理 (豆瓣)</a>，想象一件事如何具体地、落实到细节地发生，会非常有利于这件事的实现）<br>**</p><h2 id="二、让早起本身，变成一件有趣的事情"><a href="#二、让早起本身，变成一件有趣的事情" class="headerlink" title="二、让早起本身，变成一件有趣的事情"></a>二、让早起本身，变成一件有趣的事情</h2><h3 id="a-设定非常具体的小目标"><a href="#a-设定非常具体的小目标" class="headerlink" title="a.  设定非常具体的小目标"></a>a.  设定非常具体的小目标</h3><p>『明早我要在5分钟内，完成起床、穿衣服、叠被子这三件事。』我挺喜欢这样玩儿的，很有效。可以放一首喜欢的音乐进行计时。比如《凤凰展翅》（最近爱上了这首广场舞神曲是几个意思…）啥的，换着玩儿呗。别把起床看成那么苦大仇深的事情，当作游戏就好了。</p><h3 id="b-将早起和一件不功利、有趣的事情联系起来"><a href="#b-将早起和一件不功利、有趣的事情联系起来" class="headerlink" title="b. 将早起和一件不功利、有趣的事情联系起来"></a><strong>b. 将早起和一件不功利、有趣的事情联系起来</strong></h3><p>比如，<strong>每天早上同一时间，跑到某处去拍清晨的阳光（对不起，我不是故意黑撕乎的）</strong>；比如，拍摄每天早上6:30笑容满面、精神奕奕的自己，虽然丑吧……；比如，每天早上同一时刻发朋友圈假装很荔枝的样子。自己建立这种仪式化的连结，会让早起变得轻松有趣起来。把早起当作一个游戏，别和梦想啊、人生巅峰啊、革命啊联系起来。</p><h3 id="c-奖励机制"><a href="#c-奖励机制" class="headerlink" title="c. 奖励机制"></a>c. 奖励机制</h3><p>**集齐多少天清晨的阳光，就可以召唤神龙啊之类的。如何奖励，看个人喜好。最后想告诉大家，早起可以成为一件不痛苦的事情，但一开始的1周、也许是1个月吧，会是有些许痛苦的。习惯养成之初，都是如此，这个过程是必须经历的。但是，对我来说，不早起已经成为一件痛苦的事情。在出生以来的13年中，绝大多数情况，我都是在早上5点~7点之间起床。是自然醒，基本不会使用闹钟。最近夏天了，5点自然醒吧。无语凝噎……其实我真的很希望能像别人一样睡个幸福的懒觉，但最多我也就睡到7点！7点啊少年少女们！●﹏●</p><p><strong>意志和纪律需要你迫使自己行事，与此不同的是，仪式/习惯吸引你行事。（《精力管理》）<br>按部就班、具体地落实这些有关于早起的仪式/习惯之后，就可以不痛苦地早起了。我不提倡以鸡血、动力、信念等来苦苦强撑着早起，这种状态是不可持续的，</strong>鸡血很容易耗尽，人也就随之废了。**<br>另外还想说两点很重要的：</p><p>一、先保证晚上是11点之前睡觉<strong>：所需睡眠时间因人而异，就不多说了。</strong></p><p><strong>二、别赋予“早起”这件事太多意义，也不要盲目将“早起”与其他事情建立过多的因果关系</strong>，比如：早上起不来我真是个废物；早上起不来我大概什么目标都实现不了了……赋予太多沉重的意义和关联，会消耗你很多的意志力，人便进入狭窄的恶性循环，无法去做其他更多事情了。<br><strong>就。算。不。早。起。老。子。也。照。样。秒。杀。你。们。</strong>这样告诉自己，会轻松许多,反而能把更多意志力集中于实现早起。</p><p>感谢阅读。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 自律 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何培养出自驱型学习的孩子？</title>
      <link href="/2021/11/27/%E5%A6%82%E4%BD%95%E5%9F%B9%E5%85%BB%E5%87%BA%E8%87%AA%E9%A9%B1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AD%A9%E5%AD%90%EF%BC%9F/"/>
      <url>/2021/11/27/%E5%A6%82%E4%BD%95%E5%9F%B9%E5%85%BB%E5%87%BA%E8%87%AA%E9%A9%B1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AD%A9%E5%AD%90%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：珽加<br>链接：<a href="https://www.zhihu.com/question/483938762/answer/2238772652">https://www.zhihu.com/question/483938762/answer/2238772652</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>本硕毕业于北京大学，从中学起未参加过任何升学考试，也没有参加过什么培训班，一路保送进了北大。</p><p>这里谈一下，我对自驱型学习孩子的看法。</p><p>曾经，我很多次认真问过自己，也问过身边的大神学霸：你真的喜欢学习吗？</p><p><strong>答案是：几乎没有人喜欢学习，这里指的是应试教育为目的的学习。</strong></p><p>你可以想象，自己上学的时候真的喜欢学习吗？</p><p><strong>如果目的是无论如何让孩子真心喜欢上学习，那么可能就会让你失望了。</strong></p><p>以考试为目的的学习，是这个世界强加给孩子的。</p><p>难道培养自驱型的孩子就没有办法了吗？</p><p>我在大学里发现，虽然大家大多都没有那么喜欢学习，但是总有一些人，每天认认真真的看书，认认真真的写作业，甚至不断的刷题加练，好像乐此不疲。</p><p>总有一些人，每天被厌学的情绪笼罩，间歇性的踌躇满志，经常被被打回原形，拖延、懒惰、不专注，甚至最终导致辍学，整个人的生活状态都开始走向压抑与抑郁。</p><p>他们之间到底有什么样的差别呢？</p><p>家长们一定要转念：<strong>我要培养的，不是一个自驱型学习的孩子，而是一个认真学习的孩子。</strong></p><p><strong>这句话特别重要，我务必要强调一遍。</strong></p><p>其实，不断是大人还是孩子，来到这个世界上，都会遇到很多意想不到的事情，甚至大部分事情都是意想不到的，都不是我们内心里想要的。</p><p>这时候，是选择逃避吗？可能会永远处在焦虑、逃避、压抑之中。</p><p>我们能做到的最好的是什么呢？<strong>就是极致认真对待每一件事情，仅此而已。</strong></p><p><strong>如果你的孩子能够认真对待学习本身，相信我，他一定就是一个学霸，早晚会成为一个学霸，而且他会在自己未来的人生路上，越走越顺利，充满斗志，无怨无悔。</strong></p><p>举个例子，</p><p>我在初二的时候、高一的时候、大一的时候，都经历过考试的巨大失败，甚至挂科。</p><p><strong>每个孩子都会经历失败，都会经历压力，都会经历厌学，这种滋味并不好受。</strong></p><p>但是每次的焦虑与负面情绪，都会给我巨大的警醒：要更认真的对待学习，这才是人与人之间拉开差距的根本，这也是我唯一能做的事情。</p><p>于是，观念一旦转变，一切都不同了，整个人的状态都完全不同了：成绩连续的提升，不舍得给自己放假，认认真真的做好每一道题目，即使在自己最难受的时候依然如此，直到拿到了我自己都想象不到的结果，考进了最好的大学，达到了无数的奖学金，还成为了优秀毕业生。</p><p><strong>极致认真，不是在自己开心的时候认真，而是在自己充满压力、焦虑的时候，依然认真。</strong></p><p>如何才能让孩子意识到，要真正极致认真的学习呢？</p><p>在我看来，<strong>极致认真的学习=榜样法则+成长法则+圈层法则+高效法则。</strong></p><hr><p><strong>001 要有一个榜样。</strong></p><p>为什么通常老师家庭出身的孩子，通常学习都比较好呢？</p><p>因为在这个家庭的氛围下，都极其认真的重视学习这件事情。</p><p>注意，这里的重视，不是说教出来的，而是家长们做出来的。我们通常很重视说教的作用，而忽略了耳濡目染的根本。</p><p>举个例子：</p><p>我爸妈都是小学老师，其实他们在学习的知识和能力上，很难帮助到我。但是，有一个场景却经常在我学习期间，出现在我的脑海里：</p><p>那时候有很多家长来找我爸，询问自己孩子的学习与生活问题，他对每一个家长都特别认真的对待，都特别有耐心。</p><p>而且，自己经常吃完晚饭之后，一个人去学校加班加点的工作，参加各种学习，写各种工作笔记。</p><p><strong>我看到他如此认真的对待自己的事业，我也时刻提醒着自己：一定要认真对待自己的事情，那就是学习。</strong></p><p>这句话，爸从来没这么跟我说过，但是我却看到了，在我学习很苦、很累的时候，一直支撑着我往前走。</p><p>我记得，在初二的时候我差点儿抑郁，原因是学习压力大，各种焦虑，也不想学习，总觉得自己不够好。</p><p>如果是换做大部分人，可能就回家休息了，但是我要极致认真啊！</p><p>于是，我把所有的焦点，都放在了找自己的学习问题，并且解决学习问题上。</p><p><strong>不去在乎情绪对我的影响，即使有影响，我也会带着这些情绪前行。</strong></p><p>奇迹出现了，这段时间成绩竟然比我以前的所有成绩都好，甚至提前进入了高中的重点班。</p><p>这一切还是要归功于身边的人带给我的：<strong>极致认真。</strong></p><p>如果你的孩子身边，有真正极致认真对待自己事业，极致认真对待自己学习的人，家长也好，导师也好，对孩子来说，都是一个巨大的幸福。</p><hr><p><strong>002 要时刻保持成长。</strong></p><p>学习的过程，总归是不那么好受的。</p><p>那么，有没有一些方法可以让学习更好受，更开心，更有趣一些呢？</p><p>当然有：<strong>学习就像是一个爬楼梯的过程，当我们爬到了某个位置，自然就拿到了想要的结果。</strong></p><p>理解了这句话，其实学习就没有那么难受了。</p><p>拿学习中，最经常出现的的考试为例。</p><p>我在高中有一个同桌，每次考完试，如果考不好，就把卷子扔到课桌抽屉里面，再也不想见到它，再也不想看它了。</p><p>所以，他的重心，完全在考试考的好不好上面，而不是成长上面，这正好陷入了让自己不喜欢学习的陷阱啊！</p><p>什么是爬楼梯？什么是考试真正的目的？</p><p><strong>就是把不会的题目变会，把会的题目变快，仅此而已。</strong></p><p>我的做法却恰恰不同：</p><p>自己考的很差的时候，虽然也会难过，但是我总会跟自己说：太棒了！又找到了真正问题所在。</p><p>所以，我会更认真的对待自己的复盘，甚至比我做的题目的原来答案还要详细。</p><p>当走自己考的很好的时候，我总会跟自己说：太棒了！我又有一些地方做的比较好，我又有了新的经验了。</p><p>你看，<strong>不管是考得好，还是考的不好，对我来说都是巨大的成长，那学习不就很有意思了吗。那不就跟更愿意去做了吗？</strong></p><p>想想我们大人是不是也如此：如果一件事情，不管做的多么不好，都有巨大的收获，是不是就更愿意去尝试了？</p><p>所以，在面对孩子的时候一定要记住：<strong>一起都是在陪伴孩子成长的。</strong></p><hr><p><strong>003 要有一个圈层。</strong></p><p><strong>如果一个人长期的接触真正的学霸，在学习上一点儿浮躁的心都会没有了。</strong></p><p>拿大人赚钱举个例子。</p><p>如果想要赚钱，一定要去到赚钱的圈子里，不仅会发现更多的赚钱的方法、门路、信息，而且浮躁的心态会慢慢的没有了：比你优秀的人，比你还努力。</p><p>如果孩子不认真学习，一个特别好的方法就是：找机会更真正的学霸呆在一起，交流在一起，尽量的时间长一些，效果极其的明显。</p><p>我在高中的时候，被有幸选入了物理竞赛班级，这里可是真正的学霸聚集的地方。</p><p>你看到那些真正聪明的人，每天埋头苦读，每天还不断的与人交流，不断的主动跟老师请教，自己一点点厌学的几乎都没有。</p><p>连续6个月，大家吃、住、学都在一个楼里面，魔鬼式的练习，最终几乎一大半的人都拿到了省级一等奖。</p><p>痛苦吗？总归是不好受的，但是对于更认真的学习，帮助太大了！</p><p>大学的时候，我大一挂科了，成绩在后1/3。</p><p>大二、大三年级TOP的人走到了一起，让我不得不更努力，从而与最优秀的人能够说得上话，能够进行正常的学习上的沟通与交流，之后，我就在大三成为年级的TOP10%。</p><p>一定要创造机会，<strong>让自己的孩子，多去接触真正的学霸，让自己的孩子与学霸们真正的交流。</strong></p><p>孩子遇到的问题，一定被很多学霸遇到过并且解决了，甚至这些问题都不是真正重要的问题，而且能够看到学习的本质，更快的进入学习状态。</p><p>如果一个学生进入到了一个都在自驱学习的氛围里面，他真的很难不自驱。</p><hr><p><strong>004 要让自己擅长学习起来。</strong></p><p>我身边有一个朋友的孩子，他特别偏科，很喜欢学数学，但是就是不喜欢学英语。</p><p>爸妈给他找了好多老师，但是就是没什么作用。</p><p>跟他聊了两次之后，突然有一天，他把课本上几乎所有的单词都记住了，而且非常的开心。</p><p>我非常的吃惊，于是就跟他一起探讨如何做到的。</p><p>第一，他真正开始认真学习英语了，知道学好英语对自己到底意味着什么：<strong>认真。</strong></p><p>第二，掌握了记单词最核心的方法（其实核心的方法都很简单），<strong>以前学习特别低效，现在极其高效的完成了自己的目标，走向了正向循环。</strong></p><p>你看，当他可以认真对待英语，高效的学习英语的时候，那股兴趣就自然而然来到了。</p><p><strong>认真的学习态度+高效的学习方法=真正的学霸诞生。</strong></p><p><strong>其实，我们多么喜欢一个领域，真的要看看我们多么擅长这个领域。</strong></p><p>比如，</p><p>越擅长演讲的人，演讲起来越自如，越有感染力，越爱演讲。</p><p>越擅长学习的人，越喜欢去做难的题目，越喜欢挑战，越爱学习。</p><p>越擅长赚钱的人，越享受钱的乐趣，越喜欢赚钱，越愿意提起赚钱。</p><p>当我们可以掌握了真正高效的学习方法，看透学习的本质之后，学习的内驱力当然也会随之上升。</p><p>具体如何高效的学习，我就不跟大家详细介绍了。</p><hr><p>好了，这就是想要跟大家分享的内容。</p><p>总结一下：</p><p>如何培养出自驱型学习的孩子呢？最重要的就是要做到认真对待学习。</p><p><strong>要有一个榜样，要时刻保持成长，要有一个圈层，要做到真正擅长。</strong></p><p>如果想要更多的交流关于如何培养出自驱型的孩子，或者如何真正高效的学习，也欢迎来链接我。</p><p><a href="https://link.zhihu.com/?target=https://0-1-byte.com/3z">找珽加：如何培养出自驱型的孩子，如何启发孩子真正的认真学习</a></p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 自律 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理(NLP)入门学习资源清单</title>
      <link href="/2021/11/27/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-NLP-%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95/"/>
      <url>/2021/11/27/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-NLP-%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95/</url>
      
        <content type="html"><![CDATA[<p>作者：Melanie Tosik </p><p>翻译：闵黎</p><p>校对：丁楠雅</p><p>本文长度为<strong>1100字</strong>，建议阅读<strong>3分钟</strong></p><p>Melanie Tosik目前就职于旅游搜索公司WayBlazer，她的工作内容是通过自然语言请求来生产个性化旅游推荐路线。回顾她的学习历程，她为期望入门自然语言处理的初学者列出了一份学习资源清单。</p><p><img src="https://pic2.zhimg.com/80/v2-2f3c07cc5fe26257edb0ee40af1842f9_1440w.jpg" alt="img"></p><p>displaCy网站上的可视化依赖解析树</p><p><a href="https://link.zhihu.com/?target=https://demos.explosion.ai/displacy/?text=Great,%20this%20is%20just%20what%20I%20needed!&model=en&cpu=1&cph=0">https://demos.explosion.ai/displacy/?text=Great%2C%20this%20is%20just%20what%20I%20needed!&amp;model=en&amp;cpu=1&amp;cph=0</a></p><p>记得我曾经读到过这样一段话，如果你觉得有必要回答两次同样的问题，那就把答案发到博客上，这可能是一个好主意。根据这一原则，也为了节省回答问题的时间，我在这里给出该问题的标准问法：“我的背景是研究**科学，我对学习NLP很有兴趣。应该从哪说起呢？”</p><p>在您一头扎进去阅读本文之前，请注意，下面列表只是提供了非常通用的入门清单（有可能不完整）。 为了帮助读者更好地阅读，我在括号内添加了简短的描述并对难度做了估计。最好具备基本的编程技能（例如Python）。</p><p><strong>在线课程</strong></p><p>•  Dan Jurafsky 和 Chris Manning：自然语言处理[非常棒的视频介绍系列]</p><p><a href="https://link.zhihu.com/?target=https://www.youtube.com/watch?v=nfoudtpBV68&list=PL6397E4B26D00A269">https://www.youtube.com/watch?v=nfoudtpBV68&amp;list=PL6397E4B26D00A269</a></p><p>•  斯坦福CS224d：自然语言处理的深度学习[更高级的机器学习算法、深度学习和NLP的神经网络架构]</p><p><a href="https://link.zhihu.com/?target=http://cs224d.stanford.edu/syllabus.html">http://cs224d.stanford.edu/syllabus.html</a></p><p>•  Coursera：自然语言处理简介[由密西根大学提供的NLP课程]</p><p><a href="https://link.zhihu.com/?target=https://www.coursera.org/learn/natural-language-processing">https://www.coursera.org/learn/natural-language-processing</a></p><p><strong>图书馆和开放资源</strong></p><p>•  spaCy（网站，博客）[Python; 新兴的开放源码库并自带炫酷的用法示例、API文档和演示应用程序]</p><p>网站网址：<a href="https://link.zhihu.com/?target=https://spacy.io/">https://spacy.io/</a></p><p>博客网址：<a href="https://link.zhihu.com/?target=https://explosion.ai/blog/">https://explosion.ai/blog/</a></p><p>演示应用网址: <a href="https://link.zhihu.com/?target=https://spacy.io/docs/usage/showcase">https://spacy.io/docs/usage/showcase</a></p><p>•  自然语言工具包（NLTK）（网站，图书）[Python; NLP实用编程介绍，主要用于教学目的]</p><p>网站网址：<a href="https://link.zhihu.com/?target=http://www.nltk.org">http://www.nltk.org</a></p><p>图书网址: <a href="https://link.zhihu.com/?target=http://www.nltk.org/book/">http://www.nltk.org/book/</a></p><p>•  斯坦福CoreNLP（网站）[由Java开发的高质量的自然语言分析工具包]</p><p>网站网址: <a href="https://link.zhihu.com/?target=https://stanfordnlp.github.io/CoreNLP/">https://stanfordnlp.github.io/CoreNLP/</a></p><p><strong>活跃的博客</strong></p><p>•  自然语言处理博客（HalDaumé）</p><p>博客网址：<a href="https://link.zhihu.com/?target=https://nlpers.blogspot.com/">https://nlpers.blogspot.com/</a></p><p>•  Google研究博客</p><p>博客网址：<a href="https://link.zhihu.com/?target=https://research.googleblog.com/">https://research.googleblog.com/</a></p><p>•  语言日志博客（Mark Liberman）</p><p>博客网址：<a href="https://link.zhihu.com/?target=http://languagelog.ldc.upenn.edu/nll/">http://languagelog.ldc.upenn.edu/nll/</a></p><p><strong>书籍</strong></p><p>•  言语和语言处理（Daniel Jurafsky和James H. Martin）[经典的NLP教科书，涵盖了所有NLP的基础知识，第3版即将出版]</p><p><a href="https://link.zhihu.com/?target=https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p><p>• 统计自然语言处理的基础（Chris Manning和HinrichSchütze）[更高级的统计NLP方法]</p><p><a href="https://link.zhihu.com/?target=https://nlp.stanford.edu/fsnlp/">https://nlp.stanford.edu/fsnlp/</a></p><p>•  信息检索简介（Chris Manning，Prabhakar Raghavan和HinrichSchütze）[关于排名/搜索的优秀参考书]</p><p><a href="https://link.zhihu.com/?target=https://nlp.stanford.edu/IR-book/">https://nlp.stanford.edu/IR-book/</a></p><p>•  自然语言处理中的神经网络方法（Yoav Goldberg）[深入介绍NLP的NN方法，和相对应的入门书籍]</p><p><a href="https://link.zhihu.com/?target=https://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984">https://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984</a></p><p>入门书籍： <a href="https://link.zhihu.com/?target=http://u.cs.biu.ac.il/~yogo/nnlp.pdf">http://u.cs.biu.ac.il/~yogo/nnlp.pdf</a></p><p><strong>其它杂项</strong></p><p>•  如何在TensorFlow中构建word2vec模型[学习指南]</p><p><a href="https://link.zhihu.com/?target=https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html">https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html</a></p><p>•  NLP深度学习的资源[按主题分类的关于深度学习的顶尖资源的概述]</p><p><a href="https://link.zhihu.com/?target=https://github.com/andrewt3000/dl4nlp">https://github.com/andrewt3000/dl4nlp</a></p><p>•  最后一句话：计算语言学和深度学习——论自然语言处理的重要性。（Chris Manning）[文章]</p><p><a href="https://link.zhihu.com/?target=http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning">http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning</a></p><p>•  对分布式表征的自然语言的理解（Kyunghyun Cho）[关于NLU的ML / NN方法的独立讲义]</p><p><a href="https://link.zhihu.com/?target=https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf">https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf</a></p><p>•  带泪水的贝叶斯推论（Kevin Knight）[教程工作簿]</p><p><a href="https://link.zhihu.com/?target=http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">http://www.isi.edu/natural-language/people/bayes-with-tears.pdf</a></p><p>•  国际计算语言学协会（ACL）[期刊选集]</p><p><a href="https://link.zhihu.com/?target=http://aclanthology.info/">http://aclanthology.info/</a></p><p>•  果壳问答网站(Quora)：我是如何学习自然语言处理的？</p><p><a href="https://link.zhihu.com/?target=https://www.quora.com/How-do-I-learn-Natural-Language-Processing">https://www.quora.com/How-do-I-learn-Natural-Language-Processing</a></p><p><strong>DIY项目和数据集</strong></p><p><img src="https://pic2.zhimg.com/80/v2-53b36ea8adc4b7d1979d8fcee7a144f1_1440w.jpg" alt="img"></p><p>资料来源：<a href="https://link.zhihu.com/?target=http://gunshowcomic.com/">http://gunshowcomic.com/</a></p><p>•  Nicolas Iderhoff已经创建了一份公开的、详尽的NLP数据集的列表。除了这些，这里还有一些项目，可以推荐给那些想要亲自动手实践的NLP新手们：</p><p>数据集：<a href="https://link.zhihu.com/?target=https://github.com/niderhoff/nlp-datasets">https://github.com/niderhoff/nlp-datasets</a></p><p>•  基于隐马尔可夫模型（HMM）实现词性标注（POS tagging）.</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Part-of-speech_tagging">https://en.wikipedia.org/wiki/Part-of-speech_tagging</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Hidden_Markov_model">https://en.wikipedia.org/wiki/Hidden_Markov_model</a></p><p>•  使用CYK算法执行上下文无关的语法解析</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/CYK_algorithm">https://en.wikipedia.org/wiki/CYK_algorithm</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Context-free_grammar">https://en.wikipedia.org/wiki/Context-free_grammar</a></p><p>•  在文本集合中，计算给定两个单词之间的语义相似度，例如点互信息（PMI，Pointwise Mutual Information）</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Semantic_similarity">https://en.wikipedia.org/wiki/Semantic_similarity</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Pointwise_mutual_information">https://en.wikipedia.org/wiki/Pointwise_mutual_information</a></p><p>•  使用朴素贝叶斯分类器来过滤垃圾邮件</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Naive_Bayes_classifier">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering">https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering</a></p><p>•  根据单词之间的编辑距离执行拼写检查</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Spell_checker">https://en.wikipedia.org/wiki/Spell_checker</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Edit_distance">https://en.wikipedia.org/wiki/Edit_distance</a></p><p>•  实现一个马尔科夫链文本生成器</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Markov_chain">https://en.wikipedia.org/wiki/Markov_chain</a></p><p>•  使用LDA实现主题模型</p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Topic_model">https://en.wikipedia.org/wiki/Topic_model</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></p><p>•  使用word2vec从大型文本语料库，例如维基百科，生成单词嵌入。</p><p><a href="https://link.zhihu.com/?target=https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></p><p><a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Wikipedia:Database_download">https://en.wikipedia.org/wiki/Wikipedia:Database_download</a></p><p><strong>NLP在社交媒体上</strong></p><p>•  Twitter：#nlproc，NLPers上的文章列表（由Jason Baldrige提供）</p><p><a href="https://link.zhihu.com/?target=https://twitter.com/hashtag/nlproc">https://twitter.com/hashtag/nlproc</a></p><p><a href="https://link.zhihu.com/?target=https://twitter.com/jasonbaldridge/lists/nlpers">https://twitter.com/jasonbaldridge/lists/nlpers</a></p><p>•  Reddit 社交新闻站点：/r/LanguageTechnology</p><p><a href="https://link.zhihu.com/?target=https://www.reddit.com/r/LanguageTechnology">https://www.reddit.com/r/LanguageTechnology</a></p><p>•  Medium发布平台：Nlp</p><p><a href="https://link.zhihu.com/?target=https://medium.com/tag/nlp">https://medium.com/tag/nlp</a></p><p>原文链接：</p><p><a href="https://link.zhihu.com/?target=https://medium.com/towards-data-science/how-to-get-started-in-nlp-6a62aa4eaeff">https://medium.com/towards-data-science/how-to-get-started-in-nlp-6a62aa4eaeff</a></p>]]></content>
      
      
      <categories>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工程架构能力对于做好机器学习重要吗？</title>
      <link href="/2021/11/27/%E5%B7%A5%E7%A8%8B%E6%9E%B6%E6%9E%84%E8%83%BD%E5%8A%9B%E5%AF%B9%E4%BA%8E%E5%81%9A%E5%A5%BD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8D%E8%A6%81%E5%90%97%EF%BC%9F/"/>
      <url>/2021/11/27/%E5%B7%A5%E7%A8%8B%E6%9E%B6%E6%9E%84%E8%83%BD%E5%8A%9B%E5%AF%B9%E4%BA%8E%E5%81%9A%E5%A5%BD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8D%E8%A6%81%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>最近频繁参与公司的面试，在面试中，多多少少会问一些机器学习方向候选人工程架构方面的问题。不少做机器学习的候选人表示，我是做算法的，不需要了解这些工程架构的问题，甚至有表示，了解工程架构就是浪费做算法的时间。我认为，这种想法有些片面和狭隘。从我这几年一些浅薄的经验来看，工程架构能力对于机器学习从业者，不仅重要，而且是必备能力之一。</p><p>计算机科学是一门实验科学，不管多么厉害的算法，都是需要落地到应用的。一个好的机器学习算法，更是离不开好的工程实现。</p><p>举个例子，FFM算法在各种Kaggle比赛中大放异彩，我们来想想在不借用开源工具的情况下，怎么实现它。首先，核心计算逻辑，本质是不同特征的分成不同field的向量做乘法，那么怎么实现最高效？采用for循环肯定不是一个最优解法，这里就需要知道工程实现上的SIMD了；选哪一种SIMD，首先得知道手上的服务器支持哪一种，这里假设sse和avx都支持，那sse和avx除了数据流位宽有区别，还有什么其他区别吗，同一个语义可能有两种实现选哪一种等等，都离不开工程实现上的理解。假如这部分已经解决了，但是并行度还是不够，希望利用现代CPU多核并行的优势，那么是使用openmp还是自己搞一个线程池，发现使用多线程之后，加速比并不理想，那该怎么排查，是数据上的false sharing还是cache  miss不符合预期，如果是如何调整数据，如何做预取，这些都是需要工程思考的。这还没有完，这仅仅是计算，如果并行度足够了，但是发现数据放不进内存（通常都是这样），从磁盘读取的速度还跟不上训练，那怎么实现异步磁盘预取，怎么进行更多地优化，都跟实现紧密相关，因为你不仅需要知道代码怎么写最高效，还需要知道硬盘（HDD和SSD）的特性，对症下药；就算模型训练好了，怎么样把模型dump下来放到线上去，线上去的服务与训练逻辑相比没有了梯度更新，可以做的更快吗；流式更新发现很有帮助，怎么做？这些问题的解决无一不需要对工程的深入了解和掌握。</p><p>可能有同学会说，那么多开源的工具，我只需要拿过来会用就行。事实上真会这么容易吗？未必！开源的工具，往往是从通用的角度出发，没有针对特定的业务进行适配，那如何结合自己的业务场景和服务架构做整合，这也是算法人员必须要考虑的一个问题。在实际工作中，开源工具往往不能完全满足自己的需求，需要再在现有的代码做一些开发，那么至少需要了解面对的开源工具的工程架构设计，才能获得一个靠谱的解决方案。以上面说到的FFM举个例子，相信不少同学都听过甚至用过开源实现libffm，现在业务上数据太大（比如1PB），单机搞不定，需要扩展到分布式环境中去，参数也要通过parameter  server来存储，这些都是libffm不支持的，那怎么实现呢？首先，从分布式机器学习架构上讲，先确定数据并行还是模型并行，再层级分解问题，到数据通信、parameter  server数据组织和并行训练，包括其中一些关键的设计，比如这个分布式算法使用哪种一致性模型，无一不需要了解工程架构。即便就是单机能够搞定，那么serving部分的服务往往也是需要自己实现的。以XGBoost为例，训练好了模型，调用XGBoost预估，如果在多线程场景下，至少需要知道XGBoost这部分code是不是可重入的，如果不是怎么改进，更需要工程基础了。很多时候，使用开源的工具，其成本并不比自己开发一个低多少。</p><p>事实上，一个优秀的算法人员，首先必须是一个优秀的工程实现人员。我一直认为，一个算法人员的工程能力，决定了做算法的天花板。工程是算法的基础，基础不够扎实，怎么登高远望呢？大明宫含元殿能够俯瞰京师长安，那是光地基就有15米呀。大家所熟悉的这个领域的各种大牛，不仅仅是算法厉害，工程能力也是了得的。Google Brain的Jeff  Dean，这个只要是搞CS的，相信都听过，架构背景自不用多说，Google内部一系列基础的Infrastructure都是他参与的，比如BigTable等等；也正是有这样的工程能力，Google内部的算法迭代能力一直都非常强悍，『天下算法，唯快不破』，人家不仅是聪明，还快！同样的时间，一般人只能做一次迭代，但人家能够迭代五次，探索空间是一般人的五倍，做出来的算法自然很优秀，所以当GNMT出来的时候，大家很震撼，意料之外情理之中嘛，也很有Google的风格：工程真强！这样的case还很多，PS-lite的作者李沐，XGBoost的作者陈天奇，Caffe的作者贾扬清等等。</p><p>算法人员任何加强自己的工程架构能力？多写代码只是基本，最重要的是心态和意识。也就是说，要主动意识到工程的重要性。有了这个心态和意识，就会主动的去关注留意相关的知识，获取相关的信息，自发的学习，事半功倍。在这里，推荐看过的马超同学的一个回答，个人觉得不错。</p><p><a href="https://www.zhihu.com/question/23645117/answer/124708083">学习分布式系统需要怎样的知识？6280 赞同 · 181 评论回答</a></p><p>另外三年前，我也整理过一个分布式论文的列表，虽然时间长了点，但仍可参考一二。</p><p><a href="https://www.zhihu.com/question/30026369">分布式系统领域有哪些经典论文？5898 关注 · 32 回答问题</a></p><p>有了一定的理论基础之后，就应该动手实验一下了，这里还是要推荐MIT  6.824这门课，里边的lab非常不错，过一遍绝对收益颇多。然后，可以尝试把一些常用的算法，比如LR，尝试做成一个分布式的版本，尝试解决中间遇到的问题；在换个大一点数据集上测试一下，看看有没有一些新问题。如果这些都解决的不错的话，祝贺你，赶紧到实际工作中去提高业务吧。</p><p>工程和算法，是一对亲密的兄弟，很多时候，看似算法的问题，抽丝剥茧后发现，本质其实是一个工程问题。人工智能最近两年异常火爆，各种实际应用层出不穷，希望有志于此的同学，工程和算法，两手都要抓，两手都要硬。一家之言，寥寥数语，不知所言。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议15：学术会议</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE15%EF%BC%9A%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE15%EF%BC%9A%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE%E8%BF%87%E7%A8%8B%3A%E6%96%B0%E6%89%8B%E7%9A%84%E8%A7%86%E8%A7%92">学术会议过程:新手的视角</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%A4%9A%E6%96%B9%E9%9D%A2%E7%9A%84%E5%A5%BD%E5%BB%BA%E8%AE%AE">多方面的好建议</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%A6%82%E4%BD%95%E6%9C%80%E5%A5%BD%E5%9C%B0%E5%88%A9%E7%94%A8%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE">如何最好地利用学术会议</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%8F%82%E5%8A%A0%E4%B8%8D%E5%90%8C%E8%A7%84%E6%A8%A1%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE%E7%9A%84%E7%AD%96%E7%95%A5">参加不同规模学术会议的策略</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%9C%A8%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE%E4%B8%8A%E5%BB%BA%E7%AB%8B%E8%81%94%E7%B3%BB">在学术会议上建立联系</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8C%81%E8%81%94%E7%B3%BB">如何保持联系</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728470/edit#%E5%81%9A%E5%A5%BD%E4%BD%A0%E7%9A%84%E5%AE%B6%E5%BA%AD%E4%BD%9C%E4%B8%9A">做好你的家庭作业</a></p><hr><p>不幸的是,期刊文章要在研究结束后两年左右才能刊出,因此,你的图书馆里新上架的期刊的文章大概都是三年前完成的研究内容。在学术会议上获得最新的研究进展则相对容易得多。</p><h2 id="学术会议过程-新手的视角"><a href="#学术会议过程-新手的视角" class="headerlink" title="学术会议过程:新手的视角"></a>学术会议过程:新手的视角</h2><ol><li>会议通常在夏威夷之类的地方召开,并且他们对可以接纳的论文的标准又相对较低</li><li>如果有钱的话,还可以考虑是不是只单纯地作为观众去参加学术会议。</li><li>假定学术会议资助已经得到批准,你仍然需要安排旅行、食宿等事务</li></ol><h2 id="多方面的好建议"><a href="#多方面的好建议" class="headerlink" title="多方面的好建议"></a>多方面的好建议</h2><ol><li><p>你要尽量避免一些让你显得愚蠢的表现,这些表现如下:</p></li><li><ol><li>在公开场合醉酒。</li><li>因为在私下场合醉酒,结果抱病不能参加会议。</li><li>由于相信没有人会知道或者认为不是什么大不了的事而和某人发生性关系——事实上,每个人在次日的茶歇时间都会知道,你的床上伴侣可能会让你犯可怕的错误。</li><li>问一些新参会的博士年年都会问的愚蠢的问题。</li></ol></li></ol><ol><li>你的第一次学术会议是一个认识人并交朋友的好机会,这些友谊会延续到你的整个职业生涯。最好的方式就是在酒吧里和那些愿意和你谈话的人在一起进行一次严肃而且谦恭的对话。</li><li>一个减压的好方式就是在你去之前通过在公开场合演讲获得经验——例如，系里的讨论会,可以是一个很好的练习机会</li><li>你能学到导师的经验。牢记三条黄金法则：不要说谎；不要故作幽默；不要惊惶失措而在压力下吐露真相。</li></ol><h2 id="如何最好地利用学术会议"><a href="#如何最好地利用学术会议" class="headerlink" title="如何最好地利用学术会议"></a>如何最好地利用学术会议</h2><h2 id="参加不同规模学术会议的策略"><a href="#参加不同规模学术会议的策略" class="headerlink" title="参加不同规模学术会议的策略"></a>参加不同规模学术会议的策略</h2><ol><li>小型:目标是全面了解(例如:和那里的每个人谈话)。</li><li>中等:致力于和尽可能多的人说话,但是主要目标是了解与自己相关的研究情况。</li><li>大型:做精心安排，保证能够和关键人物联系上，并关注主要事件。</li></ol><h2 id="在学术会议上建立联系"><a href="#在学术会议上建立联系" class="headerlink" title="在学术会议上建立联系"></a>在学术会议上建立联系</h2><ol><li>利用活动(研讨会,小组讨论、新人聚会等)。</li><li>陈述论文(这会把你介绍给你的听众中的每一个人) 。</li><li>问好问题(其他人发现你的问题很有趣,或许向你介绍他们自己.人们会更容易记住你)。</li><li>如果你听到很有趣的对话,那么就站在周围可以让人看到的位置,直到你有机会参与谈话。一个短问题或者是一个玩笑都很好,或者直接问你是否可以参与这一话题的讨论。</li><li>让你的导师或者熟人介绍一些人给你认识。</li><li>同关键人物尽早接触。</li><li>同与你坐在一起的人说话。</li><li>养成习惯每天同不同的人在一起吃午餐。</li><li>当你在与别人对话的时候，避免滔滔不绝或者用你的观点来取悦别人,这时候问题往往比陈述要有效得多。</li></ol><h2 id="如何保持联系"><a href="#如何保持联系" class="headerlink" title="如何保持联系"></a>如何保持联系</h2><ol><li>会议联系通常会有很高的“折旧率”,你应该尝试将会议上认识的人转变为长期联系。</li><li>永远履行你做出的承诺:真的寄送论文,或者用电子邮件发信息。</li><li>在与某人进行了很好的谈话后,发一封感谢邮件给他来保持联系。</li><li>建议访问或者进行友好的相互交流。</li><li>邀请好的联系人到你的机构,比如请联系人来做一次演讲。</li><li>如果在大会上你没有机会同演讲者交谈,那么会后可以通过邮件来进行交流。</li></ol><h2 id="做好你的家庭作业"><a href="#做好你的家庭作业" class="headerlink" title="做好你的家庭作业"></a>做好你的家庭作业</h2><ol><li><p>当你收到会议相关资料后:</p></li><li><ol><li>看一眼与会人员名单。谁在这个名单上,你最期待见到谁?他们提交了论文吗?</li><li>看一下会议分组和论文标题并计划你如何出席哪一组的讨论。哪一部分不应该错过?什么时候讨论开始?谁是你期待在会上取得联系的第一个人?</li><li>参加全体会议通常是个好主意。</li></ol></li></ol><ol><li><p>每个晚上：</p></li><li><ol><li>浏览或阅读你计划参加的小组论文。</li><li>看一看其他分会场的论文(你可能会在喝咖啡时遇到他们的作者)</li><li>检查自己的次日计划。</li></ol></li></ol><ol><li><p>带上你的会议论文集到会场:</p></li><li><ol><li>查找文章并且加以分类</li><li>给文章加注释:例如,如果作者在演讲期间增加了一些信息或者如果对你的文章有一些特别的联系。</li><li>列出要问的问题。</li><li>如果会议变得枯燥乏味没有意义,有事可作(例如,阅读其他文章)。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议14：答辩</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE14%EF%BC%9A%E7%AD%94%E8%BE%A9/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE14%EF%BC%9A%E7%AD%94%E8%BE%A9/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E8%AE%BA%E8%AF%81%E4%B8%8D%E4%B8%A5%E5%AF%86%E7%9A%84%E6%84%8F%E5%A4%96%E5%8F%91%E7%8E%B0">论证不严密的意外发现</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%BC%80%E6%94%BE%E7%9A%84%E8%88%9E%E5%8F%B0">开放的舞台</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%BC%80%E6%94%BE%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD">开放的开场白</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E4%B8%AD%E5%9C%BA%E6%B8%B8%E6%88%8F">中场游戏</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%BE%AE%E5%B0%8F%E7%9A%84%E6%94%B9%E5%8A%A8">微小的改动</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E4%B9%8B%E5%89%8D">答辩之前</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E5%89%8D%E4%B8%80%E5%91%A8">答辩前一周</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E5%89%8D%E4%B8%80%E5%A4%A9">答辩前一天</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E5%BD%93%E5%A4%A9">答辩当天</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E4%B9%8B%E5%90%8E">答辩之后</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%A4%84%E7%90%86%E4%BF%AE%E6%94%B9%E5%B7%A5%E4%BD%9C">处理修改工作</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E7%AD%94%E8%BE%A9%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">答辩的一些注意事项</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E4%BD%A0%E5%9C%A8%E7%AD%94%E8%BE%A9%E4%B8%AD%E5%81%9A%E4%BB%80%E4%B9%88%3F">你在答辩中做什么?</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%AF%BC%E8%87%B4%E7%AD%94%E8%BE%A9%E4%BC%9A%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%8E%9F%E5%9B%A0">导致答辩会失败的原因</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E6%80%8E%E6%A0%B7%E6%89%93%E5%8A%A8%E8%AF%84%E5%AE%A1%E4%BA%BA">怎样打动评审人</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E5%87%86%E5%A4%87">准备</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728231/edit#%E6%88%98%E8%83%9C%E6%81%90%E6%85%8C">战胜恐慌</a></p><hr><ol><li>你必须在不确定性中工作,在一个可以预知的未来,这种不确定性可能无法证明清楚。</li><li>他们并不想看到,因为你的导师出于对你的绝对无私的爱护已代你写出了全部内容,所以你的论文精彩绝伦。他们不希望发现,由于你从未思考过某个显而易见的结论,所以你的论文没有提到它,而不是因为你认为这个结论太显而易见、不值一提而未在文中提及。</li></ol><h2 id="论证不严密的意外发现"><a href="#论证不严密的意外发现" class="headerlink" title="论证不严密的意外发现"></a>论证不严密的意外发现</h2><ol><li><p>大多数学生都会收集答辩的恐怖故事,这些故事使他们噩梦不断。在我们的经历中,答辩中遭受失败的人是很少的,而且失败似乎只归咎于两件事情:</p></li><li><ol><li>学生不听自己导师的建议,也不听其领域内的任何别的导师关于答辩的建议。</li><li>导师和学生之间的关系破裂,学生还不想办法去补救。</li></ol></li></ol><ol><li><p>答辩中的失败在原则上是可以避免的,以下是两项保护性措施:</p></li><li><ol><li>听从导师的建议。</li><li>建立起有效的人际关系网,在答辩之前,通过专题讨论会或者出版的方式展示你的研究工作,这样你的疏漏就会尽早得到警告。</li></ol></li></ol><h2 id="开放的舞台"><a href="#开放的舞台" class="headerlink" title="开放的舞台"></a>开放的舞台</h2><h2 id="开放的开场白"><a href="#开放的开场白" class="headerlink" title="开放的开场白"></a>开放的开场白</h2><ol><li>既然答辩通常都安排在系内,很多校外评审人只是作为参与者,开场白就没有必要像传统的会议那样用很多的时间问评审人是否旅行愉快之类的问题</li><li>一个开场白的例子就是校外评审人给你几张A4的纸,纸上列出他们发现的打印错误。你很容易认为这是评审人错过了你的论文的大的逻辑点,而吹毛求疵地找茬儿。事实上,根本不是这么回事。这实际上是一种礼节。</li><li>另一种广泛应用的开场白就是评审人对着提纲来谈谈你的论文多有趣或者多有可读性</li><li>如果你遇到这样的开场白,一个相对安全的回答就是干笑,用一种礼貌的声调说“谢谢”,表示你不是傻子。</li><li>第三个可能会误导你的开场白是让你给你的论文做大略的概述</li><li>这也是告诉评审人这篇论文中你觉得最重要部分的机会,这样可以让评审人知道你有多么地专业</li><li>清楚地说出事先准备好的论文概述，列出主要发现和主要贡献。</li></ol><h2 id="中场游戏"><a href="#中场游戏" class="headerlink" title="中场游戏"></a>中场游戏</h2><h3 id="微小的改动"><a href="#微小的改动" class="headerlink" title="微小的改动"></a>微小的改动</h3><ol><li>在接下来的答辩进程中，他们问你几个技术性问题检验你的专业水准,你或许不能回答出所有的问题,也不能完全符合他们的标准</li><li>你可以礼貌地询问他 们认为多维统计方法会给这个领域提供什么。然后你会和他们进入一场争论之中,这些争论最主要的一点就是让你证明你的能力。</li></ol><h2 id="答辩之前"><a href="#答辩之前" class="headerlink" title="答辩之前"></a>答辩之前</h2><p>在你做博士论文的早期,应该和你的导师讨论一下是否应该想办法在期刊上发表一两篇文章。在这个问题上,要根据不同导师和不同学科来定</p><h2 id="答辩前一周"><a href="#答辩前一周" class="headerlink" title="答辩前一周"></a>答辩前一周</h2><ol><li>在答辩前一周,重新通读你的论文和数据;加上一些关键的文献</li><li>模拟答辩包括开始的陈述部分，至少有一个模拟评审人。这个评审人要知道优秀的答辩和陈述的技能。</li></ol><h2 id="答辩前一天"><a href="#答辩前一天" class="headerlink" title="答辩前一天"></a>答辩前一天</h2><ol><li>在答辩当日找一个可靠的朋友让你随叫随到。</li><li>确认你了解评审人的名字、头衔以及主要著作。</li><li>确定你准备好了合适的干净衣服以及若干论文的副本</li></ol><h2 id="答辩当天"><a href="#答辩当天" class="headerlink" title="答辩当天"></a>答辩当天</h2><ol><li>在答辩期间非常有效地整理你关于全篇论文的思路</li><li>你可以使用技巧给自己争取一些思考时间，诸如，用一种思考的方式说“嗯”,说“那是个有趣的问题”或“我需要考虑一下”等。</li></ol><h2 id="答辩之后"><a href="#答辩之后" class="headerlink" title="答辩之后"></a>答辩之后</h2><ol><li>要确保人知道你在哪里,这样可以避免评审人到处找你。</li><li>大部分人需要把论文的一些内容作些改变,然后才能通过。在这个阶段争论怎样修改是不明智的。相反，你应该感谢对你的论文提出的修改意见,礼貌地感谢评审人和你的导师然后再去庆祝。</li><li>不要着急记住所有的修改;在此前你的导师应该已经安排好了,评审委员会将有一个清楚的修改清单,你次日就可以从他们那里拿到。</li></ol><h2 id="处理修改工作"><a href="#处理修改工作" class="headerlink" title="处理修改工作"></a>处理修改工作</h2><ol><li>抓紧时间去完成修改并且得到导师认可。写一封说明信详细阐述你在哪一部分做了改动是如何改动的——这使得评审人更容易检查你需要修改的每一项内容。</li><li>对于每一个参与此项工作的人而言,一封说明信会让工作变得更简单。</li><li>所有的这些结束之后，你或者再也不想看一遍你的论文,甚至恨不得把它烧成灰。别那么做这篇论文就像一张面具:你只看见了里面的缺点，而外面的人看到的是它闪闪发亮的外表。</li></ol><h2 id="答辩的一些注意事项"><a href="#答辩的一些注意事项" class="headerlink" title="答辩的一些注意事项"></a>答辩的一些注意事项</h2><h2 id="你在答辩中做什么"><a href="#你在答辩中做什么" class="headerlink" title="你在答辩中做什么?"></a>你在答辩中做什么?</h2><ol><li>表现出对学术系统和学科的尊重。</li><li>表现出自己精通这个领域和该领域的智力工具。</li><li>证明自己的研究思考独立完成。</li><li>参加学术讨论。</li></ol><h2 id="导致答辩会失败的原因"><a href="#导致答辩会失败的原因" class="headerlink" title="导致答辩会失败的原因"></a>导致答辩会失败的原因</h2><ol><li>认定答辩不是什么大不了的事情。</li><li>对于任何有关你所做的事情的回答都是“我导师让我做的”。</li><li>回答时只用简单的一个词语。</li><li>表现出不让步的态度。</li><li>态度极端傲慢无礼。</li><li>表现出缺少兴趣。</li><li>无法清楚描述你自己做的工作。</li><li>无法给最基本的术语下定义。</li><li>无法对你所引用的学术论文加以评论。</li><li>对考官无礼并直呼其名。</li></ol><h2 id="怎样打动评审人"><a href="#怎样打动评审人" class="headerlink" title="怎样打动评审人"></a>怎样打动评审人</h2><ol><li>来之前就精心准备。</li><li>认真聆听,理解问题并且直接表达。</li><li>进行目光交流。</li><li>表示出对你的工作的热情。</li><li>在一个更大的背景下看你的工作。</li><li>在回答问题时，能够直接指出问题针对的是论文中那一段落(强调关键段落)。</li><li>能够直接精确地说出引用的研讨会。</li><li>能够清楚地说出你的贡献的本质和大小。</li><li>除了自己的研究之外,对于未来的工作具有前瞻性思考和建议。</li><li>深思熟虑——能够清楚表达自己的工作什么是好的,什么是需要提高的,以及怎样提高。</li></ol><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ol><li>标注你的论文,在你最想提的那些页面放上“书签”(贴上注释或者类似的东西)。</li><li>判断一下是否需要相关的文件(例如关键性文章、数据示例),这些不包括在你的论文之内。你也可能根本不会用到这些东西,但是或者将这些材料准备好会让你感到安心。在头脑中过一遍那些答辩时一般会问的问题,并且思考答案。</li><li>列出你最害怕的问题的清单,并为它们设计答案</li><li>让有经验的人为你做一次模拟答辩,然后让他们说出什么地方表现较好,什么地方还有待进一步提高。(要知道,模拟答辩通常比正式答辩要难一些;模拟评审人经常会扮演极苛刻的角色,这样你才能知道如何做好最坏的打算)</li><li>浏览一下你的五个关键参考文献。(如果在你没有记住足够多的参考文献信息,那么就在参考文献旁作注解:他们做了什么,为什么重要,你的作品在什么地方和这些论文相关,这些论文对你的作品的启发。准备好使用作者名来指代这些关键论文。</li><li>浏览一下你的评审人的作品,注意他们的研究领域、研究方法和研究风格。</li><li>如果已经用足够的时间做好了准备,在前一天做一些完全放松的事情:运动,沿着海岸步行,看最喜欢的经典电影,全身按摩等。但这不包括过度饮酒、过度运动和任何让你第二天状态不好的活动。</li><li>答辩前一夜睡个好觉。</li></ol><h2 id="战胜恐慌"><a href="#战胜恐慌" class="headerlink" title="战胜恐慌"></a>战胜恐慌</h2><ol><li>暂停:让你有时间思考。</li><li>深呼吸:三次深度透彻心脾的呼吸,确保你的呼气缓慢,通常是有帮助的。</li></ol><p>喝一点水:桌子上通常都放着水。快一些做笔记。</p><p>Enable Ginger<em>Cannot connect to Ginger</em> Check your internet connection</p><p> or reload the browserDisable in this text fieldRephraseRephrase current sentence</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议13：研究设计</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE13%EF%BC%9A%E7%A0%94%E7%A9%B6%E8%AE%BE%E8%AE%A1/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE13%EF%BC%9A%E7%A0%94%E7%A9%B6%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%A0%94%E7%A9%B6%E9%97%AE%E9%A2%98">研究问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9">方法选择</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E7%9A%84%E6%96%B9%E5%BC%8F">数据采集的方式</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E9%87%87%E6%A0%B7%E5%92%8C%E6%A0%B7%E6%9C%AC%E5%A4%A7%E5%B0%8F">采样和样本大小</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%A0%94%E7%A9%B6%E7%B1%BB%E5%9E%8B">研究类型</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E8%A7%84%E6%A8%A1">规模</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95">数据分析方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%A0%94%E7%A9%B6%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%85%B8%E5%9E%8B%E9%94%99%E8%AF%AF">研究设计的典型错误</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E8%AE%A1%E5%88%92%E5%A4%A7%E9%87%8F%E7%A0%94%E7%A9%B6">计划大量研究</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%AC%AC%E4%B8%80%E6%AD%A5">第一步</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%AC%AC%E4%BA%8C%E6%AD%A5">第二步</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E9%81%93%E5%BE%B7%E8%A7%84%E8%8C%83">道德规范</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E7%BD%B2%E5%90%8D%E9%97%AE%E9%A2%98">署名问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401728023/edit#%E8%87%B4%E8%B0%A2">致谢</a></p><hr><h2 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h2><ol><li><p>研究问题的答案应该可以排除某一组似是而非的可能性。第二个研究问题将继续减小这种可能性空间,如此等等·直到最后只留下对于某个问题的唯一合理解释,而这个解释要符合事实</p></li><li><p>不好的研究问题多种多样。以下列出了最普遍的几类:</p></li><li><ol><li>为先入之见寻找支持：对这种问题的通常的看法就是一开始就去衡量某种效应,却不思考(a)这种效应是否的确存在,(b)问题的大背景是什么。</li><li>提出无法回答的问题：有些问题虽然重要,却是无法回答的。</li><li>提出一个无用的问题：值得推荐的做法是在你开始数据采集之前预计研究问题的可能答案,确保自己了解(a)为什么每种可能答案可以有效地减少该领域的问题空间,和(b)有哪些实际应用方面的启示。</li></ol></li></ol><h2 id="方法选择"><a href="#方法选择" class="headerlink" title="方法选择"></a>方法选择</h2><p>你对研究方法懂得越多,你就越有可能浪费最少的精力而获得最大的成功</p><h2 id="数据采集的方式"><a href="#数据采集的方式" class="headerlink" title="数据采集的方式"></a>数据采集的方式</h2><p>“问卷”或“访问”一般被认为是数据采集方式的同义词。但这两种类似的方式都很容易失败。两者在实验结论是否能推论到样本之外的其他族群(也就是说,得到的结论究竟在多大程度上与现实相符)上存在着严重间题。通常两者还会在样本是否具有代表性方面遇到问题</p><h2 id="采样和样本大小"><a href="#采样和样本大小" class="headerlink" title="采样和样本大小"></a>采样和样本大小</h2><p>你应该清楚能够被你所在领域接受的误差水平是多少,为什么这个误差水平是可以被接受的，它是怎样被计算出来的,它又意味着什么。</p><h2 id="研究类型"><a href="#研究类型" class="headerlink" title="研究类型"></a>研究类型</h2><h2 id="规模"><a href="#规模" class="headerlink" title="规模"></a>规模</h2><p>你同样也可以利用统计学的知识来估计,扩大样本可以在多大程度上使你获得更多发现。超出一定程度之后,增加样本数量就只是浪费资源了。</p><h2 id="数据分析方法"><a href="#数据分析方法" class="headerlink" title="数据分析方法"></a>数据分析方法</h2><p>以下分析方法是你在做任何统让分析时会遇到的,你应该熟悉它们:内容分析、编码分类、时间序列、语言分析、因果断言、记号语言学、解构法、实地理论。</p><h2 id="研究设计的典型错误"><a href="#研究设计的典型错误" class="headerlink" title="研究设计的典型错误"></a>研究设计的典型错误</h2><p>研究最大的障碍是研究者自己的假定。无知和孤立是研究的大敌。以下列出了其他一些应该注意回避的错误:</p><ol><li>不看清楚就往下跳:没有思考(对假设、证据、技术、可能出现的问题进行反思)。</li><li>无知:往往表现为做前人已经做过的研究(在图书馆呆一天可以节省六个月做多余研究的时间)。</li><li>颠倒顺序:在提炼出研究问题和明确证据的要求之前就选择研究技术(事情要一件一件地按顺序做)。</li><li>过高的期望:也叫做“眼睛比胃大”或“咬下来却嚼不动”(如果研究问题太大,换个小一点的;一辈子的研究需要一辈子去完成,但每一次只能完成一步)。</li><li>从指缝里溜走的沙子:一个精确的研究需要一个精确的问题，然而当实验的各个细节都在你的控制之下时,你可能已经忘记了你最初的目的,以及那个最根本的问题(后退几步，回忆一下研究是怎样开始的·各个步骤是怎样一节节连接在一起的；或许你需要用粗线条的方式来帮助自己明确研究的基本问题)。</li><li>偏见;要警惕偏见,要诚实,读一本这方面的好书</li><li>将轶事与事实混淆:“众所周知”的事情往往是不正确的(可以使用那些众所周知的事例来帮助自己形成问题 ,但解答问题必须寻找独立的证据)。</li><li>将统计与精确等同:弄清楚统计学能够解决和不能解决的问题,然后找到一个好的实验统计员咨询。目的是要了解各种类型的证据能够说明什么,不能说明什么。</li><li>优秀试验的错误引导;有时候你需要一种不同于别人的成功实验的实验方法。</li><li>对失败缺乏尊重:尼尔斯·玻尔(Nils Bohr)曾指出:“科学 不是那些有趣的东西,而是那些奇怪的东西。”伟大的研究往往是意想不到的。最糟糕的研究是没有带给你任何新信息的研究;你的“失败”提供了什么信息?</li><li>理论不足:回到图书馆去“补课”。</li><li>过于概括:注意自己的陈述,一丝不苟地处理推断链。</li><li>致命的独立:试图做各方面的专家(发展社交圈，与真正的专家一起喝咖啡）</li></ol><h2 id="计划大量研究"><a href="#计划大量研究" class="headerlink" title="计划大量研究"></a>计划大量研究</h2><h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><ol><li>你应该为自己寻找一个保护人或避难所。理想情况下,你的保护人和避难所应该分别是你的老板和你的工作。</li><li>你也能够对找你麻烦的人说:“这个回题你必须和我的老板谈。”</li><li>维系避难所并不是通过卑躬屈膝或贿赂,而是通过恪守要得到保护必须履行的职责,也就是说,你应该完成你分内的工作,并且支持你老板的管理工作。</li><li>但有时会出现不幸的情况,你的老板不愿意或者不能够保护你。这方面的典型例子是:你的老板是系主任,而你是系里资历最浅的讲师,不得不担任苦差事,比如组织系里的娱乐活动,给一个由300名不守规矩的二年级本科生组成的大班讲课等等</li></ol><h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><ol><li>在确保完成老板布置的工作和系里的职责后,你应该制定自己的计划。这是你的生活,你的事业,除了你没有其他人可以支配。</li><li>随便抓住一个机会,做一些最基本最简单的研究工作是很轻松的,也是很有诱惑力的行事方式。但你必须抗拒它们的诱惑力</li></ol><h2 id="道德规范"><a href="#道德规范" class="headerlink" title="道德规范"></a>道德规范</h2><h2 id="署名问题"><a href="#署名问题" class="headerlink" title="署名问题"></a>署名问题</h2><p>当引用文献是由三个或三个以上的作者合著完成时.情况就有所不同了。所有作者的名字会在第一次引用时出现(比如“史密斯,琼斯和高卜利1999”),而接下来的所有引用都将简写成“史密斯等1999”。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ol><li>当某人对你研究作出的贡献构成了研究一个不可忽略的部分时,他就有权利成为研究论文的合著者。</li><li>但如果某人对你研究的贡献只是他日常工作的一部分,他不对总体研究设计和研究性质有任何贡献,并且只按照研究者的具体指示来作出他的贡献(比如按照研究者需要的设备要求来准备实验器械的技术人员),那么一般只在致谢中肯定他的贡献,而不给予合著者身份。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议12：陈述报告</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE12%EF%BC%9A%E9%99%88%E8%BF%B0%E6%8A%A5%E5%91%8A/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE12%EF%BC%9A%E9%99%88%E8%BF%B0%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%86%85%E5%AE%B9%E4%B8%8E%E5%BD%A2%E5%BC%8F">内容与形式</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E8%AF%A6%E7%BB%86%E7%9A%84%E5%86%85%E5%AE%B9">详细的内容</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%8E%9F%E5%88%99">原则</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%9C%A8%E5%86%85%E5%AE%B9%E6%96%B9%E9%9D%A2%E5%AE%B9%E6%98%93%E7%8A%AF%E7%9A%84%E7%BB%8F%E5%85%B8%E9%94%99%E8%AF%AF">在内容方面容易犯的经典错误</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%85%B7%E4%BD%93%E7%9A%84%E5%BD%A2%E5%BC%8F">具体的形式</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%9C%A8%E5%BD%A2%E5%BC%8F%E6%96%B9%E9%9D%A2%E5%AE%B9%E6%98%93%E7%8A%AF%E7%9A%84%E5%85%B8%E5%9E%8B%E9%94%99%E8%AF%AF">在形式方面容易犯的典型错误</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%85%B6%E4%BB%96%E5%B0%8F%E7%AA%8D%E9%97%A8">其他小窍门</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%A6%82%E6%9E%9C%E6%9C%89%E5%BF%85%E8%A6%81%E7%9A%84%E8%AF%9D%E5%8F%AF%E4%BB%A5%E5%9B%9E%E9%81%BF%E9%97%AE%E9%A2%98">如果有必要的话可以回避问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%85%B3%E4%BA%8E%E9%99%88%E8%BF%B0%E6%8A%A5%E5%91%8A%E7%9A%84%E6%A3%80%E6%B5%8B%E6%B8%85%E5%8D%95">关于陈述报告的检测清单</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%9C%A8%E4%BC%9A%E8%AE%AE%E4%B8%8A%E9%99%88%E8%BF%B0%E6%8A%A5%E5%91%8A%E8%AE%BA%E6%96%87%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%AA%8D%E9%97%A8">在会议上陈述报告论文的一些小窍门</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E9%A2%84%E9%98%B2%E7%B4%A7%E5%BC%A0%E6%83%85%E7%BB%AA">预防紧张情绪</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727790/edit#%E5%AF%B9%E4%BB%98%E6%8F%90%E9%97%AE">对付提问</a></p><hr><h2 id="内容与形式"><a href="#内容与形式" class="headerlink" title="内容与形式"></a>内容与形式</h2><h2 id="详细的内容"><a href="#详细的内容" class="headerlink" title="详细的内容"></a>详细的内容</h2><h3 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h3><ol><li>大部分口头陈述报告都分为三部分。通常的建议是：“首先，告诉听众你将要讲什么，然后开始讲，最后告诉听众你刚才讲了什么。”</li><li>开头部分决定讲话的总体基调向昕众解释你所要讲的话题为何重要，并为接下来的讲话内容作铺垫。第二部分包含了讲话的主要内容，是最长的部分。结尾部分总结你的发言。每个部分可以再分出若干个小部分，这要视讲话的规定长度来定。</li></ol><h3 id="在内容方面容易犯的经典错误"><a href="#在内容方面容易犯的经典错误" class="headerlink" title="在内容方面容易犯的经典错误"></a>在内容方面容易犯的经典错误</h3><ol><li><p>事先演练讲话并给自己计时，然后根据情况添加或删除内容再次演练，直到时间恰好为止。这种方法对于较短的陈述报告非常有效。</p></li><li><p>情况下更好的办法是在演练一遍之后根据需要调整材料的数量,并事先拟定一个计划:如果到时候时间不够,将略去哪些内容;时间有余,将添加哪些内容。</p></li><li><p>你可以事先计划好在什么时候应该讲到哪个内容,并把计划好的讲话进度写在纸上,做陈述报告时你就可以参考这页纸,以确认自己是否跟上了进度。</p></li><li><p>最常见的一个问题是如何选择难度适宜的材料来陈述报告。</p></li><li><p>最好的策略是与活动主办方联系事先了解听众的程度。</p></li><li><p>指导如何做商业报告和公共演说的文章往往强调,陈述报告必须要有趣味性。然而面对于学术报告,这并不是明智之举。</p></li><li><p>如果你攻读博士学位，你要做的口头陈述报告会有两大类</p></li><li><ol><li>第一类是作为丰富你简历的一项举措,给科学学科的硕土生或本科生授课</li><li>第二类是与其他研究人员的交流(比如研讨会、会议发言、内部进行的论文进度汇报、以及使人畏惧的论文答辩)。对这两类口头陈述报告的要求截然不同。</li></ol></li></ol><ol><li>讲义上应该标注权威性的期刊文章、教科书或其他形式的信息来源,用它们来为你的课堂陈述报告提供支持。这是非常值得推荐的做法。</li></ol><h2 id="具体的形式"><a href="#具体的形式" class="headerlink" title="具体的形式"></a>具体的形式</h2><h3 id="在形式方面容易犯的典型错误"><a href="#在形式方面容易犯的典型错误" class="headerlink" title="在形式方面容易犯的典型错误"></a>在形式方面容易犯的典型错误</h3><ol><li><p>在事先熟悉一下你将要使用到的设备是个很好的主意——要尽可能掌握多种设备。</p></li><li><p>另外,要做好设备出故障的准备，拥有后备方案——比如你在陈述报告时要用到PowerPoint,为了防止由于PowerPoint出故障而使你的发言不能继续，你应该另外准备一些使用高射投影仪播放的幻灯片,作为保险起见的后备方案。</p></li><li><p>你应该使用大号字体,以便坐在后排的人也能看清。同时你的幻灯片上必须要留出部分空白，使观众们不至于被过于密集的信息吓倒。</p></li><li><p>关键概念列出要点包.然后通过口头陈述报告去解释这些要点句,而不是念幻灯片上的语句。</p></li><li><p>典型的坏现象是:</p></li><li><ol><li>有人看窗外;</li><li>有人和周围的人说笑；</li><li>有人摇头；</li><li>后面几排有人为了听清楚你说了什么不得不前倾身子，伸长脖子﹔</li><li>后面几排有人询问周围的人幻灯片显示了什么;</li><li>有人看表或看钟。</li></ol></li></ol><ol><li><p>典型的好现象是:</p></li><li><ol><li>听众做笔记;</li><li>你发表某个观点时听众点头；</li><li>听众在看你的幻灯片或书面材料时与周围的人低声交谈,表现出饶有兴趣的样子；</li><li>听众很有兴趣地观看幻灯片或阅读你发放的书面材料。</li></ol></li></ol><ol><li>会议(包括学生会议)是观察不同陈述报告风格的极佳场所。</li></ol><h2 id="其他小窍门"><a href="#其他小窍门" class="headerlink" title="其他小窍门"></a>其他小窍门</h2><ol><li>某些类型的陈述报告，比如应聘陈述报告，是具有竟争性的。在这种情况下,值得推荐的做法是思考一下别人可能会做什么,然后想出一种与别人不同,比别人更好的做法。别人会把重点放在哪些话顾上﹖他们想不到,但你可以在陈述报告中使用的元素是什么?</li><li>如果陈述报告内容你丝毫不感兴趣,那么你可以利用这个时间留意陈述者使用了哪些内行人的招数,或者犯了什么错误,这样你就能够间接地改善自己的陈述报告风格</li><li>参加别人的陈述报告会,尤其是在新学期的头两周参加此类活动.旧是一个很好的机会,可以从中了解这类场合的礼节和惯例。</li></ol><h2 id="如果有必要的话可以回避问题"><a href="#如果有必要的话可以回避问题" class="headerlink" title="如果有必要的话可以回避问题"></a>如果有必要的话可以回避问题</h2><ol><li>你不需要反对批评意见。你可以说：“这是一个有意思的观点,据我所知,它在文献中提及得不多。”</li><li>如果听众的确指出了你研究中的一个缺陷,那么向他们表示感谢,回去以后检验他们的说法——他们可能是对的,如果是的话,你越早改正问题越好。</li><li>如果你礼貌地请他们展开他们的问题,你就能看出他们的建议是否是认真的，是否值得考虑。</li></ol><h2 id="关于陈述报告的检测清单"><a href="#关于陈述报告的检测清单" class="headerlink" title="关于陈述报告的检测清单"></a>关于陈述报告的检测清单</h2><ol><li>陈述报告应该多大程度地探讨细节,你确认了吗</li><li>陈述报告的听众已经了解了哪些信息,你确认了吗?</li><li>你演练了陈述报告吗?</li><li>听众能读懂你的幻灯片吗?</li><li>你知道该如何操作陈述报告时需要用到的视听设备吗?</li><li>你事先检查过陈述报告场地吗?</li><li>你准备了陈述报告进度表吗?</li><li>如果设备故障,你有后备方案吗?</li></ol><h2 id="在会议上陈述报告论文的一些小窍门"><a href="#在会议上陈述报告论文的一些小窍门" class="headerlink" title="在会议上陈述报告论文的一些小窍门"></a>在会议上陈述报告论文的一些小窍门</h2><h2 id="预防紧张情绪"><a href="#预防紧张情绪" class="headerlink" title="预防紧张情绪"></a>预防紧张情绪</h2><ol><li>做足准备：做好准备工作并不会使你不紧张,但它能帮助你克服紧张情绪。如果你事先进行了演练,获得了不错的反响,并通过演练改进了可能出现问题的环节,那么你在实际做陈述报告时就会自信很多。</li><li>提示卡片：如果你必须记住某些关键信息(比如重要文献及其作者的名字),那么你可以把它们写在一张提示卡片上(也就是关键事实和论点的清单),这样你陈述报告时如果需要就可以参考卡片上的信息</li><li>预料到你的恐惧：把自己最担心的事都想一遍。可能会发生的最糟糕的情况是什么?你将怎样去面对这种情况?问问别人是怎样处理它们的。</li><li>找到一张友好的脸:如果你把陈述报告看做是与某个人——尤其是对你所说的感兴趣的人的一次谈话,那么你的陈述报告就比较有可能显得温暖亲切,类似于谈话。</li><li>停下来呼吸:在陈述报告过程中做短暂的停顿用来呼吸，你会感觉停顿时间很长,但观众却根本觉察不到。让自己停下来片刻是有必要的,它使你能镇定下来，组织思路,均匀呼吸。你的陈述报告会因此而更加出色。</li><li>自我介绍时告诉观众你是个学生:如果你真的很害怕,在自我介绍时可以告诉观众你还是个学生,他们可能会因此而对你比较宽容。比如,你可以感谢你的导师,这样观众就知道你还是个学生了,当然，如果合适的话,你也应该顺带感谢你研究的出资方。</li><li>研究过去陈述报告会上的听众提问:很多人会对不同的讲话人提同样的问题(比如研究方法、统计学、理论的应用,以及与某个理论的联系)。所以如果有机会的话,你应该了解一下你的听众是哪些人,事先考虑他们会针对你的陈述报告提出哪些与过去类似的问题,并考虑该怎样回答这些问题。</li><li>穿让你感到舒适的衣服:你在做陈述报告时要想的事情够多了,容不得因为鞋子夹脚或担心衣服不合适而分心。所以一定要穿上让自己感觉良好的衣服。</li></ol><h2 id="对付提问"><a href="#对付提问" class="headerlink" title="对付提问"></a>对付提问</h2><ol><li>练习:如果你已经在小型研讨会上陈述报告过你的论文,那么你一定已经遇到过一些提问,并已经有了“身临现场”的体验,这将对你很有帮助。</li><li>问题的规律性:在研讨会上或在别人做陈述报告时,留意听众一般会提出哪些类型的问题,试着从中找出规律。这会给你提供一个基础,使你能够较有效地预测别人在听了你的陈述报告后将提什么问题,你也就能够事先准备问题的答案了。</li><li>提问涉及你不熟悉的文献时如何防卫:如果你没有听说过提问者要求你做出评论的某部文献,可以把问题重新抛给提问者:“我不熟悉那篇论文,它的作者提出了什么观点?”或者你也可以通过一个问题将它与你熟悉的文献联系起来:“我不熟悉那篇论文，它是属于人工智能还是属于实证研究?”不要不懂装懂。记得在会后向提问者询问那部文献的出处。</li><li>对于技术性过高的问题可以在会后个别回答。(这个问题很有趣,但我无法给你一个简短的回答。我们能在休息时间讨论吗?)</li><li>没听明白的问题:(如果你不确定自己听懂了问题,那么可以把问题按你的理解重新组织一下:“如果我听得没错的话,你是在问我如果……”然后给出你的回答。如果你根本没有听懂问题,可以要求提问者再问一遍——他或她在第二遍时可能会把问题说得简单–些。</li><li>很长的问题;准备好纸和笔。如果有观众提了一个由多部分组成的问题,或者借提问的机会长篇大论,那么做点笔记会帮助你记住在回答时要说的话。</li><li>怪异的问题:与对待技术性过高的问题一样处理这类问题,可以说些类似这样的话:“这点很有趣,我之前没有太多思考过这个问题。我回去后会对它做一些考证和研究。”千万不要提议在休息时间与提问者讨论。</li><li>让别人帮你记下听众的问题,最好连同提问者的姓名一起记下来</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议11：写作过程</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE11%EF%BC%9A%E5%86%99%E4%BD%9C%E8%BF%87%E7%A8%8B/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE11%EF%BC%9A%E5%86%99%E4%BD%9C%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E5%86%99%E4%BD%9C%E8%AF%80%E7%AA%8D">写作诀窍</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E5%88%A0%E9%99%A4%E7%A6%BB%E9%A2%98%E7%9A%84%E5%86%85%E5%AE%B9">删除离题的内容</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E7%9D%80%E6%89%8B%E5%86%99%E4%BD%9C">着手写作</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E4%BD%BF%E8%87%AA%E5%B7%B1%E6%84%9F%E5%88%B0%E6%83%8A%E8%AE%B6">使自己感到惊讶</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%95%B4%E7%90%86%E6%80%9D%E8%B7%AF">整理思路</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%A3%80%E7%B4%A2%E5%8D%A1%E7%89%87">检索卡片</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE">思维导图</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%89%BE%E5%88%B0%E9%87%8D%E7%82%B9">找到重点</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%89%BE%E5%88%B0%E4%B8%80%E7%A7%8D%E6%A8%A1%E5%9E%8B">找到一种模型</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%A0%87%E9%A2%98">标题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E5%86%99%E4%BD%9C%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%BB%8A%E8%84%9A%E7%9F%B3">写作过程中的绊脚石</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E6%99%AE%E9%81%8D%E6%80%A7%E5%BB%BA%E8%AE%AE">普遍性建议</a></p><p><a href="https://zhuanlan.zhihu.com/p/401727273/edit#%E7%94%B1%E5%AD%A6%E7%94%9F%E8%87%AA%E5%B7%B1%E9%80%A0%E6%88%90%E7%9A%84%E5%86%99%E4%BD%9C%E9%9A%9C%E7%A2%8D">由学生自己造成的写作障碍</a></p><hr><h2 id="写作诀窍"><a href="#写作诀窍" class="headerlink" title="写作诀窍"></a>写作诀窍</h2><h2 id="删除离题的内容"><a href="#删除离题的内容" class="headerlink" title="删除离题的内容"></a>删除离题的内容</h2><ol><li>把多余的文字片断和其他的想法都放到一个安全的地方(如鞋盒&gt;保存起来,将来可能用得上。但在你写论文时不要被它们分散注意力。</li><li>在完整的第一稿出来之前不要修改</li></ol><h2 id="着手写作"><a href="#着手写作" class="headerlink" title="着手写作"></a>着手写作</h2><ol><li>尽可能早地写出初稿,这样你的任务就变成了重写或在原来基础上做改动。</li><li>使从头脑到手指传输词句的通道流畅起来,然后再开始写文章。</li><li>跳过引言部分,从最熟悉的或最易于表达的材料开始写。</li></ol><h2 id="使自己感到惊讶"><a href="#使自己感到惊讶" class="headerlink" title="使自己感到惊讶"></a>使自己感到惊讶</h2><ol><li>有时候仅仅改变作品的外观，就可以让写作素材看上去“焕然一新”,或显露出某些与原先不同的地方。</li><li>概括出你论文的核心内容,然后用很简单的语言把它写出来。</li></ol><h2 id="整理思路"><a href="#整理思路" class="headerlink" title="整理思路"></a>整理思路</h2><h3 id="检索卡片"><a href="#检索卡片" class="headerlink" title="检索卡片"></a>检索卡片</h3><p>在每张检索卡片上写上一个关键点，然后把它们铺在地上，调整它们彼此的相对位置来说明各个关键点的类别以及它们之间的关系。</p><h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p>用思维导图引导自己产生想法，并整理出各个想法之间的关系。用一张尺寸足够大的纸或者电脑软件来画这张思维导图。</p><h2 id="找到重点"><a href="#找到重点" class="headerlink" title="找到重点"></a>找到重点</h2><h3 id="找到一种模型"><a href="#找到一种模型" class="headerlink" title="找到一种模型"></a>找到一种模型</h3><p>找到和你要写的论文功能相似(比如都是展示研究成果)的一篇论文或一个章节。分析它的优点何在,它的内容、结构、特点是什么。从中得出一个模板然后用你自己的材料来填充这个模板。</p><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p>在写文章之前先把标题列出来。这样做可以使你在脑子形成一个论文的总框架,使你从一开始就可以有条理地展开写作,而不被细节性问题纠结。</p><h2 id="写作过程中的绊脚石"><a href="#写作过程中的绊脚石" class="headerlink" title="写作过程中的绊脚石"></a>写作过程中的绊脚石</h2><ol><li>很多人害怕写作。教育工作者将其描述为“对失败的畏惧”(害怕写错或写得不够好,这种焦虑情绪也构成了写作进程中的障碍)以及“对成功的畏惧”(害怕成功带来的更高预期和压力,这种焦虑情绪也构成了写作进程中的障碍)。认识和面对你的畏惧情绪很重要。是的,它很可怕.但它并不是不可能的任务 最重要的是要认识到, 无论写什么都比什么都不写好。</li><li>另一条原则是简单原则——这条原则适用于几乎所有设计活动。能达到效果的最简单的语言(最简单的词汇、句子结构、修辞方式)往往是最好的</li></ol><h2 id="普遍性建议"><a href="#普遍性建议" class="headerlink" title="普遍性建议"></a>普遍性建议</h2><ol><li>写作是一项技能,和所有其他技能一样,写作也是熟能生巧的。</li><li>向自己保证,必须每天都练习写作,每周完成一篇两三页左右的文章。</li><li>将研究材料以书面形式带到导师辅导课上。</li><li>有人对你的文章提出批评时,花时间分析一下他们的批评: 为什么他们提出这样的评论或修改意见?</li><li>有人对你的文章进行文字编辑或其他改动时,花时间分析一下他们做的改动:为什么要这么改?这些修改对文章有什么好处?</li></ol><h2 id="由学生自己造成的写作障碍"><a href="#由学生自己造成的写作障碍" class="headerlink" title="由学生自己造成的写作障碍"></a>由学生自己造成的写作障碍</h2><ol><li><p>“足够好”的过程。(如果你不动笔写,不把自己写的东西拿给别人看,你永远都不会得到反馈意见,你永远都不会知道,你的思想表达得如何.你的课题是否有趣,你又在哪些方面启发了你的读者。</p></li><li><p>如果你写不出东西来,这很可能是因为你还没想清楚自己要写什么。</p></li><li><p>问题：</p></li><li><ol><li>我知道主要论点有哪些,但我不知道怎样把它们表达出来;</li><li>我不知道怎么把草稿变成成稿;</li><li>表达不够清楚﹔</li><li>无法找到一个清晰的结构;</li></ol></li></ol><ol><li><p>好的写作一般都是不断写,不断改出来的。</p></li><li><p>可以尝试以下组织想法的窍门:</p></li><li><ol><li>思维导图。你把你的想法都画在纸上以后,就可以试着整理出它们之间的先后顺序。</li><li>把想法写在检索卡片上(一张一个想法),然后把它们分组或按某种结构摆放。用不同的方式来分类和摆放卡片可以帮助你发现,哪些想法是不符合整体框架的,哪些想法是需要着重强调的。</li><li>写作纲要:有经验的写作者往往会建议，列出一个具体的写作纲要应该是第一件必须做的事。列出不同种类的纲要会对你有帮助,这些纲要包括:内容大纲,标题加上相应部分的概要(以体现出论文的“故事情节”),标题加上对相应部分“角色”的描述(这部分是什么、它有什么作用、它如何为整个论证服务)等。</li></ol></li></ol><ol><li>你想通过表格传达什么信息?文章对表格的描述应该总结表格的信息，并传达你的意图。</li><li>一定要留出比你估计的时间多一倍的时间</li></ol><p>Enable Ginger<em>Cannot connect to Ginger</em> Check your internet connection</p><p> or reload the browserDisable in this text fieldRephraseRephrase current sentence</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议10：写作风格</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE10%EF%BC%9A%E5%86%99%E4%BD%9C%E9%A3%8E%E6%A0%BC/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE10%EF%BC%9A%E5%86%99%E4%BD%9C%E9%A3%8E%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E4%B8%8D%E8%A6%81%E6%9A%B4%E9%9C%B2%E5%BC%B1%E7%82%B9">不要暴露弱点</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E7%AC%AC%E4%B8%80%E6%AD%A5%3A%E6%B2%A1%E6%9C%89%E5%87%86%E5%A4%87%E5%85%85%E5%88%86%E5%B0%B1%E5%BE%85%E5%9C%A8%E5%AE%B6%E9%87%8C%E5%88%AB%E5%87%BA%E9%97%A8">第一步:没有准备充分就待在家里别出门</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E7%AC%AC%E4%BA%8C%E6%AD%A5%3A%E5%87%86%E5%A4%87%E5%A5%BD%E4%BA%86%E4%BB%A5%E5%90%8E%E5%B0%B1%E8%A6%81%E4%BC%A0%E8%BE%BE%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%BF%A1%E6%81%AF">第二步:准备好了以后就要传达正确的信息</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E6%83%AF%E4%BE%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%3F">惯例有什么用?</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E4%BD%93%E7%8E%B0%E5%87%BA%E4%BD%A0%E6%9C%89%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%9C%E8%A5%BF%E8%A6%81%E8%AF%B4">体现出你有重要的东西要说</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E4%BA%86%E8%A7%A3%E4%BD%A0%E7%9A%84%E6%95%8C%E4%BA%BA">了解你的敌人</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E6%B8%B4%E6%9C%9B%E6%88%90%E5%8A%9F%E7%9A%84%E6%96%B0%E6%89%8B">渴望成功的新手</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E8%81%AA%E6%98%8E%E7%9A%84%E5%A4%96%E8%A1%8C">聪明的外行</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E4%B8%8D%E8%A6%81%E8%A1%A8%E7%8E%B0%E5%87%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E8%BD%AF%E5%BC%B1%E6%88%96%E8%BF%9F%E7%96%91">不要表现出自己的软弱或迟疑</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E4%B8%8D%E8%A6%81%E8%99%9A%E5%BC%A0%E5%A3%B0%E5%8A%BF">不要虚张声势</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E5%B0%86%E5%86%99%E4%BD%9C%E5%BD%93%E6%88%90%E4%B8%80%E7%A7%8D%E8%A1%A8%E7%8E%B0%E6%80%A7%E8%A1%8C%E4%B8%BA">将写作当成一种表现性行为</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E6%88%91%E6%98%AF%E4%B8%AA%E6%80%81%E5%BA%A6%E7%AB%AF%E6%AD%A3%E7%9A%84%E4%B8%93%E4%B8%9A%E4%BA%BA%E5%A3%AB">我是个态度端正的专业人士</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E6%88%91%E7%9F%A5%E9%81%93%E6%88%91%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88">我知道我在做什么</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E5%AD%A6%E6%9C%AF%E5%86%99%E4%BD%9C%E9%A3%8E%E6%A0%BC">学术写作风格</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E8%AF%AD%E8%A8%80">语言</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E5%90%88%E9%80%82%E7%9A%84%E8%AF%84%E8%AE%BA%E6%80%A7%E8%AF%AD%E8%A8%80%E7%9A%84%E7%89%B9%E7%82%B9">合适的评论性语言的特点</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726979/edit#%E8%AF%BB%E5%87%BA%E5%AD%97%E9%9D%A2%E8%83%8C%E5%90%8E%E7%9A%84%E6%84%8F%E6%80%9D">读出字面背后的意思</a></p><hr><p>在一开始就应该认识到一个重点是:我们这里讨论的写作,重点不在于传统的语法规范或流畅的表达,而在于语言的工具性。另外一点对母语非英语的学生很重要,那就是你不需要说一口流利的英语才能写好论文。优秀学术写作的关键在于结构和形式,而语法并非关键。</p><h2 id="不要暴露弱点"><a href="#不要暴露弱点" class="headerlink" title="不要暴露弱点"></a>不要暴露弱点</h2><h2 id="第一步-没有准备充分就待在家里别出门"><a href="#第一步-没有准备充分就待在家里别出门" class="headerlink" title="第一步:没有准备充分就待在家里别出门"></a>第一步:没有准备充分就待在家里别出门</h2><p>新手们常常会犯错误的两个方面是样本的大小以及数据的多少</p><h2 id="第二步-准备好了以后就要传达正确的信息"><a href="#第二步-准备好了以后就要传达正确的信息" class="headerlink" title="第二步:准备好了以后就要传达正确的信息"></a>第二步:准备好了以后就要传达正确的信息</h2><p>要显示出你是专业人土,那么使用领域内专业人土的语言和写作惯例</p><h3 id="惯例有什么用"><a href="#惯例有什么用" class="headerlink" title="惯例有什么用?"></a>惯例有什么用?</h3><p>每门学科都有它自己的写作惯例。有一些高度正式,有一些则比较含蓄。要发现这些惯例，你就应该以分析的眼光来阅读文献。这些文献中的惯例可以:</p><ol><li>体现学科的学术标准;</li><li>使读者能够读得懂论文;</li><li>使读者能认识、比较和复制/重复研究方法;</li><li>便于读者比较和综合——使论文的形式转换成便于与其他论文相联系的形式。</li></ol><h3 id="体现出你有重要的东西要说"><a href="#体现出你有重要的东西要说" class="headerlink" title="体现出你有重要的东西要说"></a>体现出你有重要的东西要说</h3><ol><li>如果你在一篇论文的开头提出了一个有趣的问题,并暗示了一个可能解决问题的有趣的办法,而不是告诉读者是你想到了一个有趣的问题,又是你看到了解决问题的可能性,那么你就上了轨道了</li><li>从别的学科领域引入可能解决问题的方法往往是一种有效策略</li><li>你所要引人的这个领域必须比你研究课题涉及的领域更加正式,具有更加完善的研究方法体系.</li><li>你应该充分掌握你所要引入的领域，否则你提出的解决办法就不会具有可信度，那个领域的行家会把你看做是一个典型的新手,没读多少文献就开始自作主张,犯了新手常犯的错误,只是你的错误还涉及别的领域而已</li><li>期刊编辑一定会确保你的论文由相关领域的专家来鉴定。</li></ol><h2 id="了解你的敌人"><a href="#了解你的敌人" class="headerlink" title="了解你的敌人"></a>了解你的敌人</h2><h3 id="渴望成功的新手"><a href="#渴望成功的新手" class="headerlink" title="渴望成功的新手"></a>渴望成功的新手</h3><p>你的论文需要发出强有力的信号,告诉读者你不属于那一类人</p><ol><li>可以与专业人士喝杯咖啡聊一聊,让他们给你讲讲新手的典型错误有哪些</li><li>还有就是要全面深入地阅读文献,历史久远的文献也要读。你可能不相信二十年前的文献对你也会有很大帮助</li></ol><h3 id="聪明的外行"><a href="#聪明的外行" class="headerlink" title="聪明的外行"></a>聪明的外行</h3><p>如果一个问题只需要一些基本常识就能解决,你就不应该把时间浪费在这个问题上。</p><h2 id="不要表现出自己的软弱或迟疑"><a href="#不要表现出自己的软弱或迟疑" class="headerlink" title="不要表现出自己的软弱或迟疑"></a>不要表现出自己的软弱或迟疑</h2><ol><li>永远都不要表现出自己的弱点,向读者道歉，请求读者的原谅。</li><li>“推托话”在学术写作中不应该出现,在学位论文中更不应该出现。</li><li>你在证明某件事的正确性或正当性的时候,也就同时提出了两个问题:这件事的正确性或正当性为什么需要证明,以及你的证明是否充分有道理。</li><li>如果你没有,而涉及的问题又很重要,你就应该搞清楚自己的猜测是否正确。</li><li>你要么明确地指出你的某个想法只是一种猜测(也就是说它与你的论证只存在着表面的而非确凿的关联),要么就把猜测留到结尾的讨论部分,为未来的研究作铺垫。</li></ol><h2 id="不要虚张声势"><a href="#不要虚张声势" class="headerlink" title="不要虚张声势"></a>不要虚张声势</h2><p>如果你对某个问题不了解，那就花时间和精力去学习、直到你理解为止。如果你使用了一个自己都不知道是什么意思的技术术语，具有批判性的读者眼就看得出来。</p><h2 id="将写作当成一种表现性行为"><a href="#将写作当成一种表现性行为" class="headerlink" title="将写作当成一种表现性行为"></a>将写作当成一种表现性行为</h2><h2 id="我是个态度端正的专业人士"><a href="#我是个态度端正的专业人士" class="headerlink" title="我是个态度端正的专业人士"></a>我是个态度端正的专业人士</h2><ol><li>我注重细节,比如拼写正确和引用在全文中的分布合理;我做了所有该做的工作,并将其体现在成稿中；</li><li>我细致专业地完成了研究的每个环节,并将其体现在了成稿中；</li><li>我把我的研究清楚、完整地写在了论文里,论文符合我所在领域的写作惯例。</li></ol><h2 id="我知道我在做什么"><a href="#我知道我在做什么" class="headerlink" title="我知道我在做什么"></a>我知道我在做什么</h2><ol><li>我知道所有关键文献,阅读了它们,并正确地引用了它们;</li><li>我也知道其他相关文献﹐并正确地引用了它们;</li><li>我知道并了解我所在领域内的所有技术性概念,并在我的成稿中谨慎使用了所有相关的技术性概念。</li></ol><h2 id="学术写作风格"><a href="#学术写作风格" class="headerlink" title="学术写作风格"></a>学术写作风格</h2><ol><li>学术研究论文的风格与教科书的风格是很不一样的,而这两种风格又与商业报告的风格大不一样。</li><li>不要受其他领域的写作指南的影响</li></ol><h3 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h3><p>一篇好的论文会恰当地使用技术术语</p><h3 id="合适的评论性语言的特点"><a href="#合适的评论性语言的特点" class="headerlink" title="合适的评论性语言的特点"></a>合适的评论性语言的特点</h3><p>合适的评论性语言应该是:</p><ol><li><p>清楚的</p></li><li><ol><li>清晰,有条理地论证﹔不要离题;</li><li>删除不必要的内容;</li><li>语言应该是“轻度正式”——不要太难懂,不要太正式,不要有太多术语,而应该是三思后写出来的能让人读懂的文字;关键词的意思要前后一致;</li><li>例子要能够说明问题。</li></ol></li></ol><ol><li><p>诚实的</p></li><li><ol><li>从数据推导出结论的过程要严密；</li><li>要考虑到其他说明;</li><li>要承认局限性。</li></ol></li></ol><ol><li><p>中立的</p></li><li><ol><li>问题的提出形式要中立;</li><li>在研究和成稿时都要主动避免偏见;</li><li>避免使用带感情色彩的语言。</li></ol></li></ol><ol><li><p>权威的</p></li><li><ol><li>熟练使用文献;</li><li>没有含糊不清的语句;</li><li>优先获得可靠性;</li><li>内容涵盖的范围合理;</li><li>准确恰当地使用术语</li></ol></li></ol><ol><li><p>充分论证的</p></li><li><ol><li>决定或假设都必须是有理由的,并对其做出解释；</li><li>不存在任何未经论证的断言;</li><li>将数据与结果、讨论和结论区分开来。</li></ol></li></ol><ol><li><p>批判性的</p></li><li><ol><li>将论文与现有文献的联系体现出来;</li><li>能够将论文中涉及的各种学术思想组织在一个框架之下；</li><li>指出现有知识体系中的漏洞和缺陷;</li><li>在报告的同时加入智力内容。</li></ol></li></ol><ol><li><p>还应强调什么?</p></li><li><ol><li>指出结论蕴含的意义;</li><li>指出重要性和意义﹔</li><li>指出未来的研究方向和计划。</li></ol></li></ol><h2 id="读出字面背后的意思"><a href="#读出字面背后的意思" class="headerlink" title="读出字面背后的意思"></a>读出字面背后的意思</h2><p>Enable Ginger<em>Cannot connect to Ginger</em> Check your internet connection</p><p> or reload the browserDisable in this text fieldRephraseRephrase current sentence</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议9：写作结构</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE9%EF%BC%9A%E5%86%99%E4%BD%9C%E7%BB%93%E6%9E%84/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE9%EF%BC%9A%E5%86%99%E4%BD%9C%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%A5%BD%E7%9A%84%E5%BB%BA%E8%AE%AE">好的建议</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E6%A0%87%E5%87%86%E5%BB%BA%E8%AE%AE">标准建议</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%E9%94%99%E8%AF%AF%2C%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%AE%83%E4%BB%AC">一些经典错误,以及如何避免它们</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84">论文结构</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%AD%A6%E4%BD%8D%E8%AE%BA%E6%96%87%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E3%80%81%E6%A0%B8%E5%AF%B9%E6%B8%85%E5%8D%95%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E4%BD%A0%E5%BA%94%E8%AF%A5%E9%97%AE%E8%87%AA%E5%B7%B1%E7%9A%84%E9%97%AE%E9%A2%98">学位论文的常见问题、核对清单以及一些你应该问自己的问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E9%95%BF%E5%BA%A6">长度</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E7%BB%93%E6%9E%84">结构</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E6%A0%87%E9%A2%98">标题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E7%A0%94%E7%A9%B6%E9%97%AE%E9%A2%98">研究问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E7%90%86%E8%AE%BA%E5%92%8C%E8%AF%81%E6%8D%AE">理论和证据</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%BC%95%E8%A8%80">引言</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E6%9C%80%E5%90%8E%E4%B8%80%E7%AB%A0%EF%BC%88%E6%88%96%E5%87%A0%E7%AB%A0%EF%BC%89">最后一章（或几章）</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E8%A1%A8%E6%A0%BC">表格</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%9B%BE%E5%83%8F">图像</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%85%B3%E4%BA%8E%E5%86%99%E4%BD%9C%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%82%E7%82%B9">关于写作的一些观点</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E6%80%8E%E6%A0%B7%E6%89%8D%E7%AE%97%E5%86%99%E2%80%9C%E5%85%85%E5%88%86%E2%80%9D%E4%BA%86%3F">怎样才算写“充分”了?</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726539/edit#%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">其他注意事项</a></p><hr><p>描述实证性工作的论文的一种有用结构是在引言部分提出一系列明确的问题,在结果部分给出相应的表格和数据,在讨论部分展开针对问题和数据的阐释和分析。这种结构能使读者(以及你本人)对你论文的逻辑和目的有一个清晰的认识。</p><h2 id="好的建议"><a href="#好的建议" class="headerlink" title="好的建议"></a>好的建议</h2><ol><li>作为学生,你可以在论文中加入一些显示你良好判断力的语句来体现你已经达到了前两条标准(“原创性”和“对现有知识的贡献”)。</li><li>如果你写了类似这样的句子“这些发现对领域内关于这个课题的研究具有重大启示意义,这个课题的重要性之前一直被相对低估”,那么第二条标准“对现有知识的贡献”也就显而易见了</li><li>细节里、如果在论文中提到自己发表过的期刊文章，以此暗示你预想的效果,但如果在论文答辩时以此来作为一个论据，那你可能就有麻烦了——评审人最多把它视为一个加分项,也可能把它理解为你理屈词穷后的最后一招。</li></ol><h2 id="标准建议"><a href="#标准建议" class="headerlink" title="标准建议"></a>标准建议</h2><ol><li>你可以写下每一章的标题以及每一小节的小标题。你也可以就各个小节的 写作时间安排制定一个计划，要注意必须留出足够的时间来制作表格、参考书自和附录。这方面的所有事项在专门讲博士学位标准程序的书上都能找到。</li><li>有些部分,比如参考书目,是你有意识地慢慢积累起来的。而其他部分,比如实验科学博士论文的方法章节,可以在做研究的同时成稿并且一旦成稿，就不会再做太大的改动。</li><li>改动最大的是每一章的引言和讨论部分(包括第一章的文献综述部分)。通过过去几年的研究,你一定已经意识到了,对你的研究课题真正具有重要意义的问题并不是你在着手研究时所设想的那些问题</li></ol><h2 id="一些经典错误-以及如何避免它们"><a href="#一些经典错误-以及如何避免它们" class="headerlink" title="一些经典错误,以及如何避免它们"></a>一些经典错误,以及如何避免它们</h2><ol><li>不要拖到最后一刻。事先制订计划,留出充裕的时间对付意外情况,包括装订文稿的时间都应该事先安排好。</li><li>不要含糊其辞。</li><li>不要用模枝两可的语句来逃避问题——评审人能一眼就看出你是在逃避问题,并会在答辩时无情地质问你。</li><li>不要将问题简单化。你的读者也是专业的研究人员，把问题简单化会让他们认为你没有领会你研究领域的复杂性。</li><li>不要使用连你自己都不确定是什么意思的大词( big words)。错误使用大词会让你看起来像个白痴。评审人知道的大词、生僻词一定比你多,他们不会对你炫耀词汇量的行为产生什么好感。</li><li>不要依据别的学科领域或别的国家的学术写作惯例来写你的博士论文。如果你不清楚自己的学科有哪些写作惯例,应该先弄清楚。如果你反对自己学科的写作惯例,你也应该在拿到博士学位后再表示出反对意见,博士论文不是你说“不”的地方。</li><li>最后，也是最重要的一点,不要忘记三条黄金法则:不要说谎;不要故作幽默;不要惊慌失措而在压力下吐露真相。</li></ol><h2 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h2><ol><li><p>以实验为基础的研究论文的一般结构应该是：</p></li><li><ol><li>引言;</li><li>文献综述；</li><li>方法论/研究方法;</li><li>实验描述/研究方法的实施;结果/讨论;</li><li>结论以及未来研究计划,可以在其中再次重复方法论/研究/结果的顺序,以方便读者做进一步研究或重复研究过程。</li></ol></li></ol><ol><li><p>较常见的结构问题有:</p></li><li><ol><li>没有为结论的得出做铺垫；</li><li>数据和讨论的内容与方法论混在一起;事实依据太分散;</li><li>解释和讨论没有与实验结果分开;涉及两个彼此不相关的主题；</li><li>没有形成一个完整的体系(也就是说,没有回到理论上去,没有体现出问题是怎样被解决的,没有将研究结果与研究目的联系起来)。</li></ol></li></ol><h2 id="学位论文的常见问题、核对清单以及一些你应该问自己的问题"><a href="#学位论文的常见问题、核对清单以及一些你应该问自己的问题" class="headerlink" title="学位论文的常见问题、核对清单以及一些你应该问自己的问题"></a>学位论文的常见问题、核对清单以及一些你应该问自己的问题</h2><h2 id="长度"><a href="#长度" class="headerlink" title="长度"></a>长度</h2><p>有时你可以通过附录来添加内容,而不受上限的约束。但你始终要记住,你写多少评审人就要看多少。章节的数目应该由论文的结构来决定(一般一章内容对应一个独立的结构板块),论文的长度应该能学术性地概括你的研究工作,而无需比这更长。</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><ol><li>论证清晰有力吗?</li><li>论文中包含的所有内容都是论证必需的吗?</li><li>论证必需的所有内容都写在论文里了吗?</li><li>论文读起来一气呵成,还是像在念购物清单?</li></ol><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p>检查目录中的标题时﹐你可以问自己以下问题:</p><ol><li>研究问题是在哪儿被提出的?你能看出研究问题来吗?</li><li>研究方法是在哪儿得到描述的?你能看出研究的实际操作方法吗?</li><li>证据是在哪儿给出的?你能看出是什么类型的证据吗?论文的切入角度或观点是什么?</li><li>论文给出了新的模型或理论吗?研究的重要性体现在哪里?</li><li>结论有哪些?</li></ol><h2 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h2><p>论文作者很容易忘了把研究问题明确地提出来,因为作者本人对这些问题实在太熟悉了，他们无法想象居然有人会不了解这些问题。你应该问问自己:</p><ol><li>具体的研究问题是在引言章节的哪个部分被提出的?(也就是说,读者要等多久才能被告知论文讨论的焦点在哪里?)论文的主题是用一句话还是一段话来陈述的?</li><li>研究问题的表述清楚、简洁吗?</li><li>研究问题是通过哪种形式来描述的,是把它看做目标(aims)、问题(questions)、目的(goal)、需要解决的问题、提出的挑战或是其他?</li></ol><h2 id="理论和证据"><a href="#理论和证据" class="headerlink" title="理论和证据"></a>理论和证据</h2><p>理论框架为你的研究提供了逻辑依据,而只有有了证据,你才能称自己的研究是对现有知识的原创性贡献。你可以问自己以下问题:</p><ol><li>理论是如何在论文中给出的?理论是如何被运用到论证中去的?</li><li>论文是否明确表明了研究与哪些理论相关?研究设计有坚实的理论根基吗?</li><li>论文明确体现了研究对相关领域理论的贡献吗?理论和证据各占多大比例?</li><li>证据的给出符合客观性原则吗?有对前提的陈述吗?</li><li>读者能够根据你对研究方法的描述重复研究过程吗?对数据的阐释( interpretation)是独特的吗?</li><li>证据是从数据中推导出来的吗?</li><li>结论是依据证据得出的吗?</li></ol><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在引言部分,你应该(a)留给读者一个良好的第一印象,并且(b)让读者了解你花数年的时间研究某个课题是有重要原因的，引言的组成部分一般包括:</p><ol><li>陈述研究问题﹔</li><li>概括研究问题的逻辑依据</li><li>解释语言和术语(如果有必要的话);目标和目的;</li><li>概述研究对相关领域的贡献﹔指出研究方法﹔</li><li>指出论文其他部分的总体思路:论证的概述</li></ol><h2 id="最后一章（或几章）"><a href="#最后一章（或几章）" class="headerlink" title="最后一章（或几章）"></a>最后一章（或几章）</h2><p>组成部分包括：</p><ol><li>结果总结(可以具体地对照引言中提出的研究目标来写)讨论结果的普遍性;</li><li>讨论局限性(要用肯定的语气);</li><li>陈述对现有知识的贡献;</li><li>未来的研究计划(陈述要有力、肯定);</li><li>推断(要注意分寸适度)。</li></ol><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><p>表格用途：</p><ol><li>总结(比如对实验结果、数据分析和文献的总结);</li><li>比较(比如对多项研究的研究方法和结果的比较);</li><li>提供背景材料以及帮助导航(比如帮助读者明确论证思路和论文线索);</li><li>建立类别,并规定各个类别涵盖的项;</li><li>提供框架(比如为各个学术概念及其相互关系,技术及其应用提供框架)。</li></ol><p>表格不能单独存在——它们必须在论文中得到描述,并与论文相关。还要注意按照规定的格式给表格标号,标号格式要保持一致。</p><h2 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h2><p>一般图像是用来：</p><ol><li>强调重点；</li><li>展示(比如展示论文中提到的某个事物的图像)；</li><li>提供另一种叙述方式(区别于用文字或表格叙述);</li><li>总结；</li><li>对比两个事物；</li><li>澄清；</li><li>作为掩盖写作水平差的把戏,但一般这类把戏都不会得逞;</li><li>提供一个概念性的地图(通过图像来梳理各种概念、论证和过程);</li></ol><h2 id="关于写作的一些观点"><a href="#关于写作的一些观点" class="headerlink" title="关于写作的一些观点"></a>关于写作的一些观点</h2><ol><li>写作是困难的、花费时间的。做个简单的算术:你在五分钟内能写几个有用的句子?然后再根据你的回答估算一下写一篇文章,一个章节要用的时间,这样你写论文的日程安排就会现实许多了。</li><li>写作就是要把你的思想表述明白(参看我们关于“叙述线索”的讨论)。</li><li>写作是有硬性要求的:内容(写的东西要有意义);顺序(说理摆事实要有条理);完整性(每个部分都要彼此衔接﹐不能留出空白)。</li><li>材料不可靠(不严密的思考、不完整的理论或不可靠的结果)会使写作难以进行。</li><li>在写作时遇到困难往往体现的是你的研究存在问题,而不是你的写作态度存在问题——如果你在写作过程中遇到困难,应该细致地考察你的研究目标、步骤,以及你在写作时使用的素材。</li><li>在动笔前组织好你的想法、概念和素材。</li><li>表达要准确。</li></ol><h2 id="怎样才算写“充分”了"><a href="#怎样才算写“充分”了" class="headerlink" title="怎样才算写“充分”了?"></a>怎样才算写“充分”了?</h2><ol><li>一个较为完整的、具有分析性的文献评论。</li><li>体现出研究目标已经完成,也就是要给出新的认识,并用事实根据来支持这种认识。</li><li>指出研究发现的重要性及其对知识的贡献。对研究发现的可推广性和局限性都要有认识。</li></ol><h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><ol><li>如果你做的某项研究证明你最初的直觉预测是完全错误的,那么在这种情况下,你应该将这项研究写到论文里去。</li><li>如果错误不是很严重，而你又快来不及完成论文了,你的导师可能会给你一些有用的建议，使你能够争取到更多的时间,使交稿期限不再是一个问题。另一个解决办法是将这个错误的数据分析从你的成稿中彻底删除,如果这个分析只占成稿的很小一部分的话。</li></ol><p>在成稿的过程中,却发现有人发表过和我的论文几乎一模一样的东西。应该可以重新组织你的文章,加入对那个人论文的讨论,并在同时体现出自己的研究与那个人的研究是有区别的。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议8：写作</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE8%EF%BC%9A%E5%86%99%E4%BD%9C/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE8%EF%BC%9A%E5%86%99%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%AD%A6%E6%9C%AF%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87">学术期刊论文</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%9C%A8%E5%93%AA%E9%87%8C%E5%8F%91%E8%A1%A8">在哪里发表</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%85%B3%E4%BA%8E%E9%80%92%E4%BA%A4%E6%9D%90%E6%96%99%E7%9A%84%E8%A6%81%E6%B1%82">关于递交材料的要求</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%8E%A5%E4%B8%8B%E5%8E%BB%E6%80%8E%E4%B9%88%E5%81%9A">接下去怎么做</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%8F%91%E8%A1%A8%E7%9A%84%E8%BF%87%E7%A8%8B">发表的过程</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%8E%A5%E5%8F%97">接受</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%96%87%E5%AD%97%E7%BC%96%E8%BE%91">文字编辑</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%9B%BE%E8%A1%A8">图表</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%A0%A1%E6%A0%B7">校样</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%85%B6%E4%BB%96%E7%9B%B8%E5%85%B3%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">其他相关注意事项</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%AD%A6%E4%BD%8D%E8%AE%BA%E6%96%87%E7%9A%84%E5%8F%91%E8%A1%A8">学位论文的发表</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D">开始之前</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E8%91%97%E4%BD%9C%E6%9D%83">著作权</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%8A%95%E5%AF%84%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9">投寄以及修改</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%8F%91%E8%A1%A8%E5%90%8E">发表后</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E8%AE%BA%E6%96%87%E6%A3%80%E6%9F%A5%E4%B8%80%E8%A7%88%E8%A1%A8">论文检查一览表</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%86%85%E5%AE%B9">内容</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%B0%86%E4%BD%A0%E7%9A%84%E8%AE%BA%E6%96%87%E6%94%BE%E5%88%B0%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E7%9A%84%E5%A4%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%8E%BB%E8%80%83%E5%AF%9F">将你的论文放到学术研究的大环境中去考察</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%A0%B9%E6%8D%AE">根据</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E7%BB%99%E4%BA%88%E5%88%AB%E4%BA%BA%E5%BA%94%E5%BE%97%E7%9A%84%E8%82%AF%E5%AE%9A">给予别人应得的肯定</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E6%96%87%E7%8C%AE%E4%BD%BF%E7%94%A8">文献使用</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E5%8F%91%E8%A1%A8%E5%9C%A8%E5%93%AA%E9%87%8C%3F">发表在哪里?</a></p><p><a href="https://zhuanlan.zhihu.com/p/401726245/edit#%E8%AE%BA%E6%96%87%E5%A4%96%E8%A7%82">论文外观</a></p><hr><ol><li>如果你是一个经验丰富的学者,有人试图说服你收某个你从未听说过的人做你的博士生,那么你想要了解 的第一件事就是这个人的写作能力如何。</li><li>写作能力指的是:有能力用质量好的、学术性的语言写作,最好可以根据不同情况用不同风格写作,从学术期刊文章到为争取可能的资助人而写的公开信都应该能够应对。</li><li>值得指出的一点是:质量好的学术性英语和正式的符合语法的英语不是一回事——我们遇到过很多学生,他们能写质量非常高的学术性英语文章,但他们的母语并不是英语，他们的英语语法在某些方面还存在着错误。</li></ol><h2 id="学术期刊论文"><a href="#学术期刊论文" class="headerlink" title="学术期刊论文"></a>学术期刊论文</h2><p>在写期刊论文之前首先要与导师沟通,确认在你的特定处境下进行期刊论文写作是否是明智之举。如果导师持否定意见,并且给出充分理由,那么你就应该听取导师的意见;如果他们持赞同意见并祝你好运,那你就可以计划下一步该怎么做了。</p><h2 id="在哪里发表"><a href="#在哪里发表" class="headerlink" title="在哪里发表"></a>在哪里发表</h2><ol><li>要考虑的第一个问题就是在哪里发表、这涉及以下几方面的因素:期刊的声誉、读者群.你选的课题是否适合该期刊的侧重点、该期刊接受投稿的比例是多少。一个基本战略是挑选最有声誉的、同时又比较有可能发表你的文章的期刊。这就又涉及如何预测可能性的问题。与知情的内行坐下来喝一杯咖啡是一个很好的策略。</li><li>第一项功课就是阅读你所选的期刊为其投稿人写的投稿须知。投稿须知一般会印在期刊上,发布在期刊的网站上,或者从期刊的编辑那里得到。从投稿须知中,你能了解到文章的字数限制、图表的制作流程,以及你需要寄出的论文复印件的份数。</li></ol><h2 id="关于递交材料的要求"><a href="#关于递交材料的要求" class="headerlink" title="关于递交材料的要求"></a>关于递交材料的要求</h2><ol><li>你的递交材料应该包括一封附信、若干数目的文章原件、一份电子文档(如果要求的话),以及说明材料中规定的任何其他材料。</li><li>附信应该简洁、有礼貌。编辑应该可以从中了解到,负责与他联系的作者是谁(如果文章由多人合著),以及这个作 者的联系方式。</li><li>文章本身应该符合投稿须知的要求。接下来几段文章将描述投稿须知中的一些规定,并对它们的重要性进行解释。</li><li>你的文章通过了这第一道检验之后,编辑会将原件寄给审阅人, 以征求他们的意见。这方面的流程各个期刊有所不同。大多数会寄去纸样,因为审阅人往往喜欢在纸上作注,而 电子文档意味着他们还 要将论文打印出来,而且论文的格式和字体还可能不符合他们的要求</li><li>然而大多数的稿件都需要进行较多的问题编辑,其中的错误往往出现在对参考文献的引用上(比如引用的文献在论文中的出版年份与在参考书目中标注的出版年份不一致)。因为这个原因,文稿必须是两倍行距的,以便文字编辑以及审阅人作注和修改。如果你寄出一份单倍行距的文稿,那就说明你是个业余人士,根本不了解情况。</li><li>机构投寄了的、如果你同时向两家或更多的期刊寄出同一篇文章，并被发现(可能性很大,因为一个领域内的审阅人数量有限),那么你就会被这些期刊列入黑名单(也就是永远不能在这些期刊发表文章)</li></ol><h2 id="接下去怎么做"><a href="#接下去怎么做" class="headerlink" title="接下去怎么做"></a>接下去怎么做</h2><ol><li>你的论文、你的目标应该是使被拒绝的论文数量达到一个合理比例。如果你的每篇论文都被接受,这说明你选择的期刊水平不够高，你应该去找声誉更好的期刊。</li><li>修改论文的明智做法就是要发挥主观能动性。列出要求修改的环节,系统地进行修改,并在寄第二稿时附上给编辑的信,在其中详尽清晰地告诉编辑你在哪些地方做了怎样的改动。这么做会大大便捷编辑的工作,他们可能就会因此对你产生足够的信任,直接接受你的文章,而不再将第二稿交给审阅人鉴定。</li></ol><h2 id="发表的过程"><a href="#发表的过程" class="headerlink" title="发表的过程"></a>发表的过程</h2><h2 id="接受"><a href="#接受" class="headerlink" title="接受"></a>接受</h2><p>如果你进入到这个阶段,你会收到一份编辑的来信或电子邮件,通知你论文将被发表在某份期刊上。编辑可能会要求你再寄去一些改动后文稿的原件和/或电子文档。</p><h2 id="文字编辑"><a href="#文字编辑" class="headerlink" title="文字编辑"></a>文字编辑</h2><ol><li>文字编辑在发现一些有出人或遗漏的参考文献信息、正文和表格中数据不统一等情况时,将寄给你一份需要你澄清的问题清单。</li><li>水平高的、经验丰富的研究员通常会在收到文字编辑来信的当天就对其所有疑问给予完整的答复，这种做法很受文字编辑的赞赏。新手往往做不到这一点，他们在核对原文的痛苦过程中将会更加深刻地领悟:为什么导师如此强调参考文献的信息必须记录准确</li></ol><h2 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h2><p>很多期刊都会要求将图表单独列出,附在正文后面,并在正文中需要插入表格的位置上标出记号,注明表格序号(通常是空一行,在需要加人表格的确切位置上画一条线,注上“表格1在此”)。</p><h2 id="校样"><a href="#校样" class="headerlink" title="校样"></a>校样</h2><p>作者在收到校样后必须在一个规定的、极短的时间内完成核对,并将核对后的校样再寄到编辑部,从收到校样到再次寄出校样通常仅24小时到三天的时间。</p><h2 id="其他相关注意事项"><a href="#其他相关注意事项" class="headerlink" title="其他相关注意事项"></a>其他相关注意事项</h2><ol><li><p>首先,你寄出的复印稿件必须是你寄出的电子文档的无差别打印版本。如果编辑组发现你做了改动,电子文档和原件之间存在差异,他们会极其气恼。</p></li><li><p>第二点,在校对时不要对文章内容进行改动——只有印刷错误才需要更正。如果你擅自添了几个词,这在排版上会引起连锁反应,影响到后面几页的版面效果,还会大大提高印刷成本。</p></li><li><p>在这里还需要指出的一点是:文字编辑们其实是比较人性的、所以如果你有什么不明白的地方,可以打电话或发电子邮件给他们与他们展开建设性的讨论。</p></li><li><p>大多数的系都会要求得到你文章的一份拷贝存档，在大学研究评估(RAE)或类似评估活动中使用。抽印本比你文章的复印件对系里更加有用,因为系里的年报需要包含发表你文章的期刊的国际标准书号以及页码等细节信息，抽印本或期刊提供这些信息，而你自己的复印稿则不包含这些细节信息。</p></li><li><p>最后一点大多数期刊会要求你签署一份放弃版权的协议出版你文章的条件。英国作家学会目前正在这个问题上进行平静、礼貌而有效的抗议，有些期刊已经因此开始转变它们的政策。与此同时,你无需对版权放弃协议感到恐惧。如果你有疑惑,可以联系作家学会或咨询有经验的同事。</p></li><li><p>关于期刊文章要牢记的主要几点是:</p></li><li><ol><li>大多数文章都会被拒;</li><li>成功的学者已经把自己的脸皮磨得很厚了.失败的学者却做不到这一点；</li><li>评审人也是人,他们说话不客气或彼此意见不一致时,不要太介意；</li><li>成功的学者之所以成功是因为他们在经历中学习成长;即便是成功的学者也有迈出第一步的时候。</li></ol></li></ol><h2 id="学位论文的发表"><a href="#学位论文的发表" class="headerlink" title="学位论文的发表"></a>学位论文的发表</h2><p>这些指导原则是我们所贯彻的。它们不是完美无缺的,也不是绝对的唯一的真理。但你会发现把它们作为出发点是有用的。这些指导原则对于本科生,科学硕士生及博士生都适用。</p><h2 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h2><ol><li>如果你和你的导师认为论文有发表的可能性,那你们就应该尽早在基本问题上达成共识——最好在投身于任何具体项目之前。如果在此刻达不成共识,以后也不 可能达成共识。  如果双方争执激烈,那么就有必要更换项目或合作伙伴。因为牵涉到知识产权,所以如果发表获得经济利益,双方在知识产权上达成共识将非常重要。</li><li>在将论文寄给哪份期刊发表这一点上达成共识。</li><li>在如果论文被拒将采取什么行动上达成共识。你不希望在论文被拒后出现这样的情形:合著人对论文稍加改动后独立地以他们自己的名义寄给其他期刊发表。一个解决办法就是事先选出一个合著人,如果论文被拒,这个合著人将在改进论文,将论文投寄给其他期刊的过程中承担主要责任,如果论文得以发表,他也就是该论文的第一作者。</li></ol><h2 id="著作权"><a href="#著作权" class="headerlink" title="著作权"></a>著作权</h2><ol><li>著作权是在一开始就应该达成共识的问题。一般情况下,作者应该包括学生和导师,学生是第一作者。如果这方面达不成共识,就不用再走下一步了。</li><li>如果要发表的论文是由若干个研究项目汇编而成的,那么汇编人应该是第一作者。</li><li>必须是对论文作了大量贡献的人才能被列为合著人。如果只是一次给予建议的师生谈话(比如你咨询了导师以外的一名教职人员),这不能构成对论文的显著贡献。如果你想要就论文的某些部分听取系里其他老师的意见,你应该在事先告诉你的导师,以免产生误解和不愉快。</li></ol><h2 id="投寄以及修改"><a href="#投寄以及修改" class="headerlink" title="投寄以及修改"></a>投寄以及修改</h2><ol><li>在论文寄出去前,所有作者和合著人都应该看过并同意终稿的内容与形式。</li><li>不要同时寄给多家期刊。这么做会被期刊列入黑名单。这种惩罚是有道理的:同时向多份期刊投寄文章可能使编辑在无意中触犯版权法规。同时它还会浪费编辑和审阅人的宝贵时间。这些人的工作压力已经很大,他们不希望有人再给他们带去不必要的任务.</li><li>如果论文在修改后可以被发表,那么所有作者和合著人都应该在第二稿寄出前得到机会发表自己的修改建议(比如每人都得到一份拷贝，并且每人都得到指示,必须在两周内对其给出评论和建议）。</li></ol><h2 id="发表后"><a href="#发表后" class="headerlink" title="发表后"></a>发表后</h2><ol><li>每个作者和合著人都应该得到至少一份论文抽印本。</li><li>所有作者和合著人都应该得到至少一份关于论文的评论性报道的拷贝。</li></ol><h2 id="论文检查一览表"><a href="#论文检查一览表" class="headerlink" title="论文检查一览表"></a>论文检查一览表</h2><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol><li>你有明确的研究问题吗?</li><li>你证明了问题的有趣性吗?</li><li>你证明了问题的重要性吗?</li><li>你证明了答案不是平淡无奇的吗?</li><li>你的“叙述线索”清晰可见吗?</li><li>你的论证清晰有条理吗?</li></ol><h2 id="将你的论文放到学术研究的大环境中去考察"><a href="#将你的论文放到学术研究的大环境中去考察" class="headerlink" title="将你的论文放到学术研究的大环境中去考察"></a>将你的论文放到学术研究的大环境中去考察</h2><ol><li>你有没有参照现存文献来定位你的论文?</li><li>你的论文是否清晰地体现出,它是建立在什么理论基础之上的,它又在哪些方面对该理论作出了贡献?</li><li>你是否讨论了你研究的前提、历史和局限性?</li><li>你是否讨论了你的论文对未来研究的意义?</li></ol><h2 id="根据"><a href="#根据" class="headerlink" title="根据"></a>根据</h2><ol><li>你有没有依照学科要求清楚地给出根据?</li><li>你对数据的阐释与数据本身有区别吗?</li><li>你的结论有根据吗?</li><li>读者能否根据你论文的描述复制或模仿你的研究?</li></ol><h2 id="给予别人应得的肯定"><a href="#给予别人应得的肯定" class="headerlink" title="给予别人应得的肯定"></a>给予别人应得的肯定</h2><ol><li>你和其他合著人有没有就著作权以及作者排序达成共识?</li><li>你有没有给予所有作出贡献或提供帮助的人应得的肯定?</li><li>你的参考书目是否正确完整?</li></ol><h2 id="文献使用"><a href="#文献使用" class="headerlink" title="文献使用"></a>文献使用</h2><ol><li>你引用了最具重大影响的文献吗?</li><li>你引用了经典文献吗?你引用了基础性文献吗?</li><li>你论文第一页上的引用达到了五处以上吗?</li><li>你使用的文献的时间跨度是否是从开创某领域的研究到新进的研究发展?</li><li>你是否引用了一些特殊的文献,能体现出你了解超出某个课题标准文献之外的信息?</li></ol><h2 id="发表在哪里"><a href="#发表在哪里" class="headerlink" title="发表在哪里?"></a>发表在哪里?</h2><ol><li>你决定了要在哪里发表了吗?</li><li>你查清楚发表的最后期限了吗(如果有最后期限的话)?</li><li>你阅读和遵守了投稿须知吗?</li></ol><h2 id="论文外观"><a href="#论文外观" class="headerlink" title="论文外观"></a>论文外观</h2><ol><li>你遵守了投稿须知的相关规定吗?</li><li>你论文中的表格和引用符合出版社的印刷风格吗?你有没有检查单词的拼写?</li><li>你有没有使用合适的具有国别性的拼写习惯(英英拼写或美英拼写)?</li><li>你论文中的小标题对内容有没有起到明确清晰的指示作用?你的论文外观格式是否符合你所在学科的学术惯例?</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议7：论文类型</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE7%EF%BC%9A%E8%AE%BA%E6%96%87%E7%B1%BB%E5%9E%8B/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE7%EF%BC%9A%E8%AE%BA%E6%96%87%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p>[数据导向型论文(Data-driven papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%90%91%E5%9E%8B%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#数据导向型论文</a>(Data-driven papers))</p><p>[教学指导性论文(Tutorial papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E6%95%99%E5%AD%A6%E6%8C%87%E5%AF%BC%E6%80%A7%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#教学指导性论文</a>(Tutorial papers))</p><p>[方法导向型论文(Method-mongering papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E6%96%B9%E6%B3%95%E5%AF%BC%E5%90%91%E5%9E%8B%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#方法导向型论文</a>(Method-mongering papers))</p><p>[意识唤醒型论文(Consciousness-raising paper)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E6%84%8F%E8%AF%86%E5%94%A4%E9%86%92%E5%9E%8B%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#意识唤醒型论文</a>(Consciousness-raising paper))</p><p>[理论性论文(Theoretical papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E7%90%86%E8%AE%BA%E6%80%A7%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#理论性论文</a>(Theoretical papers))</p><p>[评论性论文(Review papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E8%AF%84%E8%AE%BA%E6%80%A7%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#评论性论文</a>(Review papers))</p><p>[概念展示型论文( Demonstration of concept papers)](<a href="https://zhuanlan.zhihu.com/p/401681744/edit#%E6%A6%82%E5%BF%B5%E5%B1%95%E7%A4%BA%E5%9E%8B%E8%AE%BA%E6%96%87">https://zhuanlan.zhihu.com/p/401681744/edit#概念展示型论文</a>( Demonstration of concept papers))</p><hr><h2 id="数据导向型论文-Data-driven-papers"><a href="#数据导向型论文-Data-driven-papers" class="headerlink" title="数据导向型论文(Data-driven papers)"></a>数据导向型论文(Data-driven papers)</h2><ol><li>如果你的论文的中心是数据，那么你需要好的数据。这意味着数据可靠并且有趣。“可靠”的意思是：样本的大小、质量和代表性等应该达到一个毋庸置疑的标准。研究人员中的新手(有时候不仅仅是新手)常常会担心样本的大小是否达标，被调查对象是否足够多</li><li>能够吸引人的研究结果往往是让人吃惊的，间时又是有用的。这个结果应该有一个基础坚实的样本作为依据，样本数据必须足够可信，以至于可以将其作为未来进一步研究的基础。</li><li>一篇经典的、建立在数据基础上的研究论文能为其作者带来声誉。然而大多数以数据为基础的研究论文都无法有新突破。你必须拥有可靠的、有趣的数据才能为这类数据导向型论文建立起在学术界的声誉。</li></ol><h2 id="教学指导性论文-Tutorial-papers"><a href="#教学指导性论文-Tutorial-papers" class="headerlink" title="教学指导性论文(Tutorial papers)"></a>教学指导性论文(Tutorial papers)</h2><p>教学指导性论文是描述一种研究方法并解释如何使用这种方法的论文这类论文很有价值，然而学术期刊往往不发表此类论文，因为原创性研究是学术期刊的精神所在，而教学指导性论文通常不涉及原创性的研究。但是如果你发表了一篇经典的、论述某种研究方法的教学指导性论文这篇论文就会在未来几年里一直被人们引用。</p><h2 id="方法导向型论文-Method-mongering-papers"><a href="#方法导向型论文-Method-mongering-papers" class="headerlink" title="方法导向型论文(Method-mongering papers)"></a>方法导向型论文(Method-mongering papers)</h2><p>这类论文同样描述一种研究方法,但目的是为了指出这种研究方法应该得到更广泛的应用。所描述的方法可以是原创的(比如由作者本人发明的),也可以是在别的领域里已经发展成熟,却还未在作者想要将其推广的领域内得到充分重视的研究方法。</p><h2 id="意识唤醒型论文-Consciousness-raising-paper"><a href="#意识唤醒型论文-Consciousness-raising-paper" class="headerlink" title="意识唤醒型论文(Consciousness-raising paper)"></a>意识唤醒型论文(Consciousness-raising paper)</h2><p>这类论文并没有它听上去那么玄妙。它的目的是为了唤起人们对某些问题的意识,因为这些问题在研究领域里还没有得到充分的重视。这些问题往往涉及其他领域的方法和概念在研究者所在领域内的应用。</p><h2 id="理论性论文-Theoretical-papers"><a href="#理论性论文-Theoretical-papers" class="headerlink" title="理论性论文(Theoretical papers)"></a>理论性论文(Theoretical papers)</h2><ol><li>理论性论文有很高的威信。这类论文讨论的是理论问题，比如象征推理( symbolic reasoning) 的内在局限性。它可以产生巨大的影响力。发表了的理论性论文通常出自某个领域的权威之手</li><li>我们的建议是在你不能确认自己能够胜任,或者拿不出证据来证明你的能力之前,不要尝试写理论性的论文。</li></ol><h2 id="评论性论文-Review-papers"><a href="#评论性论文-Review-papers" class="headerlink" title="评论性论文(Review papers)"></a>评论性论文(Review papers)</h2><ol><li>每隔差不多十年的时间，一个领域内总会有某些人决定，是时候写一篇论文,来考察自上一篇评论性论文发表以来领域内又取得了哪些重要进展</li><li>评论性论文对普通读者来说很有价值,因为它的信息很完整,包括概述和关键的资料文本,为读者提供了进入某个领域研究体系的快捷渠道。</li></ol><h2 id="概念展示型论文-Demonstration-of-concept-papers"><a href="#概念展示型论文-Demonstration-of-concept-papers" class="headerlink" title="概念展示型论文( Demonstration of concept papers)"></a>概念展示型论文( Demonstration of concept papers)</h2><ol><li>概念展示型沦文与很多其他类型的论文都有重叠,尤其是方法导向型论文。概念展示型的论文是为了向读者展示一个特定的概念(通常是一种方法,但也有例外——比如也可以是一种概念性框架)是可行的、有用的、有趣的。这类论文在申请研究经费时很管用。</li></ol><p>如果你很清楚你要论证的是什么,该用什么方法去论证,那么只需要来自一个样本的一组数据就可以完成一篇概念证明型的论文。写这类论文最难的还是要找到一个好的概念。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议6：阅读</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE6%EF%BC%9A%E9%98%85%E8%AF%BB/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE6%EF%BC%9A%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%89%BE%E5%88%B0%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">找到正确的参考文献</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E7%A0%94%E7%A9%B6%E8%80%85%E7%9A%84%E6%A0%B8%E5%BF%83%E6%96%87%E7%8C%AE%E5%BA%93">研究者的核心文献库</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0">文献综述</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%9C%A8%E7%BA%BF%E6%90%9C%E7%B4%A2">在线搜索</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E4%BC%A0%E8%BE%BE%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%BF%A1%E6%81%AF">传达正确的信息</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%9C%A8%E7%BA%BF%E6%A3%80%E7%B4%A2%E6%A6%82%E8%BF%B0">在线检索概述</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%9D%A5%E6%BA%90">来源</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E7%AD%96%E7%95%A5">策略</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%9C%89%E7%BB%8F%E9%AA%8C%E7%9A%84%E8%AF%84%E5%AE%A1%E4%BA%BA%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87">有经验的评审人如何读论文</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%A0%87%E9%A2%98%E9%A1%B5%E4%B8%8E%E8%87%B4%E8%B0%A2">标题页与致谢</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%91%98%E8%A6%81">摘要</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E9%A6%96%E9%A1%B5">首页</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%96%B9%E6%B3%95">方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E7%BB%93%E6%9E%9C%E9%83%A8%E5%88%86">结果部分</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E8%AE%A8%E8%AE%BA%E9%83%A8%E5%88%86">讨论部分</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E7%BB%93%E8%AE%BA">结论</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%A4%A7%E9%87%8F%E9%98%85%E8%AF%BB">大量阅读</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E4%BD%BF%E7%94%A8%E6%96%87%E7%8C%AE%E4%B8%AD%E7%9A%84%E6%9D%90%E6%96%99">使用文献中的材料</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%89%BD%E7%AA%83">剽窃</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%A6%82%E4%BD%95%E5%BC%95%E7%94%A8">如何引用</a></p><p>[文献回顾(literature survey)和文献综述( literature review)有何区别?](<a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%96%87%E7%8C%AE%E5%9B%9E%E9%A1%BE">https://zhuanlan.zhihu.com/p/401681418/edit#文献回顾</a>(literature survey)和文献综述( literature review)有何区别%3F)</p><p>[做一份带说明的文献录( annotated bibliography)](<a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%81%9A%E4%B8%80%E4%BB%BD%E5%B8%A6%E8%AF%B4%E6%98%8E%E7%9A%84%E6%96%87%E7%8C%AE%E5%BD%95">https://zhuanlan.zhihu.com/p/401681418/edit#做一份带说明的文献录</a>( annotated bibliography))</p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E6%A0%B8%E5%BF%83%E6%96%87%E7%8C%AE%E5%BA%93">核心文献库</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681418/edit#%E5%AD%A6%E6%9C%AF%E8%AE%AD%E7%BB%83">学术训练</a></p><hr><h2 id="找到正确的参考文献"><a href="#找到正确的参考文献" class="headerlink" title="找到正确的参考文献"></a>找到正确的参考文献</h2><ol><li><p>用了基本上所有需要被引用的参考文献/那么你怎么知道哪些文献是你需要引用的呢?</p></li><li><p>最简单的办法是礼貌地询问你的导师,该从哪儿着手。如果你的导师不知道,礼貌地问别人,并时时向导师汇报你的进程,以免步人写作误区。你应该传达这样的信息﹒那就是,你是一个勤奋的、能从别人的建议中受益的学生,而不是一个不想自己动脑筋做研究的懒虫(可以先说明自己已经读了什么,然后询问下一步该做哪些工作)。导师在这方面能起到举足轻重的作用。</p></li><li><p>如果你很用功也很走运的话,你的导师可能会对你说类似这样的话:“这方面你需要找的人是X,我已经发了电子邮件给他,他会很乐意给你一些指导。这是他的电子邮件地址。”这类话是一个令人鼓舞的信号,在学术界,它传达了以下这些意思:</p></li><li><ol><li>你可以省去很多麻烦﹔</li><li>你可以得到机会与领域内的专家接触﹔</li><li>我对你有足够的信任.所以才让你自己去与这些重要人物谈话。</li></ol></li></ol><h2 id="研究者的核心文献库"><a href="#研究者的核心文献库" class="headerlink" title="研究者的核心文献库"></a>研究者的核心文献库</h2><ol><li>大多数优秀的研究者都在记忆中储存了一个相关文献的数据库。这就意味着，从所有阅读过的资料中,他们会选择50~100篇文章记在脑子里。随着阅读不断进行,这个脑子里的核心文献库也会随着学科发展或研究者的兴趣转变而做相应调整。然而核心的某些部分会留在记忆中长达数年。有趣的是，一篇论文的参考书目在理想情况下至少应该包含的文献数目恰好是100左右博士生应该做的一件事就是组建第一个“核心”文献库。</li><li>那么,应该从哪里着手寻找参考文献呢？你的导师可能会提醒你留意在的已经读过的学术文章里出现过的文献综述，以及评论性文章。导师也可能会指点你进行在线搜索。搜索的关键词可以是你的导师提供的，也可以是你在读过的文章中找到的。</li></ol><h2 id="文献综述"><a href="#文献综述" class="headerlink" title="文献综述"></a>文献综述</h2><ol><li>文献综述最常用的,也是我们赞同的一种结构,是从某领域的最早研究开始以时间顺序讨论所有迄今为止的重要研究。按照这种结构,你的引用就应该从该领域最早的文献开始,接着介绍一些各个时期最重要的文献,最后写当下的重要文献。之后 ,你的参考文献则一 般从旧的研讨会参考文献开始,接着是近期的关键参考文献和次重要的参考文献,最后是最新的重要参考文献。</li><li>你对这样的上层结构满意了之后,就可以将其中每个部分分解为几个小部分,如果必要的话,可以不断重复这个过程。最后你得到的应该是各部分的小标题,次级小标题等等,而这些就组成了论文的整体框架。</li><li>过渡段是将文中的两个部分连接起来的段落,它总结上一段。引出下一段的内容,并向读者解释上下段的联系。指向标是告诉(或暗示)读者接下来会出现什么内容的段落。</li><li>我们听说一些挑剔的导师会给学生明确的指示,规定哪些资料是能作为前期熟悉某个领域的工具来阅读,而不能在论文中作为信息来源引用的。</li></ol><h2 id="在线搜索"><a href="#在线搜索" class="headerlink" title="在线搜索"></a>在线搜索</h2><p>文献综述虽然是指导你了解文献的有用媒介,但它并不是万能的。文献综述的作者在写作时也不可能考虑到你的特殊需要。所以你有必要自己去摸索,寻找与你的研究有关的文献。</p><h3 id="传达正确的信息"><a href="#传达正确的信息" class="headerlink" title="传达正确的信息"></a>传达正确的信息</h3><ol><li>学者打交道的对象是知识与信息，所以应该知道怎样获取、解释和展示知识与信息。其中一个重要环节就是要找到最好的信息来源,以确保你对研究课题的评估是建立在最为可靠的知识与信息的基础上的。</li><li>学术文献有一个长幼尊卑的排名。这个排名在某种程度上是一种势利行为,但基本上还是依据每份刊物的质量监管水平得出的结果。一份刊物的质量监管越严格,它享有的声誉也就越高。</li><li>“在线”指的是图书馆数据库和光盘检索,“在线”不需要你上因特网。</li></ol><h3 id="在线检索概述"><a href="#在线检索概述" class="headerlink" title="在线检索概述"></a>在线检索概述</h3><ol><li>如果你没有找到任何别人做过的研究工作，你应该确认自己的检索是否全面,必须在很有把握的情况下,你才能说“这个领域到目前为止还没有人研究过”。</li><li>大多数聪明的专业人士会选择其他表达方式,比如“这个领域在过去似乎没有引起人们的注意”。如果存在着他们没有发现的文献,这种说法能使他们不至于完全丢面子。</li></ol><h3 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h3><p>进入索引也就意味着,(a)你很有可能找 到与课题有关的重要信息,以及如果找不到任何信息,很有可能的确没有相关的学术文章发表过——对于博士阶段侧重于原创性的研究来说,这是很重要的考虑因素。</p><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>一种简单有效的方法就是输人一个关键词,再在系统给出的条目中寻找有关记录。你可以从这些相关记录中再次寻找可以作为关键词的选项,进行二次检索。作者的姓名往往可以用来做关键词(但像史密斯这样过于常见的姓名除外)——在某个课题上发表过一篇文章的作者通常会有其他相关文章发表,这些文章的合著者姓名又可以作为关键词使用。技术用语也是如此,你可能会发现很多不同的名字指的都是同一个概念。你可以输入一个较具专业性的关键词(如果前一次检索得出的条目过多的话),或者一个较为宽泛的关键词(如果前一次检索得出的条目过少的话)。</p><h2 id="有经验的评审人如何读论文"><a href="#有经验的评审人如何读论文" class="headerlink" title="有经验的评审人如何读论文"></a>有经验的评审人如何读论文</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><p>必须符合的要求</p></li><li><ol><li>你采用的格式,包括每一个标点符号,都正确吗?你是否引用了所有重要的核心文献?</li><li>你引用的文献是否具有多样性,有没有覆盖最重要的以及最新的文献?</li><li>你的文献来源是权威性的学术期刊,还是教科书和因特网? 使用,教科书和因特网只能是在极特殊的情况下才拿来使用。(在这个阶段,你应该已经把学术期刊作为主要参考资料来)</li></ol></li></ol><ol><li><p>加分项</p></li><li><ol><li>你是否引用了一些不常见的，只有某领域的高级研究人员才知道的文献?</li><li>你是否引用了正在印刷,还未出版的文献?(如果是,这意味着你已经真正成为一名学术圈的成员,有资格获得预先印发稿。)</li><li>如果你引用了预先印发稿上的内容,该稿件的作者在领域内是否占有重要地位?</li><li>你是否引用了你本人在权威期刊上发表过的文章,其中有没有与著名学者合著的文章?</li></ol></li></ol><h2 id="标题页与致谢"><a href="#标题页与致谢" class="headerlink" title="标题页与致谢"></a>标题页与致谢</h2><ol><li>将题目分成两部分往往是很奏效的方法。第一部分的题目应该是好记、能吸引人的。第二部分的题目用冒号与第一部分隔开,它解释和阐明第一部分题 目的含义。</li><li>如果你使用由两部分组成的题目,关键词往往会在第二部分出现(但没有规定必须如此)。</li><li>关于致谢,还有一种可能的说法:一些老道的研究人员,如果他的论文需要提交评委会审阅,他们会在感谢词中感谢所有他们不愿意看到出现在评委会名单上的学者名字。这样做的逻辑在于,因为潜在的利益冲突问题,任何在感谢词中出现的人员都不会被请来当论文的评审人。所以,如果你与领域内的某个学者起过争执,关系不和,你不想让他审阅你的论文,你可以在致谢词中礼貌地写上:“我要衷心感谢X在这个问题上与我进行的多次讨论。”这样你遇到麻烦的可能性就大大降低了。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li><p>你做了什么,为什么这么做,你的发现是什么,你的研究在理论和实践上有什么重要意义。如果你在一开始就对自己是否能取得重要的研究成果心存怀疑，那你就应该重新设计研究方案</p></li><li><p>我们能给出的最好的建议是尽可能多地获取专家的反馈。另一个好办法是阅读你的研究领域里的重要学术论文,学习它们的摘要是怎么写的。</p></li><li><p>好的摘要</p></li><li><ol><li>恰当地使用专业术语清晰</li><li>陈述重要发现及其意义</li><li>拼写和标点符号使用正确(这是论文主体的第一一页﹐决定着读者的第一印象,所以你应该费点心思把它写好）。</li></ol></li></ol><ol><li><p>不好的摘要</p></li><li><ol><li>不妥当的写作风格</li><li>大话连篇</li><li>自我推销</li><li>空洞的内容</li><li>不清晰的表达</li></ol></li></ol><ol><li><p>每一张表格都应该是有用的。你认为有必要用多少表格就应该用多少表格,但不要多于必要的数目。还要防止将一个信息用多种形式表达——比如,同时使用矩形图和饼图。</p></li><li><p>掩饰论文低劣质量的惯用招数有:</p></li><li><ol><li>幽默(第二条黄金法则:不要故作幽默)</li><li>剪贴画(clip art)</li><li>用不同图表展示同一材料</li><li>用很多颜色来装饰图表</li><li>附件材料过多</li></ol></li></ol><ol><li>如果以上的任何一点表现在了你的目录页(或论文的其他部分）里,你就有可能遇上麻烦。</li></ol><h2 id="首页"><a href="#首页" class="headerlink" title="首页"></a>首页</h2><ol><li><p>好的首页</p></li><li><ol><li>清晰</li><li>合适的写作风格(要显得正式,即使这意味着文章枯燥生硬）使用正确的参考文献</li><li>提出好的研究问题</li></ol></li></ol><ol><li><p>不好的首页</p></li><li><ol><li>体现不出学术性不清晰</li><li>没有提出研究问题,或者问题提得不好</li><li>恳求读者的同情体谅,以及其他透露出文章弱点的迹象</li></ol></li></ol><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>方法部分的存在是有原因的。那就是,如果任何人想要复制你的研究,或者在你研究的基础上继续改进,那么他们有必要了解你究竟做了哪些工作,是怎么做的,研究调查的对象又是谁。第二个原因是,读者了解你的研究方法才能对你研究工作的质量进行有效评估。</p><h2 id="结果部分"><a href="#结果部分" class="headerlink" title="结果部分"></a>结果部分</h2><ol><li><p>好的结果</p></li><li><ol><li>足够多的表格</li><li>表格设计和排列有逻辑</li><li>表格看上去整洁观</li><li>数据合计没有错误</li></ol></li></ol><ol><li><p>不好的结果</p></li><li><ol><li>表格太少或太多</li><li>表格排列混乱,读者看不出每个表格说明的结果</li><li>表格看上去粗糙,不整洁</li><li>数据合计有问题</li></ol></li></ol><h2 id="讨论部分"><a href="#讨论部分" class="headerlink" title="讨论部分"></a>讨论部分</h2><p>因为某些原因,往往很难从讨论部分看出论文作者对专业知识技能的掌握是否过硬。这可能是因为,即便是缺乏经验的研究员，也可能在绝望中生出灵感,编一些精彩又可信的故事来解释他们在研究中的发现。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>结论通常还包括关于“将来研究”的部分 论应该对引言中提出的问题给出一系列清楚明确的回答。这些回答应该由结果部分(如果是实验科学学科的论文)的数据以及讨论部分(每一类论文都必须有)的内容作为依据。</li><li>学者们常常利用将来研究的部分为自己在未来的研究领域抢得一块地盘。有经验的研究人员往往会在这个部分简要描述自己未来的研究计划和课题；有经验的读者能清楚认识到，到论文发表时，它的作者可能已经花了一年的时间在新课题的研究上了，所以自己也就没有必要草率地往那个研究方向上努力了。</li></ol><h2 id="大量阅读"><a href="#大量阅读" class="headerlink" title="大量阅读"></a>大量阅读</h2><p>在你自己的学科领域,你的阅读应该有深度、有广度、有时间跨度——和你选择的研究领域有关的文献你应该深入地了解，其他相关研究领域内的文献，以及你自己的领域内所有过去进行的研究，你都应该有一个大致的把握。与你自己的研究课题相关的所有学术作品，包括最具专业性的学术期刊文章,你都应该仔细阅读</p><h2 id="使用文献中的材料"><a href="#使用文献中的材料" class="headerlink" title="使用文献中的材料"></a>使用文献中的材料</h2><h2 id="剽窃"><a href="#剽窃" class="headerlink" title="剽窃"></a>剽窃</h2><p>不同文化对剽窃的定义各有不同,但你的论文是否有剽窃行为将按照英国学术界对剽窃的定义来判断。英国学术界对剽窃的定义是很严格的:剽窃是指间接或直接地使用别人的想法、语句或材料，而不对此进行指明。</p><h2 id="如何引用"><a href="#如何引用" class="headerlink" title="如何引用"></a>如何引用</h2><ol><li><p>作者通过使用文献可以传达很多东西。其中有一些是关于论文内容的(.工具性使用),而另一些则是关于他们自己的(表现性使用):</p></li><li><ol><li>建立你的权威性;</li><li>引用你自己发表过的文章；</li><li>覆盖面——能体现出你熟悉学术规范的要求;</li><li>深度——能体现出你掌握了那些最不为人所知的文献；</li><li>在排除你不想涉及的领域同时,指出你是了解这些领域的；表达对审阅人的尊敬；</li><li>体现你研究问题的合理性；体现你研究方法的合理性；体现你数据分析的合理性;提供一个理论框架或视角证实你的研究发现。</li></ol></li></ol><ol><li><p>思考以下问题:</p></li><li><ol><li>如果你引用的文献是1925年出版的,这说明了什么?</li><li>如果你的参考文献涵盖的主要是书籍,这说明了什么?</li><li>如果你的参考文献只涵盖了3名重要学者的作品,这说明了什么?</li><li>如果你的参考文献只涵盖了最近两年内发表的论文,这说明了什么?</li><li>如果你的参考文献中有5成是你自己发表的文章,这说明了什么?</li><li>如果你引用的文献总是成群出现(比如,斯邦吉1982;布鲁戈1998；格鲁莫2002),这说明了什么?</li></ol></li></ol><ol><li><p>现在尝试以下的做法:</p></li><li><ol><li>阅览一些出版了的学术论文。</li><li>在这些论文中出现了多少处引用?</li><li>这些引用是怎样布局的?</li><li>第一处引用是在哪儿出现的?</li><li>引用的文献有哪些种类?标题是概括的,还是具体的?这些参考文献是何时出版的?作者是谁?</li><li>再看一看论文的讨论及结论部分。</li><li>在这些部分里出现了孔处引用?是在什么情况下出现的?</li><li>它们与背景介绍或引言部分用到的参考文献是相同的吗?</li></ol></li></ol><h2 id="文献回顾-literature-survey-和文献综述-literature-review-有何区别"><a href="#文献回顾-literature-survey-和文献综述-literature-review-有何区别" class="headerlink" title="文献回顾(literature survey)和文献综述( literature review)有何区别?"></a>文献回顾(literature survey)和文献综述( literature review)有何区别?</h2><p>文献回顾与文献综述的区别在于前者是做汇总,后者是做批评</p><h2 id="做一份带说明的文献录-annotated-bibliography"><a href="#做一份带说明的文献录-annotated-bibliography" class="headerlink" title="做一份带说明的文献录( annotated bibliography)"></a>做一份带说明的文献录( annotated bibliography)</h2><h2 id="核心文献库"><a href="#核心文献库" class="headerlink" title="核心文献库"></a>核心文献库</h2><ol><li><p>最少应该包括的内容:</p></li><li><ol><li>参考书目的基本信息(读者要找到这部文献必须知道的信息);</li><li>你阅读该文献的日期；</li><li>你认为它在哪些方面是有趣的/重要的/让人不解的。将你的感受记录下来。你的笔记不应该抄袭原文献的摘要部分,而应该是你自己的批判性思考。它可以是非正式的、不符合语法的,甚至是偏激的,只要它表达出了你想表达的意思就行。如果你在重读某一文献时有了新的感受,那么应该对笔记进行补充——但即使你改变了你的观点或着法,也一定要保留原来的笔记。</li></ol></li></ol><ol><li><p>它能包括其他一些有用的信息﹐比如:</p></li><li><ol><li>文献来源(是复印稿、图书馆藏,还是个人收藏?);关键词,最好是不同类型的关键词;</li><li>后续研究要用到的参考文献;</li><li>你是怎么发现某部文献的(比如是谁推荐了它,或引用了它)与某部文献相关的其他文献信息﹔</li><li>有关作者信息的摘要。</li></ol></li></ol><h2 id="学术训练"><a href="#学术训练" class="headerlink" title="学术训练"></a>学术训练</h2><ol><li><p>不要删除以前做下的文献记录。“废弃物”可以重新分类或单独保存,某一年的“垃圾”可能会成为下一年的“宝藏”(反之亦然)。同时,保留分类变动的记录是很有意义的，它的一个 做法就是列出你现 在使用的所有类别的“定义”金对分类进行调整之后,不要丢弃原来的分类方案,而是应该将它保存起来,作为记录的一部分。</p></li><li><p>文献目录的其他好处</p></li><li><ol><li>它能帮助你回顾你的思考过程;</li><li>它能反映出你的阅读和思想是怎样不断成熟的,你是怎样一步步认识到某些问题的重要性的,又是怎样一步步完善你的阅读记录的;</li><li>当你看到一条参考书目却记不起那部文献的具体观,点时,摘要能给你关键性的提示;</li><li>当你在答辩前重读某些文献,产生了与以前不同的理解时﹐你以前做下的笔记会对你很有帮助。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议5：人际关系网络</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE5%EF%BC%9A%E4%BA%BA%E9%99%85%E5%85%B3%E7%B3%BB%E7%BD%91%E7%BB%9C/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE5%EF%BC%9A%E4%BA%BA%E9%99%85%E5%85%B3%E7%B3%BB%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%BB%BA%E7%AB%8B%E4%BA%BA%E9%99%85%E5%85%B3%E7%B3%BB%E7%BD%91%E7%BB%9C">建立人际关系网络</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E8%AE%BE%E5%AE%9A%E7%9B%AE%E6%A0%87">设定目标</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%BB%BA%E7%AB%8B%E4%BA%BA%E9%99%85%E5%85%B3%E7%B3%BB%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B7%A5%E5%85%B7">建立人际关系网络的工具</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%A5%89%E6%89%BF">奉承</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%92%96%E5%95%A1">咖啡</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%B7%A7%E5%85%8B%E5%8A%9B%E9%A5%BC%E5%B9%B2">巧克力饼干</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E4%BA%92%E6%83%A0">互惠</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8E%A5%E8%A7%A6%E2%80%94%E2%80%94%E9%99%8C%E7%94%9F%E7%94%B5%E8%AF%9D">第一次接触——陌生电话</a></p><p><a href="https://zhuanlan.zhihu.com/p/401681171/edit#%E5%9C%A8%E4%BC%9A%E8%AE%AE%E4%B8%8A">在会议上</a></p><hr><h2 id="建立人际关系网络"><a href="#建立人际关系网络" class="headerlink" title="建立人际关系网络"></a>建立人际关系网络</h2><ol><li>你的人际关系网络的核心当然是你的导师。</li><li>网络的第二层重要的组成部分是你的非正式“委员会”(也就是那些会帮助你,确保你的研究具有高品质的人士)。要组建这样一个“委员会”,你需要一小群值得信任的,对你的研究抱有兴趣的学者.他们会愿意帮你做一些工作,阅读你写的东西，作出评论,给予建议和批评,提供有用的信息，把你介绍给其他研究人员等等。他们可以是能够提供特殊知识和技能的专家,也可以是能够提出深刻问题的博学者。</li><li>第三层主要组成部分是你个人的支持团队,也就是鼓励你,给予你支持的人。他们会帮助你平衡生活与工作,在你兴致高昂地做研究时为你送来比萨。他们可以是家人、同学或是老朋友,他们也可能是系里那些善于让你保持头脑清醒的老师。</li></ol><h2 id="设定目标"><a href="#设定目标" class="headerlink" title="设定目标"></a>设定目标</h2><ol><li><p>以下是人际关系网络组成人员的三个最好的来源:</p></li><li><ol><li>特别相关论文的作者﹔</li><li>你在会议上遇到的、发表了有趣的并与你研究有直接关联的意见可以说的人;</li><li>你信任的人向你推荐的人(比如你的导师或非正式委员会成员推荐的人）。</li></ol></li></ol><ol><li>仅仅告诉他你也在从事相关的研究是不够的，他听了会想:“那又怎么样?”你有必要明确告诉他你的需要，是想让他阐释关于他的著作的某些问题,还是想找个机会共同探讨,又或者是想邀请他审阅你的论文。你的问题越有重点,越能显示你是有备而来,事情的成功概率就越大。记住,你想要接近的人必定有一定的身份地位，也就必定会有其他学生想要接近。</li></ol><h2 id="建立人际关系网络的工具"><a href="#建立人际关系网络的工具" class="headerlink" title="建立人际关系网络的工具"></a>建立人际关系网络的工具</h2><p>两种主要的、历史悠久的工具是奉承和贿赂。奉承就是要说好听的话;贿赂主要是借咖啡、巧克力饼干和帮助找到难找的参考书等实际好处的形式。(为了防止误解，有必要澄清一下,通过金钱和性行贿是不道德的,也是非法的,我们坚决反对这类做法。)</p><h2 id="奉承"><a href="#奉承" class="headerlink" title="奉承"></a>奉承</h2><p>有效奉承的秘诀是:它应该是坦率的、明确的、简洁的和准确的。也就是说,它应该从奉承者的口中自然而然地、不加掩饰地流出,并让被奉承者产生具体和准确的回应。</p><h3 id="咖啡"><a href="#咖啡" class="headerlink" title="咖啡"></a>咖啡</h3><p>名人也是人,在举行会议的场所,如果有片刻休息,请他们喝一杯好咖啡他们会很乐意的。请他们喝咖啡有很多好处。首先是可以借此得到一些非正式的建议——就业前景、组织政策,以及某个研究领域的未来。其次它也提供了一个在会场或类似场合放松的机会。在漫长的会议期间,请别人喝一杯高品质的咖啡可以被看作是一种真正的善意行为,尤其是当你做到考虑周到、说话礼貌的情况下。</p><h2 id="巧克力饼干"><a href="#巧克力饼干" class="headerlink" title="巧克力饼干"></a>巧克力饼干</h2><p>巧克力饼干真的是非常有效的“贿赂物”。如果你给某人现金作为成为你试验对象的酬劳，会有一定效果。但如果你给他们高档巧克力饼干和地道的咖啡，效果会更好。他们会变得更加合作，也会更加友好地与你交流。</p><h2 id="互惠"><a href="#互惠" class="headerlink" title="互惠"></a>互惠</h2><p>人们很忙。你感兴趣的人往往更忙。要想从他们的宝贵时间里 借出一点来,可以采用交换的形式——为他们做一些有价值的事,他们会因此交换部分时间给你。</p><h2 id="第一次接触——陌生电话"><a href="#第一次接触——陌生电话" class="headerlink" title="第一次接触——陌生电话"></a>第一次接触——陌生电话</h2><ol><li><p>在第一次接触之后应该准备一些材料供对方参考,比如一份篇幅为一页纸的研究概略,或是一份包含你研究发现的会议发言稿。一定要确保你寄过去的材料能很好地展现你的学术水平:确保书写清晰,没有错误,可以让你的导师或其他有经验的读者先检查一遍。</p></li><li><p>如果你收到了答复,那么一定要立刻寄出封感谢信。如果你有一份很好的关于你研究项自的概述,或者研究报告的节选,也可以附带寄出。</p></li><li><p>我们在这里还给出了一些你在写信前需要考虑的问题:</p></li><li><ol><li>收信人的姓名和称呼你写对了吗?</li><li>你提的问题能显示出你是做了准备吗?</li><li>你的导师或指点你的前辈认为你提的问题有趣吗你的问题让一个正常人回答大概要花多长时间?(如果超过十分钟,你就应该考虑重新组织问题。)</li><li>你的信是否长到需要翻页才能读完?(如果是,缩短它。)</li><li>你的信是否很好地展示了你自己?它展示的是你是否是一个能够正确拼写、思考和提出有趣问题的人?</li><li>你的信给收信人提供了什么东西吗?如果是,你是否能兑现这个承诺?</li></ol></li></ol><h2 id="在会议上"><a href="#在会议上" class="headerlink" title="在会议上"></a>在会议上</h2><ol><li><p>如果你看到你想认识的人正在和一个你认识的人交谈,你可以请这个人帮你介绍一下。如果对方正站在一群人中讨论问题，你可以凑到人群边上听他们的谈话,等待自己说话的机会(一个问题或一个笑话都是不错的选择)或询问能否加入讨论。把人叫住的最佳时机是会议结束后人们离席准备享用咖啡或午餐的空当。但要注意不要耽误对方享用茶点，应该边走边谈。</p></li><li><p>该准备名片,除此以外还可以递上一份你的研究概况(必须是事先通过导师审阅修改过)。</p></li><li><p>别以为自己是不引人注目的,甚至是不值得别人感兴趣的。以下给出了关于伟大学者的一些事实,可以帮助你更真实地看待他们:</p></li><li><ol><li>他们伟大的原因往往是因为他们喜欢新鲜的想法和问题——所以他们通常会乐于听到好想法和好问题</li><li>他们和我们一样,无法抗拒赞美之辞;</li></ol></li></ol><p>他们也曾是学生﹐他们中的很多人仍对学生时代记忆犹新。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议4：导师</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE4%EF%BC%9A%E5%AF%BC%E5%B8%88/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE4%EF%BC%9A%E5%AF%BC%E5%B8%88/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%AF%BC%E5%B8%88%E7%9A%84%E8%A7%92%E8%89%B2">导师的角色</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%92%8C%E5%AF%BC%E5%B8%88%E5%BB%BA%E7%AB%8B%E8%89%AF%E5%A5%BD%E5%85%B3%E7%B3%BB%E7%9A%84%E6%96%B9%E6%B3%95">和导师建立良好关系的方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%A6%82%E4%BD%95%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9B%B8%E5%A4%84">如何与导师相处</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%AD%A6%E7%94%9F%E5%BA%94%E8%AF%A5%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85">学生应该做的事情</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%AD%A6%E7%94%9F%E7%9A%84%E6%AD%A3%E5%BD%93%E8%AF%B7%E6%B1%82">学生的正当请求</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E5%AD%A6%E7%94%9F%E9%9C%80%E8%A6%81%E5%90%91%E5%AF%BC%E5%B8%88%E6%B1%87%E6%8A%A5%E7%9A%84%E4%BA%8B%E6%83%85">学生需要向导师汇报的事情</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E4%BD%A0%E8%83%BD%E5%A4%9F%E8%87%AA%E5%B7%B1%E5%81%9A%E7%9A%84%E4%BA%8B">你能够自己做的事</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E4%BA%8B%E6%83%85%E4%B8%8D%E9%A1%BA%E5%88%A9%E7%9A%84%E6%97%B6%E5%80%99%2C%E4%BD%A0%E7%9A%84%E5%BA%94%E5%AF%B9%E7%AD%96%E7%95%A5">事情不顺利的时候,你的应对策略</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E4%B8%BA%E4%BD%BF%E5%AF%BC%E5%B8%88%E8%A7%81%E9%9D%A2%E4%BC%9A%E6%9C%89%E6%95%88%E8%80%8C%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%84%E5%88%92">为使导师见面会有效而设计的一个简单的规划</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680914/edit#%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BC%9A%E7%A0%B4%E5%9D%8F%E4%BD%A0%E4%B8%8E%E5%AF%BC%E5%B8%88%E5%85%B3%E7%B3%BB%E7%9A%84%E8%A1%8C%E4%B8%BA">一些常见的会破坏你与导师关系的行为</a></p><hr><ol><li>你不能像以前普通上课那样,为了保持低姿态或其他原因,藏在同学中默默无闻。作为博士研究生,你需要表现自己,展示自己。</li><li>常见的原因莫过于学生不听取导师意见。稍不常见或不为人知的则是因为指导教师很无能而造成的“恐怖”。</li><li>坐在桌边抱怨你的导师不够完美,于事无补。除非情况已经变得十分糟糕,否则你必须充分利用你所拥有的一切主意这个过程需要积极的态度和举措,而不能太被动;不要简单地忍受你开始所处的处境,相反,你需要找出自己所拥有的资源和智慧,并且充分利用这些优势。</li><li>问问自己哪些特点可能会使自己在面对任何导师指导时都会很尴尬,哪些特点会.对于某个具体类型的导师指导引起麻烦或不便。然后要思考自己希望在哪些方面有改进或取得进步、如何对待导师、博士学位将对你造成什么样的影响等问题。</li></ol><h2 id="导师的角色"><a href="#导师的角色" class="headerlink" title="导师的角色"></a>导师的角色</h2><ol><li><p>设置博士学位的目的在于展示你作为一个独立的研究者能够自己发现新的知识的能力，如果你期待你的导师熟知你研究课题的所有方面,那么你就没有理解博士学位的意义所在。</p></li><li><p>指导博士论文有很多方法,导师可以扮演很多不同的角色;每个学生都与众不同,也就需要不同的支持和帮助。</p></li><li><ol><li>其中一种类型的学生很大程度上可以独立完成大部分工作,指导见面会就成为师生双方都很享受的时刻:因为彼此能够相互学习。</li><li>另外一种类型的学生从来不主动去学习什么知识,他们需要不间断的反馈、积极的鼓励,看起来总是期待有人一勺一勺的来喂他们,甚至到了一种令人担忧的程度。</li></ol></li></ol><ol><li><p>其他角色包括:</p></li><li><ol><li>专业技术支持:例如一些技术技巧培训,如何应用图书馆或专业软件以及相关文献；提供与其他研究者的联系机会和方式;指导建构论文主题；培养批判性阅读技巧。</li><li>拓展型智力支持:例如帮助学生培养讨论技巧和批判性思维能力;提供本领域内的高级知识与研究论题;提供专家意见与建议,指导学生学会在此领域内做研究工作。</li><li>行政管理支持:例如帮助寻找资金，寻找其他的资源；保护学生不受学校中政治或行政的麻烦的伤害;帮助学生发表论文或著作。</li><li>管理:例如提供一整套制度(会议、最后期限、目标等),规定最终期限和实施方案。</li><li>个人支持:例如进行就业指导,提供情感支持、忠告和建议等。</li></ol></li></ol><ol><li>试着从这个角度来审视一下自己的表现：你有多少次没有出席会议?有多少次迟到?多少次没有任何准备就来开会?多少次希望导师把所有的事情都想到?</li><li>所以在与导师的交流过程中,练习自己传达信息的能力。明确你希望从博士学位课程中学到哪些知识,希望从单独见面会中学到哪方面的知识,并且把你的想法汇报给你的导师。</li><li>如果导师在哪方面没有达到你的期望值,或者让你失望的话,学会原谅这些。这样你才会更有机会和导师最终形成不错的关系。</li></ol><h2 id="和导师建立良好关系的方法"><a href="#和导师建立良好关系的方法" class="headerlink" title="和导师建立良好关系的方法"></a>和导师建立良好关系的方法</h2><ol><li><p>如果不是你努力工作完成课程拿到学位的话，他们有什么理由比你更努力呢?新力学习包括充分重视导师见面会：你应该主动安排这些见面会，提前发布一些相关信息,起草一份日程表，准备一整套报告材料和准备提出的问题等。</p></li><li><p>会议召开前几天,会议组织者应该做的事情包括下面几点:</p></li><li><ol><li>分发会议日程表﹔</li><li>如果会议地点不是设在导师办公室的话,在开始会议前,核实会议地点是否可用﹔</li><li>提醒与会者开会时间与地点;</li><li>分发一些情况简介的资料﹐包括上次会议的备忘录。</li></ol></li></ol><ol><li><p>在会议期间,会议主席应该做以下事情:</p></li><li><ol><li>记录会议日期以及与会者名单；</li><li>检查是否所有人对上次会议的备忘录没有异议；</li><li>检查上次会议颁发的措施是否已经付诸实施；</li><li>记录所有决议。包括重要事件与备送货物等(核对无误:每个人都同意记录的内容)；</li><li>记录所有协商通过后颁布的措施规定(核对无误:每个人都同意记录的内容)；</li><li>确定下一次会议召开的时间与地点。</li></ol></li></ol><ol><li><p>会议结束以后:</p></li><li><ol><li>组织者应该补写本次会议的备忘录,然后分发给与会者；</li><li>每个人应该按照会议上面协商通过的决议做事。</li></ol></li></ol><ol><li><p>在这些非正式会议上,学生们会表现出某些极其常见却令人厌烦的习惯,这些习惯包括以下几点:</p></li><li><ol><li>没有充分重视最后期限的重要性;</li><li>没有尊重导师的时间,导师时间紧迫,事情繁多(你只是众多的事情中的一件);</li><li>在最后一分钟提出很多要求,而不给导师时间去阅读、思考和提问等；</li><li>通常情况下,期待导师第二天就阅读你的每一稿;期待由导师来安排一切事情;</li><li>不听取导师的意见就擅自组织一些事情(从某种程度上讲、独立精神固然重要,但是你要保证自己的独立精神是恰到好处才可以)。</li></ol></li></ol><h2 id="如何与导师相处"><a href="#如何与导师相处" class="headerlink" title="如何与导师相处"></a>如何与导师相处</h2><p>攻读博士学位期间,不同类型的策略可以帮助你处理与导师的关系,使双方保持愉快。但是并不是每个人都意识到这些策略的重要性并将其付诸实施这些策略包括:</p><ol><li>互相帮助:比如你的导师需要一本参考书,这本参考书不容易找到,而你却帮老师找到了这本资料。然后反过来你可以请求导师给你一些有关工作申请的建议;</li><li>表明你很珍惜导师的知识和经验；</li><li>努力按照导师的方法做事情，但是制定出标准和日期,这样到期你就可以按照标准检验与衡量该方法成功程度了(尤其是在你极其不愿意按照导师的方法做事的时候)；</li><li>不要因为不喜欢就拒绝做某件事情,而是主动提出可选择项作为参考;</li><li>搜索自己导师的研究成果——令人吃惊的是,尽管其导师的研究成果可能是该领域内最有价值的参考资料,很少有学生会这样做;</li><li>允许自己导师有人性化的一面——包容他身上的弱点与不足,充分利用导师身上的优势与强势。</li></ol><h2 id="学生应该做的事情"><a href="#学生应该做的事情" class="headerlink" title="学生应该做的事情"></a>学生应该做的事情</h2><ol><li>你的博士学位论文进展是否正常,有两个标志:标志之一在于你一直在发现新想法,这些想法是你导师以前所不知道或没有听说过的；另外一个标志就是导师发现你所有想法中至少有一个非常有意思,而且值得拿来进行研究讨论。</li><li>将你的发现写成提纲交给导师阅读,然后如果导师希望了解更多细节的话,你应该提供相关资料的全文,这样做不但很有帮助,而且对导师来说也显得很有礼貌。</li><li>针对这些想法,导师很有可能会提出反对意见,你需要做的就是找出导师提出这些意见的理由并且仔细斟酌,而不要躲进一个角落里生气懊恼。一位优秀的导师在其自身研究中，新的观念也会层出不穷,但是同时也会由于各种原因抛弃其中的大部分。现在你还只是一个学徒,要学会接受意见,改进研究方法。</li></ol><h2 id="学生的正当请求"><a href="#学生的正当请求" class="headerlink" title="学生的正当请求"></a>学生的正当请求</h2><p>如果需要用到专门的技巧,你可以向导师咨询(例如:会议文件的具体格式是什么?如何阅读一篇论文?如何在阅读时做笔记?)如果导师能够通过具体例子给你讲解，而不是只告诉你如何去做,那么这样将会非常有帮助</p><h2 id="学生需要向导师汇报的事情"><a href="#学生需要向导师汇报的事情" class="headerlink" title="学生需要向导师汇报的事情"></a>学生需要向导师汇报的事情</h2><p>你应该向导师汇报以下事情:</p><ol><li>你的工作状态﹔</li><li>你感兴趣的事情与你关注的事情;</li><li>外界观点:准确及时地向导师报告你从与他人谈话和他人论文中获取的反馈信息;指出具体的赞扬与批评;</li><li>决议与转折点(导师具有敏锐的洞察力,能够先发制人,制止你做出草率的错误决定,这点对你大有裨益)﹔</li><li>生活环境:告诉导师影响你工作的个人或实际事情,最好是在这些事情还没有成为主要问题之前就要告诉他们。</li></ol><h2 id="你能够自己做的事"><a href="#你能够自己做的事" class="headerlink" title="你能够自己做的事"></a>你能够自己做的事</h2><ol><li><p>另外一件很值得做的事情,就是成立一个非正式的委员会,组成人员包括学院内部与校外的教师与学生,他们可以为你完成博士论文提供指导意见或建议。关于这点﹐你必须铭记于心的是:这个委员会的人员只能起到辅助你的导师的指导作用,但是不能够取代导师的地位</p></li><li><p>总之,和导师相处时,你需要遵守以下几条最重要的原则。</p></li><li><ol><li>诚实﹔</li><li>善于表达(讲出你的意思与需要);提供消息(让导师知道)﹔</li><li>尊敬导师﹔</li><li>成熟(对自己负责）。</li></ol></li></ol><h2 id="事情不顺利的时候-你的应对策略"><a href="#事情不顺利的时候-你的应对策略" class="headerlink" title="事情不顺利的时候,你的应对策略"></a>事情不顺利的时候,你的应对策略</h2><ol><li><p>如果你的导师忙于其他事情,你可以用非正式委员会弥补这个空缺,向他们咨询意见,但同时要向导师汇报你工作的进展状况。</p></li><li><p>真正严重的问题包括下面所列的问题:</p></li><li><ol><li>歧视:性别歧视﹑种族歧视等。大多数学校都有相关程序处理这些歧视问题。不管你愿不愿意卷入正式程序当中,你应该另找导师。</li><li>知识产权问题:“吸收”或者窃取他人作品,阻挠研究进程、禁止发表研究结果。关于这点,良好的习惯会很有帮助,比如说,让别人了解你在研究什么项目﹐并且及时记录阶段性成果,但是仅仅这样做是不够的。</li><li>无法沟通:不管什么时候,你努力去跟导师沟通,你都不成功。</li><li>骚扰:性骚扰、以强凌弱、诽谤﹑麻木不仁。同样,大部分学校对这类事件也会有相关规定,通过相关程序处理此类问题,或者至少有一位专业人士帮你处理。</li></ol></li></ol><ol><li><p>如果发现自己身上发生上述事情,你必须极其小心地开始行动，并且采取一定的方式方法。你需要做的事情如下所列:</p></li><li><ol><li>弄清楚学院是如何协调导师的指导监督工作的；调换导师必须经过一定的程序;重点在于,一旦学校接收你为本校学生，它就有义务找一位导师指导监督你的学位课程。也许学校有关欺凌或骚扰的适当的政策来约束或禁止此类行为。谨慎地做完这方面的调查。</li><li>建立一个档案:记录事件,或保存电子邮件等。尽力客观地记下事实，包括日期和细节等。如果真的出现问题,事实会说明一切。</li><li>秘密向第三者咨询。一般程序规定，要求有一个指定的第三者、一个“第三者证人”(其主要任务为回顾师生间的指导关系的进展状况）、一位研究生辅导员教师(负责管理所有研究生)、一位教授或者研究院主任、一位同等机会的干事、研究生学院院长。有的时候，你可以在学院找一个人,比如一位善良的教授,他很同情你,并且了解真相。有的时候，找一位不是学院内部的人讨论这些问题,会更容易-一些。无论如何,你要选择一位受人尊敬的学者,他要经验丰富，并且富有同情心。和人谈论这件事的时候,你要尽量镇静，尽量客观,并且带着你的文件,向他咨询建议,认真听取他人的意见。</li><li>邀请一位第三者做顾问(不一定和你咨询建议的人是同一个人)。一般你要邀请一位资深的长者或者领导做自己的代表。这个人能够和你一起参加导师见面会,因此弄清楚事情的真相,这个人有资格和你的导师交涉,或者帮助你通过相关程序解决问题。选择第三者时要非常慎重,听取人家的意见和建议。</li></ol></li></ol><h2 id="为使导师见面会有效而设计的一个简单的规划"><a href="#为使导师见面会有效而设计的一个简单的规划" class="headerlink" title="为使导师见面会有效而设计的一个简单的规划"></a>为使导师见面会有效而设计的一个简单的规划</h2><ol><li>提交一份供讨论的文件:在见面会前一个星期,给导师送上一份文件,这份文件可以是进展报告,可以是学习计划，可以是你正在阅读的文献的批评,或者加注的参考书目,可以是资料,或者会议论文的草稿等,一切可以代表你在做的课题的内容的文件都可以。当然,你有一些具体的问题需要讨论会对你更加有好处,而且准备一些材料也有助于集中精力思考。见面会上,你要随身带上供讨论文件的复印件。</li><li>提供关键的论文:会议之前,把你认为对导师很重要的论文寄给他,尤其是当你希望讨论这些论文的时候,就更有必要事先给导师送上一份。同时确保每一处引用都已经在文中注明了。给导师提供论文,也是你对导师的礼貌的体现,这些文章在手边,也方便讨论。</li><li>准时出席:如果你迟到了,就要有所表示,例如,你可以带些巧克力饼干给导师品尝。</li><li>写下你的目标:弄清楚你希望从导师见面会中得到些什么不管是技术方面、管理方面还是感情方面的指导性建议。将这些需求按照优先顺序排列,在见面会之前给自己列出这样一个清单。这样能在开会之前找到些有趣的话题讨论,因为如果你没有想法的话,就要准备提出些问题,这样才能够和导师开展讨论。</li><li>同你的导师确认日程:查明你的导师要出席的会议;安排好日程。</li><li>展示自己最好的一面:在发言之前专心听讲,认真思考。做好准备,坦诚地向导师汇报自己的工作进展。提出一些显而易见的问题——这些问题虽然在你看来会有些愚蠢,但是实际上很少是这样。大家都很容易忽视最明显的问题。集中精力想出新的观念,而不是太注重感情。信任你的导师,不要把事理解成私人性质的。如果不喜欢导师提出的建议,你可以提出与其相反的建议,这样可以暴露出你思维中的矛盾,利于导师对你进一步的指导。</li><li>做好笔记。</li><li>预定下一次的见面会:在离开之前,和导师约定好下一次见面会的日期。做好预备的日程安排。</li><li>会议结束后，总结出会议达成的计划议案,以电子邮件的形式发给导师:在会议结束后,立即写出一份计划议案,包括你的计划与你导师的计划;可能的话，最好在其中包括最终期限,并且发电子邮件给所有有关人士,要求他们确认你的总结正确无误。其中包括下次会议召开的日期。</li></ol><h2 id="一些常见的会破坏你与导师关系的行为"><a href="#一些常见的会破坏你与导师关系的行为" class="headerlink" title="一些常见的会破坏你与导师关系的行为"></a>一些常见的会破坏你与导师关系的行为</h2><ol><li>隐瞒(真正存在的问题或者想象的问题）</li><li>忽视(你没有懂得的建议或者你不喜欢的忠告)</li><li>混淆(事业与娱乐或者事业与私人问题)</li><li>背后议论人长短(关于你导师或同事的流言飞语)</li><li>贬低(导师、学院﹑学校)</li><li>避开或绕开(你的导师,没有询问导师的意见就做决定)想当然地认定(某件事的意思;你有资格做的事情)</li><li>犯罪(违法或者不道德的行为——这些与上述所列的过失属于不同的类型)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议3：学术体制</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE3%EF%BC%9A%E5%AD%A6%E6%9C%AF%E4%BD%93%E5%88%B6/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE3%EF%BC%9A%E5%AD%A6%E6%9C%AF%E4%BD%93%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E5%A4%B4%E8%A1%94%E5%92%8C%E5%B2%97%E4%BD%8D">头衔和岗位</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%BA%86%E8%A7%A3%E4%BD%A0%E7%9A%84%E2%80%9C%E6%95%8C%E4%BA%BA%E2%80%9D">了解你的“敌人”</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%BA%BA">人</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E6%BB%A5%E7%AB%BD%E5%85%85%E6%95%B0%E8%80%85">滥竽充数者</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E6%A0%A1%E5%A4%96%E8%AF%84%E5%AE%A1%E4%BA%BA">校外评审人</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%BD%A0%E7%9A%84%E5%AF%BC%E5%B8%88">你的导师</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E5%85%B6%E4%BB%96%E5%AD%A6%E7%94%9F">其他学生</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E7%89%A9">物</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%B9%A6%E7%B1%8D">书籍</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87">期刊论文</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E5%9B%A0%E7%89%B9%E7%BD%91">因特网</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%B9%A0%E6%83%AF">习惯</a></p><p>[习得无助感(learned helplessness)](<a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E4%B9%A0%E5%BE%97%E6%97%A0%E5%8A%A9%E6%84%9F">https://zhuanlan.zhihu.com/p/401680676/edit#习得无助感</a>(learned helplessness))</p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E8%A1%A8%E7%8E%B0%E6%80%A7%E8%A1%8C%E4%B8%BA">表现性行为</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E6%97%B6%E9%97%B4%E5%A4%A7%E6%8A%8A%E8%8A%B1%E8%B4%B9%2C%E7%A0%94%E7%A9%B6%E6%AF%AB%E6%97%A0%E8%BF%9B%E5%B1%95">时间大把花费,研究毫无进展</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680676/edit#%E5%85%B6%E4%BB%96%E4%B8%8D%E5%90%8C%E7%A7%8D%E7%B1%BB%E7%9A%84%E4%B8%8D%E8%89%AF%E4%B9%A0%E6%83%AF">其他不同种类的不良习惯</a></p><hr><h2 id="头衔和岗位"><a href="#头衔和岗位" class="headerlink" title="头衔和岗位"></a>头衔和岗位</h2><p>有关高校里面的各种学术头衔和工作岗位，此处略过</p><h2 id="了解你的“敌人”"><a href="#了解你的“敌人”" class="headerlink" title="了解你的“敌人”"></a>了解你的“敌人”</h2><p>你的敌人不仅仅是指“狼群”(比如你做演讲时的遇到的不友好的观众)。你的敌人也并不都是指人。有的物也是你的“敌人”;有的“敌人”甚至是你自己的习惯。</p><h2 id="人"><a href="#人" class="headerlink" title="人"></a>人</h2><h3 id="滥竽充数者"><a href="#滥竽充数者" class="headerlink" title="滥竽充数者"></a>滥竽充数者</h3><p>棒球帽对于指导教师识别滥竽充数者是很有用的,因为通常情况卞,这能够让他们有理由相信这个学生的博士论文应该由别人来指导(最好是由一个令自己讨厌的同事来指导)。一般情况下,滥竽充数者除了戴棒球帽之外,还会伴有闲散等坏习惯,这类人认为研究就是浪费时间,因此倾向于去因特网上搜索,而不去做适当的文献调查；更加令人吃惊的是,他们非常喜欢设计蹩脚的调查,其中往往夹杂一些惨不忍睹的问卷,或者一些完全没有现场记录的所谓采访稿件</p><h3 id="校外评审人"><a href="#校外评审人" class="headerlink" title="校外评审人"></a>校外评审人</h3><ol><li>有两种主要类型的校外评审人。事实上,校外评审人是你的朋友，但是几乎没有人能够体会理解到这个深度,所以他们就被当做敌人了。他们这类人要么会毁掉你,要么使你更加坚强。</li><li>明智的指导教师在你刚开始写论文时就会指出一些可以充当校外评审人的专业人士,这样你就会在学术上适当与他们保持足够的距离，将来可以请他们担任自己的校外评审人,他们也会比较公平地对待你,虽然不一定比较温和但是能够保证公平公正。</li><li>你的论文最好看起来像是专业人士的作品,注意重要的细节(你引用的文章是不是完整?是不是正确?)尔需要用专业语言写作,就像写给另外一个专业人士-样，避免用教材或通俗文摘中所用的简单语言。</li></ol><h3 id="你的导师"><a href="#你的导师" class="headerlink" title="你的导师"></a>你的导师</h3><p>你的导师扮演着平衡者的角色。一方面,你作为一个有潜质的学术新人,将来可能会发表文章,成为导师与外界的混乱、黑暗、对手等斗争中的有力支持。另一方面,你需要时间和精力,这些又很匮乏，而你又可能花费时间与精力去做其他短期盈 利的事情,比如说，申请基金﹑发表期刊文章、专注学术政治,还有在家自己动手做些事 情或偶尔照顾一下家庭。</p><p>如果导师清楚地告诉你哪一步要做什么的话,你可能会感觉良好,但是你工作了以后呢?工作了以后你要单独完成很多事情，没有导师指导你,你该怎么办呢?</p><h3 id="其他学生"><a href="#其他学生" class="headerlink" title="其他学生"></a>其他学生</h3><p>研究生最喜欢的事情之一就是与他人做比较然后因为“攀比”结果不满意而沮丧。这种思维游戏有益于销售巧克力(巧克力可以使人心情愉悦)除此没有什么其他益处。避免这种思维的有效方法之一就是撤谎(“噢!进展很顺利,谢谢关心——很多有趣的发现,写作也很顺利”)。这样就能非常迅速地阻止宿命论者向你传播其信条，劝说他们不再打扰你。</p><h2 id="物"><a href="#物" class="headerlink" title="物"></a>物</h2><h3 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h3><p>书籍有很多种。教科书适合于本科生，其内容都是很简单的叙述。作为研究生你应该阅读真正的书籍。你一般会在期刊文章中或专业书籍中才能看到这样复杂的文字。在大多数学科中,书籍只能提供最前沿最时尚的信息；书籍是非常有用的参考资源,尤其是那些曾经改变某个领域的经典书籍。但这些书并不是唾手可得的,需要通过寻找得到.</p><p>书籍有一点对你很重要也很有帮助,那就是可以让你先对这个领域有个大概的了解、然后再决定是否继续钻研下去。正常情况下,书中的一章涵盖了你所需要了解的这个层面的 所有细节——比如,  你不知道要不要在研究中涵盖博弈论,正在犹豫不决。这时候,关于这个论题的论文或者书籍对你来说篇幅有点长,很有可能使你失去耐心或者颇受打击，而书的一章可能对你比较合适。而且如果你决定沿着这个方向继续研究的话，这些章节可以提供你所需要的基本概念,并且这些概念又是你弄明白更多的高级文献所必须了解的知识。</p><h3 id="期刊论文"><a href="#期刊论文" class="headerlink" title="期刊论文"></a>期刊论文</h3><p>来在哪个地方就会大有用途。如果你的研究领域是理科类的,那么读一些类似于《新科学家》(New Scientist )、《科学》( Science),《自然》( Nature）和《科学美国人》(Scientific  American)等期刊,会对你很有裨益,这样可以及时了解世界其他地方都发生了什么事情。不论你学的是什么学科,尔应该了解本专业最主要的期刊是什么刊物,并且定期阅读。</p><p>作为一个博士生,你应该把目标放在发表论文上面。发表论文并没有人们想象得那么困难,关键是你必须知道自己在做什么。详细情况请参考关于如何在期刊上面发表文章的章节,了解这些细节就是个不错的开始。同样也要仔细阅读你打算投稿的杂志的“投稿须知”(Notes to contributors)部分。很多人都不屑于细读这部分内容，而因为懒惰付出了代价。</p><h3 id="因特网"><a href="#因特网" class="headerlink" title="因特网"></a>因特网</h3><p>你永远无法了解事实上发生的事情与人为放  在网络上的内容反映的情况之间存在怎样的关系。对于你的研究领域内的系统信息而言，你需要采用适当的信息来源，比如文献数据库(bibliographic  databases),这些能够反映出最新发现的状况。你应该了解自己专业的主要信息源,要和相关的图书管理员处好关系,因为我们查资料的时候,他们有时候能够帮上大忙。学会如何做研究也非常有用处。</p><h2 id="习惯"><a href="#习惯" class="headerlink" title="习惯"></a>习惯</h2><h3 id="习得无助感-learned-helplessness"><a href="#习得无助感-learned-helplessness" class="headerlink" title="习得无助感(learned helplessness)"></a>习得无助感(learned helplessness)</h3><p>博士研究生尤其倾向于有这种无助感。一般情况下,一段时间内,他们感觉到学业没有进展就会感到没有必要继续下去了。</p><p>设计周全的研究有很多优点,其中之一便是在每个研究阶段,你都准确地知道自己在做什么工作,而且为了迎接即将到来的每种可能性,你都能做到未雨绸缪,知道如何应对。这样做的缺点便是你可能提前知道自己将要花很长的时间去啃资料,但是这样也有好处,既然已经知道要做什么，你就应该清楚地知道如何去啃掉资料以及如何处理最后得到的结果。如果不能提前知道这一点，那么你就需要重新考虑你的研究设计了。</p><h3 id="表现性行为"><a href="#表现性行为" class="headerlink" title="表现性行为"></a>表现性行为</h3><ol><li>表现性行为经常伴随着习得性无助出现。工具性行为指能够帮助我们实现目标的行为。</li><li>表现性行为指那些旨在向他人表现你是什么样的人的行为。一个学生坐在图书馆里辛苦研读6篇其研究领域中的核心教材,其行为被称为工具性行为。</li><li>评审人一般给你的论文打分,却不关心你是怎样的人;有人在两天内成功写出一篇很优秀的文章,有人也许花了几个月的时间结果只是一篇普普通通的文章,这种情况下,前者会比后者更受欢迎,分数也会更高。所以要懂得适时地运用表现性行为。</li></ol><h3 id="时间大把花费-研究毫无进展"><a href="#时间大把花费-研究毫无进展" class="headerlink" title="时间大把花费,研究毫无进展"></a>时间大把花费,研究毫无进展</h3><p>出现这种事倍功半的情况,是有很多不同的原因的。</p><ol><li><p>原因1:</p></li><li><ol><li>虽然你花了很多时间,但是研究却毫无进展,原因在于你根本不知道自己在做什么,也不知道这样做是为了什么。</li><li>在这样的情况下,重新设计你的研究不失为一个好主意。这指的是:你需要修改研究课题,确保不是把所有赌注放在哪个具体的</li></ol></li></ol><ol><li><p>原因2:</p></li><li><ol><li>你忙于做些无形的准备活动。这些活动对于做好研究尤其重要;从定义上看，虽然付出努力,但是却不能够创造任何可见的成果。比方说,你忙于准备文具,或者你在做文献准备,查看有没有人已经做过类似的研究;或者你在做大量泛读，了解一些背景知识(相对于精读而言)。这些事情必须要做。文具很重要，背景知识也不可或缺。</li><li>这里大力推荐泛读﹐因为很多一流的作品都来自于将一部作品应用于另外一部作品。</li></ol></li></ol><h3 id="其他不同种类的不良习惯"><a href="#其他不同种类的不良习惯" class="headerlink" title="其他不同种类的不良习惯"></a>其他不同种类的不良习惯</h3><p>研究者面临着很多令人头痛的坏习惯,比如说不善于安排时间、习惯于拖拖拉拉、不愿意费力气去了解这一行的关键。为了能够适当处理自己所有的问题和困难,你需要学着去找出这些不良习惯然后加以改正。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议2：攻读博士学位的程序</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE2%EF%BC%9A%E6%94%BB%E8%AF%BB%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E7%9A%84%E7%A8%8B%E5%BA%8F/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE2%EF%BC%9A%E6%94%BB%E8%AF%BB%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E7%9A%84%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E5%8F%96%E5%BE%97%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E7%9A%84%E7%A8%8B%E5%BA%8F">取得博士学位的程序</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E6%8F%90%E4%BA%A4%E8%AE%BA%E6%96%87%E5%92%8C%E7%AD%94%E8%BE%A9">提交论文和答辩</a></p><p>[候选人申报表(The candidate declaration form)](<a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E5%80%99%E9%80%89%E4%BA%BA%E7%94%B3%E6%8A%A5%E8%A1%A8">https://zhuanlan.zhihu.com/p/401680278/edit#候选人申报表</a>(The candidate declaration form))</p><p>[年度报告(The annual report)](<a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E5%B9%B4%E5%BA%A6%E6%8A%A5%E5%91%8A">https://zhuanlan.zhihu.com/p/401680278/edit#年度报告</a>(The annual report))</p><p><a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E5%8D%87%E7%BA%A7(Transfer)">升级(Transfer)</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E4%B8%80%E4%BA%9B%E6%9C%89%E7%94%A8%E7%9A%84%E4%B9%A0%E6%83%AF">一些有用的习惯</a></p><p><a href="https://zhuanlan.zhihu.com/p/401680278/edit#%E5%A1%AB%E5%86%99%E8%A1%A8%E6%A0%BC">填写表格</a></p><hr><p>就程序而言，你需要记住有关正义战争论的一些观点。如果你一下子想不起来，那么请记住关键的一条：完全没有希望获胜的战争就没有进行的必要。在你攻读博士学位期间，你能说服学院改变程序的可能性可以说是零，因此你需要做的是与该体制达成一致。第一种方式就是把学院的程序看成是为了今后学术生涯所需的重要技能而进行的有用实践。无论你选择从哪个角度来看待这些程序,制定这些程序的目的则是要以一系列标准来衡量你,而绝不会考虑你对于这些标准能否正好体现你的才能的意见。你需要做的是先了解这些标准然后使你自己真正、完全符合这些标准。</p><h2 id="取得博士学位的程序"><a href="#取得博士学位的程序" class="headerlink" title="取得博士学位的程序"></a>取得博士学位的程序</h2><h2 id="提交论文和答辩"><a href="#提交论文和答辩" class="headerlink" title="提交论文和答辩"></a>提交论文和答辩</h2><p>博士学位的取得是一个漫长的过程,但最终结果却只是由一篇论文和一次讨论而定。论文即你写的博士论文,而讨论即答辩,参加答辩时你会被一组睿智、知识渊博、难对付的评审人问及一些尖锐的问题。学术体制只通过测评你的论文和答辩中的表现来决定你是否能获得博士学位,你其他的努力都与之毫不相关。人们对此通常持三种主要观点：</p><ol><li>你仍需重视各个阶段并恰当地处理,因为如果不这样做的话,你就不能到达提交论文和答辩这个阶段﹔</li><li>论文和答辩是整个过程中最为重要的﹐前面的阶段只要通过就可以了﹔</li><li>整套程序是为了你获得博士学位之后所从事的工作做的准备工作。</li></ol><p>第一种观点最为普遍,持这一观点的主要是学校的管理者和担心拿不到学位的学生(可能占博士生的大多数)。因为这样的想法能降低由于目标不高而轻易失败的风险,就像跨栏一样﹒如果不瞄准更高的自标,那么很容易刚开始就被绊倒。持第二种观点的人较少。其目的性较强,容易使人忽略中间程序。持第三种观点的人最少,但事实上这一观点对于博士学位本身和以后的学术生涯都是最有好处的。</p><h2 id="候选人申报表-The-candidate-declaration-form"><a href="#候选人申报表-The-candidate-declaration-form" class="headerlink" title="候选人申报表(The candidate declaration form)"></a>候选人申报表(The candidate declaration form)</h2><p>提交论文之前你需要通过一份表格正式通知你所在学院你已经做好准备,这个表被称为“候选人申报表”。它主要有两个目的:</p><ol><li>它要求你的导师保证你研究作品的质量，因为在为申报表签字的时候他们必须声明已经阅读过完整的稿件且肯定该作品有被考核的价值﹔</li><li>它促使组织开始为你选定评审人,这个过程可能需要花些时间,因为要递交简历,填好完整表格以及获得相关委员会的批准。</li></ol><h2 id="年度报告-The-annual-report"><a href="#年度报告-The-annual-report" class="headerlink" title="年度报告(The annual report)"></a>年度报告(The annual report)</h2><p>攻读博士的每个学年,大多数学院都会要求你的研究生导师每年交一个报告概括你当年所取得的进步,评估你继续完成学业的潜能并且提出你能否继续下一学年学习的建议。有个很明智的策略，即找出你上一年度的报告,在填写当年报告的时候清楚地写明你所取得的进步——委员会的相关人员很有可能会对照去年的报告来审查今年,有点像是在玩“找差别”的游戏。</p><h2 id="升级-Transfer"><a href="#升级-Transfer" class="headerlink" title="升级(Transfer)"></a>升级(Transfer)</h2><p>在进行论文和答辩之前、你要经历一个被称为“升级”的过程，它的全称是“注册升级成为博士生”,有时也被叫做“通过试用期”。这个阶段不论对学术还是行政来说都很重要。</p><p>升级是一个很重要的阶段,一般情况下涉及对你的表现做出一些真实的学术评定。这通常需要你创造两件东西。其一是一份文件,该文件能体现:(1)你做了很多细致的工作;(2)你了解了包括描述和介绍作品在内的相关学术技能。其二是临场的表现,在答辩会或是学院的研讨会上,你能够向大家介绍你的研究成果,演示你准备的文件上的很多东西,这里涉及口头演说的技巧。非常巧合的是，这两者对于论文写作和论文答辩都可被视为是十分有用的训练。这些文件准备工作和口头陈述是为了展示你的能力——不是要求你尽善尽美,也不是要把你的研究计划具体化。</p><h2 id="一些有用的习惯"><a href="#一些有用的习惯" class="headerlink" title="一些有用的习惯"></a>一些有用的习惯</h2><h2 id="填写表格"><a href="#填写表格" class="headerlink" title="填写表格"></a>填写表格</h2><p>一些有用的习惯,排列顺序不分先后:</p><ol><li>开始填表之前将每一份表格从头至尾通读一遍；</li><li>如果这份表格很重要而你只有一份的话﹑复印一份并先在这份复制的表格上填写,然后再抄写到原表上；</li><li>如果你不确定表格中某一栏该如何填写。那么请参考注释——绝大多数的表格都会附有注释,但是很多人却不愿意花工夫去看一下;</li><li>如果你觉得自己很怕填表,那么向他人求援;如果你的恐惧很严重﹐那么试着考虑请能治疗恐惧症的人来帮助你——这样做通常是快速而且令人愉快欣喜的。</li><li>填完表后将每一份填好的表格都复印,把这些表整齐的归档放好——它们会有助于提醒你如何填表,也是你上次申请的记录。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给研究生的学术建议1：你想攻读博士学位吗？</title>
      <link href="/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE1%EF%BC%9A%E4%BD%A0%E6%83%B3%E6%94%BB%E8%AF%BB%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E5%90%97%EF%BC%9F/"/>
      <url>/2021/11/22/%E7%BB%99%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%BB%BA%E8%AE%AE1%EF%BC%9A%E4%BD%A0%E6%83%B3%E6%94%BB%E8%AF%BB%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>强烈建议刚入学的或者即将入学的研究生（特别是博士生）阅读本文，博主读这本书的收获挺大的。</p><p><strong>目录</strong></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E7%9A%84%E6%9C%AC%E8%B4%A8%E4%B8%8E%E5%86%85%E6%B6%B5">博士学位的本质与内涵</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%8F%96%E5%BE%97%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E7%9A%84%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B">取得博士学位的标准流程</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%B0%86%E5%8D%9A%E5%A3%AB%E5%AD%A6%E4%BD%8D%E8%A7%86%E4%B8%BA%E4%BD%A0%E8%A6%81%E5%88%B6%E4%BD%9C%E7%9A%84%E7%B2%BE%E5%93%81%E6%A9%B1%E6%9F%9C">将博士学位视为你要制作的精品橱柜</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E2%80%9C%E5%88%B6%E4%BD%9C%E6%A9%B1%E6%9F%9C%E2%80%9D%E7%9A%84%E6%8A%80%E5%B7%A7">“制作橱柜”的技巧</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%BF%85%E9%9C%80%E7%9A%84%E6%8A%80%E8%83%BD">必需的技能</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%AD%A6%E4%BC%9A%E8%BF%90%E7%94%A8%E5%A4%96%E4%BA%A4%E6%89%8B%E8%85%95">学会运用外交手腕</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E6%89%BE%E5%AF%B9%E4%B8%8E%E4%BD%A0%E4%B8%80%E8%B5%B7%E5%96%9D%E5%92%96%E5%95%A1%E7%9A%84%E9%82%A3%E4%B8%AA%E4%BA%BA">找对与你一起喝咖啡的那个人</a></p><p><a href="https://zhuanlan.zhihu.com/p/401679022/edit#%E5%B7%A5%E5%85%B7%E6%80%A7%E8%A1%8C%E4%B8%BA%E5%92%8C%E8%A1%A8%E7%8E%B0%E6%80%A7%E8%A1%8C%E4%B8%BA">工具性行为和表现性行为</a></p><hr><h2 id="博士学位的本质与内涵"><a href="#博士学位的本质与内涵" class="headerlink" title="博士学位的本质与内涵"></a>博士学位的本质与内涵</h2><ol><li>如果纯粹从功利主义的角度来说，博士学位是一个资格证明，它表明你的研究能力足以被某大学录用。如果你想要谋求一份大学老师的工作，有博士学位是最好不过的。如果你想做服务于工业的研究人员,这个学位也是非常有帮助的。博士学位不仅在全世界都能得到认可，而且还具有相当的影响力。也就是说，一个国家的博士即使到了另外一个国家也能得到认可。另外，博士学位通常能保证你达到更高的收入水平。</li><li>从专业的角度来说，博士学位意味着你做过相当多的研究,将之写成论文，并和专业的学者进行过讨论。这一切都可以证明你有独立做研究的能力。虽然有导师的帮助和建议。但至少从理论上来讲，博士项目是一项由你自己做主的工程</li></ol><h2 id="取得博士学位的标准流程"><a href="#取得博士学位的标准流程" class="headerlink" title="取得博士学位的标准流程"></a>取得博士学位的标准流程</h2><p>首先你选择一个研究的主题,然后找一个愿意做你导师的人。之后则是在导师的研究所签合同，办妥攻读博士学位相关的一系列手续。办完这些手续之后，在接下来的一两年里，你就应该开始着手对你的选题进行研究。只有通过这段时间的研究，你才能够了解自己的进展如何，是否能将博士学业进行到底。如果一切进展顺利的话，你还有必要继续做一到两年的研究。到攻读博士学位的第三或第四个年头的时候，你需要就你的研究选题写出一个全面的文献报告(即博士学位论文，通常在300页左右)。这篇论文会提交至专家小组审阅，之后这组专家会对你的论文提出若干问题，考察你是否对这个选题有足够的了解。一般而言，他们会得出这样的结论：你的论文需要做一定改动。如果在规定的时间内你所做的改动让他们满意了，你就可以被授予博士学位了。</p><h2 id="将博士学位视为你要制作的精品橱柜"><a href="#将博士学位视为你要制作的精品橱柜" class="headerlink" title="将博士学位视为你要制作的精品橱柜"></a>将博士学位视为你要制作的精品橱柜</h2><ol><li>个人对研究领域的兴趣和选题在伦理道德上的重要性这些方面通常不会被列入这个清单之中。在你的论文中花大量篇幅详细讨论这些问题并没有任何意义——你被授予博士学位仅表示大家承认你“制作橱柜”的技巧已经达到杰出手工艺者的水平</li><li>在你的学科领域内会有不少介绍研究方法的书，翻看这些书的目录对你来说也有一定的意义，因为目录中会涉及许多学科领域内的重要话题。你所在大学的博士生规章制度对你来说应该也是很有帮助的。</li></ol><h2 id="“制作橱柜”的技巧"><a href="#“制作橱柜”的技巧" class="headerlink" title="“制作橱柜”的技巧"></a>“制作橱柜”的技巧</h2><p>大部分学科都需要下面列出的这些技能，但也有个别不同情况。</p><ol><li><p>使用学术语言</p></li><li><ol><li>正确使用专业术语</li><li>注意标点符号和语法等细节问题</li><li>注意排版设计(空格、版面设计、标题的格式)从而让你的论文理解起来更容易</li><li>能够合理组织语言并且表达出清楚连贯的论证,包括学会使用“路标”的方法——比如用若干小标题帮助读者理清文章的结构</li><li>使用正确的学术“语言”进行写作</li></ol></li></ol><ol><li><p>了解背景文献知识</p></li><li><ol><li>正确引用以往一些重要的文献,以表明你读过这些文章，并批判性地对它们做出评价</li><li>参考文献要能准确地反映出文献的发展过程——从以往的重要文献到当前的研究报告</li><li>辨别哪些文献对于你的博士项目是关键的，揭示这些文献对你的选题的意义所在，以及他们和你的选题有何不同从其他学科引用的相关文献和概念</li><li>把所有引用的文献组织成一个具有连贯性和批判性的结构。以表明你不但弄明白了这些文献，而且你也清楚地了解什么是重要的，什么是不重要的</li></ol></li></ol><ol><li><p>研究方法</p></li><li><ol><li>了解在你的学科领域使用的主要研究方法，包括如何搜集数据、如何做记录以及如何分析数据</li><li>了解在你的学科领域内,哪些内容可以构成“证据”(论据)，而哪些内容是被看做已知的知识</li><li>熟知并适当运用至少一种研究方法</li><li>批判性地分析你所在学科领域的某一种标准研究方法,以表明你知道该方法的优,点和局限所在</li></ol></li></ol><ol><li><p>理论</p></li><li><ol><li>通晓你所在学科领域内的关键理论流派和理论概念</li><li>了解这些理论如何在你的研究选题里体现</li><li>能够为你所在领域的理论争论贡献出一些有价值的东西</li></ol></li></ol><ol><li><p>其他方面</p></li><li><ol><li>独立做上面提到的全部工作的能力,而不是简单地做导师让你做的事情</li><li>清楚地意识到你所做的一切与你的学科之间的关系,以及对这个学科有何贡献</li><li>对你所在的学科有全局性的概观</li></ol></li></ol><h2 id="必需的技能"><a href="#必需的技能" class="headerlink" title="必需的技能"></a>必需的技能</h2><p>重点是可迁移性技能</p><h2 id="学会运用外交手腕"><a href="#学会运用外交手腕" class="headerlink" title="学会运用外交手腕"></a>学会运用外交手腕</h2><ol><li>在开始的一段时间里，你有必要规矩和低调一些,只有这样你才能够学到一些想要的东西。</li><li>一个很重要的技巧是：对于一个问题,要知道什么时候不加计较，而什么时候应该坚持到底(机敏、礼貌但坚定)。否则你会发现自己赢了小战役却丢了大战场。</li><li>还需要记住的一点是：大部分博士生都清楚地知道他们想要什么，而不是他们需要什么——这两者之间的差别有时候是巨大的。所以我们需要掌握以下重要技巧。</li></ol><h2 id="找对与你一起喝咖啡的那个人"><a href="#找对与你一起喝咖啡的那个人" class="headerlink" title="找对与你一起喝咖啡的那个人"></a>找对与你一起喝咖啡的那个人</h2><ol><li>要找到你真正需要了解的东西,最好的方式是找对陪你喝咖啡的那个人，然后征求他们的宝贵建议(机智巧妙地)。在是这个合适的人选?答案是“一个很有知识的人”，这也就是说大多数情况下这个人不大可能是另外一个在读博士生</li><li>有必要对他们给予正确的评价，并且对他们给出的建议要保密，除非他们特别要求你公开说明。最有用的知识往往是人们最不愿意提供的，例如想找到合适的可以提供帮助的人选的时候，究竞谁好谁坏这样的问题。</li></ol><h2 id="工具性行为和表现性行为"><a href="#工具性行为和表现性行为" class="headerlink" title="工具性行为和表现性行为"></a>工具性行为和表现性行为</h2><ol><li>工具性行为是指一系列为了达到预定目标而进行的行动。打个比方来说,如果你的目标是要学会开车,那么工具性行为就包括报名参加驾驶培训班、购买一份交通法规等。</li><li>表现型行为则是指那些可以表现出来,让别人看到你是什么样的人的举止。比如坐在报告厅的最靠前的位置,以很引人注意的方式记大量的笔记，以显示你对待学习的态度是十分认真的。</li><li>工具性行为和表现性行为是同等重要的。从我们的经验来看、学生们往往比较擅长一些工具性行为,而不幸地在一些表现性行为方面表现得十分糟糕。出现这种情况的原因通常是因为没有人给他们解释他们应该如何表现。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读时如何做笔记？</title>
      <link href="/2021/11/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%97%B6%E5%A6%82%E4%BD%95%E5%81%9A%E7%AC%94%E8%AE%B0%EF%BC%9F/"/>
      <url>/2021/11/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%97%B6%E5%A6%82%E4%BD%95%E5%81%9A%E7%AC%94%E8%AE%B0%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="文献阅读时如何做笔记？"><a href="#文献阅读时如何做笔记？" class="headerlink" title="文献阅读时如何做笔记？"></a>文献阅读时如何做笔记？</h1><p> 具体点：哪些方面的内容需要记下来？以什么样的方式记下来利于以后查找利用？ </p><h2 id="Answer-1"><a href="#Answer-1" class="headerlink" title="Answer #1"></a>Answer #1</h2><p>记笔记每个人有每个人的方法，不说优劣，但的确有些方法效率会更高一些。 记笔记不仅仅是记录，还有笔记查找和整理的问题。 如果笔记仅仅只是记录，那也不比不记好不了多少，还省点力气写字。  笔记有一个非常关键的作用是能把文章串接起来，乔布斯所说的 Connect the dots. 通过笔记，将不同文献的知识点串接起来，就很容易激发我们创新的意识，然后idea 便会随之喷薄而出。  你还记得上次是什么时候整理笔记的吗？你是怎么做的呢？ 你花了多少时间？ 这只视频里介绍的方法，能让你随时翻阅你做过的笔记，甚至都不用打开文献即可查看。省时省力。 只能帮你到这儿了！</p><blockquote><p>作者：青藤学术<br>链接：<a href="https://www.zhihu.com/question/19600673/answer/1958042394">https://www.zhihu.com/question/19600673/answer/1958042394</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="Answer-2"><a href="#Answer-2" class="headerlink" title="Answer #2"></a>Answer #2</h2><p>我会先通过阅读用自己的话整理出一个框架，文献的话用不同颜色来做不同的标记，比如绿色用来标记什么样的内容在我这里是固定的，就会很节省时间，也是及时在旁边做一些摘要，方便理解。我还有自己的文档，同一类型的文章放在一个文档之中，最好善用一级标题和二级标题，这样可以在之后方便寻找。</p><blockquote><p>作者：平凡的巴答<br>链接：<a href="https://www.zhihu.com/question/19600673/answer/1912505181">https://www.zhihu.com/question/19600673/answer/1912505181</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="Answer-3"><a href="#Answer-3" class="headerlink" title="Answer #3"></a>Answer #3</h2><blockquote><p>作者：朱小洛<br>链接：<a href="https://www.zhihu.com/question/19600673/answer/47262208">https://www.zhihu.com/question/19600673/answer/47262208</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>正在读Reading的我偶尔看到这个自己在很久以前关注了的题目有了新的回答，突然感慨良多。想想如果从一开始自己能有一个较为系统的笔记整理，那可能所读之物能更有成效吧。现在想好好写个长答案，把自己一直以来的一些经验总结一下，也算是对自己方法的反省。</p><p>在英语文献阅读上，自己尝试了很多方法。作为一个Master of HR新生（第一学期），最折磨自己的事情莫过于根本停不下来的Reading，一开始，超级多的Required Reading挤满了我正常的休息时间，搞得我几近崩溃，而且重点是经常看得一头雾水！作为一个“强迫症”患者，有一种读了就一定要搞懂的仪式感（当然来不及的时候强迫症会缓和很多），而且把每次学术阅读都当作提升英语水平的机会去认真理解。如果你和我一样是这样喜欢<strong>把Reading理解透彻，不惜消耗很多的时间和精力去钻研，来达成最后完全理解的成就感，以及获得知识的满足感，</strong>那我的方法可能会适用于你：D。（当然，时间较为紧迫的时候完全可以精简——去掉大部分查字典的步骤）</p><p>首先，所谓磨刀不误砍柴工，我们来了解几个能对学术阅读有用的工具/方法：</p><p><strong>1、The Academic Word List</strong></p><p>（简称AWL,即英语学术词汇表）包含了学术英语中使用频率最高的570个词目。虽然这对学术写作的帮助更多，但是对学术阅读的帮助也是巨大的，建议熟悉这些单词，不用完全背诵，因为你会在之后越来越多的学术阅读中，越来越熟悉这些学术词汇。</p><p><strong>2、好的电子词典</strong></p><p>读完奶爸的那本书，立马下载了欧陆词典，然后安装了外置的朗文现代以及牛津高阶。越来越喜欢用朗文，因为发现用英语解释英语往往才能更好地表达英文单词的意思，而且很多时候朗文的长度和分类比牛津短，但是表达的内容却不必牛津少，这让我觉得很有意思。</p><p>还有一个使用电子词典取词的好处是，因为现在很多文献都可以找到电子文档，直接可以使用屏幕取词，比起看到一个输入一个，效率会更高。</p><p>By the way, 我觉得欧路更新以后没有之前那个版本好用了，有这样感觉只有我吗QAQ</p><p>**<br>3、云笔记**</p><p>现在有很多云笔记的软件，可以将你的笔记随时随地储存在云端，不用担心丢失。印象笔记和有道云笔记我都有用，都挺好的，可惜印象用多了也没舍得买会员，居然这个月到达了每月上限，这让我挺郁闷的……我也不知道我做了什么就上限了啊QAQ</p><p>言归正传，这些云笔记不仅可以使用文件夹/笔记本来分类，也可以用标签来索引，更可以用Ctrl+F来搜索关键词，我感觉比起手抄笔记，寻找信息的效率更高。云笔记出了记录最后的学习笔记，还可以分门别类地记录英语词汇、英语词组、好词好句等，以便复习以及不时之需。只是输入的时候，可能不太能像手写那般随心所欲……不过这个问题不存在在我身上，因为……我有数位板，哈哈哈哈哈……</p><p>咳咳，好的，言归正传，刀既然磨好了，那我们就来看看做笔记的过程。其实总的来说很简单，无非就是：<strong>阅读与总结</strong>。另一种说法，就是：<strong>输入与输出</strong>。对于读书来说，万变不离其宗。学以致用——将所学之物化为满腹经纶才是最高的境界。</p><p><em><strong>阅读理解部分</strong></em>，我推荐使用<strong>Staging</strong></p><p>这个方法是我在上正课之前读语言课程学到的，即，在一边（建议右侧）标注出所指的一段内容在整个推导的过程中，处于什么样的一个地位，如，论点arguement、原因reason、例证example、结果outcome、定义definition、主题topic等。</p><p>你不仅要理解内容，更要理解结构，因为结构能帮助你更好地理清思路，理解外国人的逻辑，从而真正地理解文献的内容，要知道外国人的思路很多时候与国人的思路有很大差别，而这些差别往往是导致理解困难的主要原因之一。在理清逻辑关系的同时，也能帮助你加深记忆，因为这不仅仅是翻译，而是将前后的关系搞清楚，那么推导也就不是什么令人困惑的过程了。</p><p>Staging的具体方法，我使用一下课本内容（Centre For English Teaching，2015）（应该不会侵权吧= =）</p><p><img src="https://pic1.zhimg.com/50/71687973cc3fc43095759614501ea43b_720w.jpg?source=1940ef5c" alt="img"></p><p>这里是一个非常非常简单的例子。实际上，我们看的文献会比这个复杂很多。而且，也不是文献的所有段落都需要这样详细地去理解（因为很多都是废话= =），一些重要的点，或者是比较困难去理解的地方，可以这样拆开来做笔记。这个方法是我觉得对有些枯燥晦涩的学术论文来说，很实用的一个方法。</p><p><em><strong>总结部分</strong></em>，除了正常罗列的那种总结，我推荐<strong>脑图</strong>和<strong>多使用模型/图形</strong>。</p><p>使用<strong>脑图</strong>。</p><p>有这样的想法是因为，某一天上课突然看到前排学霸在上课之前自己预习了，然后做好脑图直接打印了带过来，我坐在后排看到他的笔记简直震惊了，完全被惊艳到。因为在我初中的时候，我曾学过速读速记，当时的任务是需要每周读课上课后各两本名著，每次在课上做完速读后，回顾一本书，就需要像那样做一次脑图。我看过很多很多书，但是大部分都忘记了，唯独初中读的那些都还记得非常清楚，特别是直接在课堂上画完脑图的书，虽然读得非常快，但是印象还是很深刻。只是后来，因为自己懒=。=，并且使用速读看各种垃圾网络小说，所以就渐渐搁置了这个方法，现在回想起来……自己真是懒啊QAQ</p><p>人的思考很多时候是发散性的，使用脑图可以把握逻辑关系，利用记忆规律，平衡逻辑与想象。我做脑图的方法是，在理解透之后，合上书本/不要看文献，先按照自己的理解和记忆画一遍，然后再根据文章内容进行补充与修改。单纯的看完抄书效果会比较差。</p><p><strong>使用模型/图形</strong>。</p><p>很多时候，图形能比文字更清晰地表达逻辑关系。许多文献里，作者都会建立一个什么模型，我想这应该不是因为看起来好看，而是因为理解起来好理解，比如马斯洛的需求层次理论、SWOT分析等。所以，理解作者的模型，然后在你的笔记中拷贝下来，可以省掉很多很多作者关于这个模型的解释——只要你真的理解了。</p><p>好了，以上就是我今天（现在）所能想到的一些建议了。针对题主的补充问题再做一下解答：</p><p>哪些方面的内容需要记下来？</p><ul><li>脑图——包涵结构，因果关系，逻辑关系。</li><li>作者花费大量篇幅阐述的模型，与一些重要的解释和阐述</li><li>用自己的话再做一次总结，方便搜索关键词</li></ul><p>以什么样的方式记下来利于以后查找利用？</p><ul><li>使用云笔记，善用分类和标签</li><li>拷贝文献的关键词以方便查找</li></ul><p>以后还有想到的话再做补充吧~</p><p>2015年5月9日</p><p>以上</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读文献需要注意的问题</title>
      <link href="/2021/11/22/%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2021/11/22/%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="如何高效阅读阅读文献"><a href="#如何高效阅读阅读文献" class="headerlink" title="如何高效阅读阅读文献"></a>如何高效阅读阅读文献</h1><h2 id="Answer-1"><a href="#Answer-1" class="headerlink" title="Answer #1"></a>Answer #1</h2><p>我们在进行会计学术研究，撰写论文时需要阅读大量的文献，那么在阅读文献的过程中需要注意些什么问题呢？一起来看看！</p><p>1、文献阅读的一般要求</p><p>无主题地广泛浏览。</p><p>定期浏览一些重要期刊，及时了解到最新的研究动态和发展方向、研究内容和科研成果。</p><p>围绕研究方向泛读：根据自己的研究方向和研究内容大量阅读有关文献，使自己对该研究领域有深入细致的了解。</p><p>2、文献阅读的技巧</p><p>（1）学会从Abstract和Introduction中筛选和判断</p><p>必须要学会只看Abstract和Introduction便可以判断出这篇论文的重点和自己的研究有没有直接关联，从而决定要不要把它给读完。假如有能力每50篇论文只根据摘要和简介便能筛选出其中最密切相关的10篇论文，那么效率将大大提高。</p><p>（2）仔细阅读需要把握一个“度”</p><p>硕士生面对的知识是浩瀚的，因此只需要把握一定的程度就够了。硕士生必须学会选择性的阅读，而不是全部精读从而导致过多地纠结在某些细节知识中。多吸收“点子”比较重要，而不是细节知识。</p><p>（3）整批逐层地阅读</p><p>对于相关性比较近的文献，不需要逐篇逐篇地整篇一次读懂，而是要把这一系列的文献一整批一起读懂到某个层次。这样，第一轮读完后，可以根据第一轮所获得的知识判断出哪些论文与自己的议题不相关，不相关的就不需要再读下去了。这样才可以从广泛的论文里逐层准确地筛选出真正非懂不可的部分。</p><p>（4）要敢于想象与猜测</p><p>看到文献的题目后，要发挥不太离谱的想象能力，去大胆地猜测文献的主题、研究目标、研究方案、实验方法以及预期的实验结果。</p><p>3、带着哪些问题去精读</p><p>这篇论文最主要的创意或者闪光点是什么？</p><p>这些创意在应用上有什么好处？</p><p>这些创意和应用上的好处是在哪些条件下才能成立？</p><p>还能从另一个角度去探讨吗？</p><p>这篇论文最主要的缺点或局限是什么？</p><p>这些缺点或局限在应用上有什么坏处？</p><p>这些缺点和应用上的坏处是因为哪些因素引入的？我们能克服吗？如何克服？</p><p>这篇论文的哪些部分（点子）值得参考？</p><p>对自己的研究，有哪些指导意义和启示？</p><h2 id="Answer-2"><a href="#Answer-2" class="headerlink" title="Answer #2"></a>Answer #2</h2><blockquote><p>作者：匿名用户<br>链接：<a href="https://www.zhihu.com/question/494646493/answer/2190060477">https://www.zhihu.com/question/494646493/answer/2190060477</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>先要弄清英文文献的结构，</p><p><strong>怎么阅读文献</strong></p><p>1、<strong>对于从来没有接触过的学术领域，建议先看中文综述，然后是中文博士论文</strong>，而后是英文综述，最后是英文期刊文献。这样做的好处是，通过中文综述，你可以首先了解这行业的基本名词，基本参量和常用的制备、表征方法。此外，中文综述里可能包含大量的英文参考文献，这也为后续的查找文献打下一个基础。</p><p>2、自己学术领域的文献，要<strong>注重看摘要</strong>， 多数文章看摘要，少数文章看全文，这样确保自己有量，才有可能从量变到质变。 </p><p>3、<strong>通读全文</strong>：读第一遍的时候一定要认真，争取明白每句的大意，<strong>能不查字典最好先不查字典</strong>。因为读论文的目的并不是学英语，而是获取信息，查了字典以后思维会非常混乱，往往读完全文不知所谓。可以在读的过程中将生字标记，待通读全文后再查找其意思。  阅读全文时，要<strong>注重确立句子的架构，抓住主题</strong>。</p><p>4、归纳总结：每次读完文献 (不管是细读还是粗读)， 合上文献后，问自己，<strong>文章最重要的 take home message 是什么，</strong>如果不知道，就从abstract、conclusion 里找，并且从discuss 里最好确认一下。这样一来, 一篇文章就过关了。 take home message 其实都不会很多，基本上是一些concepts，<strong>如果你发现你需要记得很多，那往往是没有读到重点。</strong></p><p>5、<strong>看完的文献，不要丢在一边不管</strong>，可以根据需要，3－4个月温习一遍。</p><p>6、<strong>学会记笔记</strong>，重要的结论，经典的句子，精巧的方案一定要记下来，供参考和学习。<strong>平时读文章的时候，看到好的英文句型，重要结论和经典句子，最好都有意识的记一下，并且去看作者引用的reference文章里是否也有好的英文句型、重要结论和经典句子。这样做的好处是能够不断构建新的链接，组建一个供自己参考的论文句型、重要观点和经典句子的干货“堆场”。</strong></p><p>7、<strong>给自己定阅读量目标</strong>，阅读量的增加，可以形成融汇贯通。阅读文献的数量，积累多了，自然就由量变发展为质变了。</p><p><a href="https://link.zhihu.com/?target=https://www.itglchina.com/thread-902.html">如何快速略读英文文献？三分钟带你了解五种常见的学术文本结构</a> </p><p><a href="https://link.zhihu.com/?target=https://www.itglchina.com/thread-452.html">如何更有效阅读英文文献？</a></p><h2 id="Answer-3"><a href="#Answer-3" class="headerlink" title="Answer #3"></a>Answer #3</h2><blockquote><p>作者：掌桥科研<br>链接：<a href="https://www.zhihu.com/question/494646493/answer/2191890023">https://www.zhihu.com/question/494646493/answer/2191890023</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>阅读国外文献难在哪由于人各有异，在介绍阅读方法之前，梳理一下难点是有必要的。英文文献的难只是阅读者表层的感觉，更核心的问题是英语阅读中的障碍。对于阅读者来说，其受到的障碍越多，感觉到困难感觉就越强烈。下文列举了四类常见的阅读障碍。 </p><p><strong>第一类：词汇障碍</strong></p><p>在学术文章的阅读过程中，不可避免地会遇到完全没见过的新词。这个障碍的根源是以英文为母语的国家和非英文母语国家所要求的词汇量不一。在我国，非英语专业研究生入学考试考纲词汇量为5500左右，即便加上常见的基础性词汇，预估在7500-10000之间，但这与母语国家研究生水平所要求的词汇量有明显差距,。</p><p>词汇障碍造成的问题是遇到一个生词，思路很容易被打断，从而控制不住去翻字典获取其中文翻译。这样做的危害有两点，一是容易失去阅读的连贯性，感到一头雾水。二是一些单词仅仅是直译，与文中意思相差甚远，反而形成误导。</p><p> <strong>第二类：句式结构障碍</strong></p><p>复杂的句式结构容易让阅读者搞不清楚文章的主次结构和概念间的逻辑关系。在学术文章中，往往出现的复杂句式是从句套从句，从而出现多个主谓结构。这种情况往往是个耗时间的活儿，需要句子结构分析，逐步找出每一句的主谓结构，并且理清从句之间的相互关系，并可以准确把握。 </p><p><strong>第三类：背景知识障碍</strong></p><p>由于阅读的文献往往是该领域的前沿著作或者经典高引用文章，因此对阅读者的学力水平有着相对较高要求。例如，在计量类文章中，阅读文章的前提是对数理统计、回归模型等有一定的了解。但幸运的是，背景知识的障碍有一部分是可以通过温习本学科内的重要概念英文表达而解决的。 </p><p><strong>第四类：逻辑结构障碍</strong></p><p>英文文献写作过程中，作者往往遵照了一定范式以确保其符合所在学科的学术规范。对于阅读者来说，当不熟悉或者不习惯该逻辑结构时，就会感觉跟不上作者的表达思路。在这方面，对于经管类学生来说，可以通过阅读社会科学方法论相关文章进行概要的理解。</p><p><strong>方法一：实用的阅读工具</strong></p><p>“工欲善其事，必先利其器”，文献阅读通常是纸质版或者电子版，故此需要借助以下工具完成文章的阅读、记录和整理。</p><p>三色荧光笔——荧光笔的作用是标记重要部分，防止阅读过程中的“读后忘”现象。根据你的阅读习惯，可以赋予不同颜色以不同意义。例如，红色代表文章大问题主线相关，绿色代表详细阐释，蓝色代表拓展阅读。各人不同，需要根据自己的习惯加以调整。</p><p>掌桥科研——该翻译利器支持多国语言互译，包括英法意德日俄西等国际主流语言。 上传文档多种格式，包话PDF、DOC/DOCX、PPT/PPTX文档格式。该翻译利器翻译文档完全保留原文格式，且双页对照，查看阅读更加的方便。</p><p>链接放在这里：<a href="https://link.zhihu.com/?target=http://zhangqiaokeyan.com/?from=lsdn">http://zhangqiaokeyan.com/?from=lsdn</a>。</p><p><strong>方法二：阅读方法的改变</strong></p><p>明确阅读目的——阅读目的决定你所采取的阅读方式是精读还是泛读、跳读等。一般来说，在课程论文中，老师会针对某个主题给出该领域的经典文献清单，并要求写出阅读后的概要。那么此时，你的阅读目的可以进一步表述为了解、熟悉、掌握该主题下某位大牛的相关观点。</p><p>带着问题去阅读——一般来说，每篇文章都围绕着一个核心问题，并且这个问题会跟文章标题有很大相关性。因此，在阅读文章正文之前，应该结合标题，对文章核心问题有一个明晰的表述，必要时应该进行一定的思维发散。例如说，在“Bad Management Theories Are Destroying Good Management Practices”中，我们可以清晰知道标题是个陈述句，表明作者鲜明的态度。那么这篇文章的重要内容就是，作者进一步阐述HOW和WHY，并且使用了怎样的方法，这样的方法是否足以支撑其观点。</p><p>阅读顺序——以精读为例，笔者的习惯是从摘要开始，摘要的输出是一个关于本文章的一个大致思路框架和所涉及到的一组概念。注意，摘要的关键词一定要弄清楚，因为在全文会反复地出现。 其次是介绍和文献回顾部分。这部分是输出是在这研究领域，前人从何种角度做了哪些工作，作者的态度是继承还是批判，与文章核心的问题的联系点在哪里，作者需要进一步做什么工作，从什么角度。 紧接着是假设、具体问题和实验部分。这部分往往占据着文章的大部分篇幅，作为阅读者，此部分的输出是清晰地概述作者用了哪些假设，假设是否合理；作者所使用的方法对于具体问题来说是否是合理恰当的。 然后是发现部分。这部分的输出是将作者的实验变量、关键环节、实验结果以表格等方式表示出来。如果是数据分析，请务必弄清楚作者对关键数值的解释。 最后是结论部分。结论部分一般来说会比较层次分析，比较好读。这部分实质上是作者关于核心问题的观点看法。因此，需要弄清楚的是，文章通读下来，是否清楚作者得出结论的依据在哪和分析过程。这部分的输出是本篇文章的完整的思路模型。 如果是基于论文目的地阅读文献，参考文献也是不可忽视的重要部分，有助于阅读者对该领域完整、体系的理解。</p><h2 id="Answer-4"><a href="#Answer-4" class="headerlink" title="Answer #4"></a>Answer #4</h2><blockquote><p>作者：论文网<br>链接：<a href="https://www.zhihu.com/question/494646493/answer/2225027589">https://www.zhihu.com/question/494646493/answer/2225027589</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>1.通过标题和关键字，确认这篇文章是否值得精读，摘要部分会提供给我们一篇文章的主要研究手段和研究成果。读完摘要后我们需要跳到结论部分，如果一篇文章的结论部分和你的课题高度相关，那么这篇文章你可以继续读下去，反之从时间和效率的角度考量则需要略过。</p><p>2.先扫一遍图表以及他们的标题、图注，对于文章的数据内容有一个自己的初判，考察一下自己对于图表的解读是否和后面即将读到的文章内容一致，然后我们回到引言部分，去了解研究开展的背景，以及为什么作者要展开这项研究。通过这两部分的阅读可以帮助读者进一步了解该项研究的内容和目标。</p><p>3.精读文献：了解文章的细节信息，包括实验具体步骤等，对于自身科研项目的开展设计有重要参考价值。精读环节我们需要深度挖掘的是文章的结果和讨论部分，尤其是实验部分。通过对细节信息的研读，我们能够了解作者是如何开展实验，获取初始数据，进行数据分析，解读数据内涵等等。</p><p>4.做笔记的目的是帮助读者梳理对于文章的理解，列出对自己有价值核心信息，也方便自己在后面的科研工作中能够通过笔记来快速定位相关文献，在笔记中用一句话 (英文) 概括实验、结果、讨论章节中的每一段内容，组成一个阅读笔记。这样既可以锻炼英语书写表达能力，也可以逼迫自己在理解的基础上进行一定量的输出，这是一个加深理解和记忆的过程。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一个博士生接受怎样的训练是完整、全面的科研训练？</title>
      <link href="/2021/11/22/%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%A3%AB%E7%94%9F%E6%8E%A5%E5%8F%97%E6%80%8E%E6%A0%B7%E7%9A%84%E8%AE%AD%E7%BB%83%E6%98%AF%E5%AE%8C%E6%95%B4%E3%80%81%E5%85%A8%E9%9D%A2%E7%9A%84%E7%A7%91%E7%A0%94%E8%AE%AD%E7%BB%83%EF%BC%9F/"/>
      <url>/2021/11/22/%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%A3%AB%E7%94%9F%E6%8E%A5%E5%8F%97%E6%80%8E%E6%A0%B7%E7%9A%84%E8%AE%AD%E7%BB%83%E6%98%AF%E5%AE%8C%E6%95%B4%E3%80%81%E5%85%A8%E9%9D%A2%E7%9A%84%E7%A7%91%E7%A0%94%E8%AE%AD%E7%BB%83%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="一个博士生接受怎样的训练是完整、全面的科研训练？"><a href="#一个博士生接受怎样的训练是完整、全面的科研训练？" class="headerlink" title="一个博士生接受怎样的训练是完整、全面的科研训练？"></a>一个博士生接受怎样的训练是完整、全面的科研训练？</h1><p>在我博士刚入学的时候，系里给我们这些新生开过一个会，主要讲一个博士生要具备什么样的能力<br>才算是一个合格的博士。<br>写在问前，全文共计 5603 字，覆盖了博士所需技能和能力的方方面面，建议收藏后再看，可以时不<br>时拿出来与自己对照，有则改之无则加勉。<br>主要的材料是引用的这个：The Vitae Researcher Development Framework[1]（vitae 研究者发<br>展框架）<br>该框架分为四个领域，每个领域又分为三个子项，分别是：</p><h1 id="领域A：-知识和智力"><a href="#领域A：-知识和智力" class="headerlink" title="领域A： 知识和智力"></a>领域A： 知识和智力</h1><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Domain</span> A: Knowledge <span class="keyword">and</span> intellectual abilities: The knowledge, intellectual abilities <span class="keyword">and</span></span><br><span class="line">techniques <span class="keyword">to</span> <span class="keyword">do</span> research</span><br></pre></td></tr></table></figure><ul><li>A1: Knowledge base - 知识库</li><li>A2: Cognitive abilities - 认知力</li><li>A3: Creativity - 创新能力</li></ul><p><strong>A1: Knowledge base</strong></p><ol><li>subject knowledge - 领域知识</li></ol><ul><li>至少具有核心知识和对关键概念，问题和研究思路的基本理解。</li><li>了解自己研究领域和相关领域的最新进展。</li><li>正在努力为知识做出原创性贡献。</li><li>正在发展对国际和非学术方面的知识创造的广泛认识。</li></ul><ol start="2"><li>Research methods - theoretical knowledge:- 研究方法之理论知识</li></ol><ul><li>了解相关的研究方法和技术及其在自己研究领域中的适当应用。</li><li>证明自己研究中使用的原理和实验技术。</li></ul><p>一辈闲<br>@Prof.鎏</p><ol start="3"><li>Research methods – practical application: - 研究方法之实际应用</li></ol><ul><li>使用与研究领域相关的一系列研究方法并记录自己的活动。</li><li>在自己的学科领域显示出不断增强的能力，并且持续加深对可替代方法和分析技术的认识。</li></ul><ol start="4"><li>Information seeking - 信息搜集</li></ol><ul><li>掌握搜索和查找技能。</li><li>识别并访问适当的书目资源，档案和其他相关信息来源，包括网络资源，主要资源和存储库。</li><li>充分利用各种当前的工具和技术。</li><li>评估来源的可靠性，价值，权威性和相关性。</li><li>寻求相关团体的反馈以获取其他见解。</li></ul><ol start="5"><li>Information literacy and management - 信息素养与管理</li></ol><ul><li>适当地设计和执行用于使用信息技术获取和整理信息的系统（例如文字处理，电子表格，模拟系<br>  统，数据库）。</li><li>增强对信息/数据安全性和寿命问题的意识。</li><li>知道从哪里获得专家建议，包括但不限于信息/数据经理，档案管理员和图书馆员。</li></ul><ol start="6"><li>Language - 语言</li></ol><ul><li>掌握适用于研究的语言（包括技术语言）。</li><li>学习另外的语言。</li></ul><ol start="7"><li>Academic literacy and numeracy - 学术素养和计算能力</li></ol><ul><li>能够在学术环境中适当地理解，解释，创造和交流。</li><li>为演示准备语法正确的内容。</li><li>以适合目的和上下文的风格写给专业和非专业观众。</li><li>具有数学能力。</li><li>了解并应用学科/研究领域可能使用的任何统计数据。</li><li>分析数据并使用适当的计算机软件包。</li><li>具有IT素养并且能够使用信息和数字技术。</li></ul><p><strong>A2: Cognitive abilities - 认知力</strong></p><ol><li>Analysing - 分析</li></ol><ul><li>严格分析和评估自己的发现和其他发现。</li><li>验证其他人的数据集。</li></ul><ol start="2"><li>Synthesising - 融合</li></ol><ul><li>发现自己的研究与以前研究的联系，从融合中得到收获。</li></ul><ol start="3"><li>Critical thinking - 批判性思考</li></ol><ul><li>能够理解争议，并能以口头和文字表达自己的观点和假设。</li><li>具有识别和验证问题的能力</li><li>认识多种认知范式和替代范式</li></ul><ol start="4"><li>Evaluation - 评估</li></ol><ul><li>汇总，记录，报告并反思进度。</li><li>评估自己的研究活动的影响和结果。</li><li>评估主要和次要研究信息/数据的质量，完整性和真实性。</li><li>接纳并提出建设性批评。</li></ul><ol start="5"><li>Problem solving - 问题解决</li></ol><ul><li>将自己研究的基本主题独立出来；</li></ul><h2 id="•-并提出基础研究问题和假设。"><a href="#•-并提出基础研究问题和假设。" class="headerlink" title="• 并提出基础研究问题和假设。"></a>• 并提出基础研究问题和假设。</h2><p><strong>A3. Creativity - 创造力</strong></p><ol><li>Inquiring mind - 好奇心</li></ol><ul><li>表现出学习和获取知识的意愿和能力。</li><li>展示出灵活性和开放性。</li><li>培养一种提问技巧的风格。<br>2 Intellectual insight - 远见卓识</li><li>吸收并运用思想。</li><li>通过调查/寻求信息来创造想法和机会。<br>3 Innovation - 创新</li><li>了解创新和创造力在研究中的作用。</li><li>尝试从事跨学科研究。</li></ul><ol start="4"><li>Argument construction - 论据构造</li></ol><ul><li>建设性地捍卫研究成果。</li><li>提供一些支持成果的证据。</li><li>清晰简洁地构造论点。</li></ul><ol start="5"><li>Intellectual risk - 智力风险（？）</li></ol><ul><li>测试界限，愿意挑战学科/研究领域内的思想现状。</li><li>向重要的受众公开想法并对其他研究进行严格评估。</li></ul><h1 id="领域B：个人效能-成为合格研究人员的个人素质和方法"><a href="#领域B：个人效能-成为合格研究人员的个人素质和方法" class="headerlink" title="领域B：个人效能 - 成为合格研究人员的个人素质和方法"></a>领域B：个人效能 - 成为合格研究人员的个人素质和方法</h1><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Domain B: Personal effectiveness: The personal qualities <span class="keyword">and</span> approach <span class="built_in">to</span> be <span class="keyword">an</span> <span class="keyword">effective</span></span><br><span class="line">researcher</span><br></pre></td></tr></table></figure><ul><li>B1: Personal Qualities - 个人素质</li><li>B2: Self-management - 自我管理</li><li>B3: Professional and career development - 专业和职业管理</li></ul><p><strong>B1 Personal Qualities - 个人素质</strong></p><ol><li>Enthusiasm - 热情</li></ol><ul><li>保持自己的研究热情和动力。</li><li>认识到需要对自己的工作充满激情和自豪感。<br>即使是平凡的工作也有要有干劲。</li></ul><ol start="2"><li>Perseverance - 坚持不懈</li></ol><ul><li>表现出自律，积极性和坚定信念。</li><li>在遇到障碍和挫折时坚持不懈，但可以寻求同事，主管或领导者的支持。</li><li>有效地处理日常研究工作。</li></ul><ol start="3"><li>Integrity - 正直</li></ol><ul><li>在研究领域和所处机构了解和展示出良好研究的实践。</li><li>必要时寻求指导。</li></ul><ol start="4"><li>Self-confidence - 自信</li></ol><h2 id="•-发掘一些个人能力并愿意证明它们。"><a href="#•-发掘一些个人能力并愿意证明它们。" class="headerlink" title="• 发掘一些个人能力并愿意证明它们。"></a>• 发掘一些个人能力并愿意证明它们。</h2><h2 id="•-认识到自己知识，技能和专长的界限，并酌情利用一些外部支持。"><a href="#•-认识到自己知识，技能和专长的界限，并酌情利用一些外部支持。" class="headerlink" title="• 认识到自己知识，技能和专长的界限，并酌情利用一些外部支持。"></a>• 认识到自己知识，技能和专长的界限，并酌情利用一些外部支持。</h2><ol start="5"><li>Self-reflection - 自我反思</li></ol><ul><li>花时间反思实践和经历。</li><li>扬长，补短。</li><li>寻求个人反馈。</li><li>从错误中学习。</li></ul><ol start="6"><li>Responsibility - 责任</li></ol><ul><li>逐步对自己的项目和自己的身心健康承担全部责任。</li><li>进一步发展独立性。</li></ul><p><strong>B2: Self-management - 自我管理</strong></p><ol><li>Preparation and prioritisation - 准备和优先级</li></ol><ul><li>准备和规划以达到目标，并能在得到必要的支持下进行调整。</li></ul><ol start="2"><li>Commitment to research - 致力于科研</li></ol><ul><li>致力于并完成第一个项目，同时获得研究证书（credentials）</li></ul><ol start="3"><li>Time management - 时间管理</li></ol><ul><li>有效地管理自己的时间以完成研究项目。</li><li>坚持清晰的计划。</li></ul><ol start="4"><li>Responsiveness to change</li></ol><ul><li>在需要时调整方法； 寻求指导并认识到风险。</li></ul><ol start="5"><li>Work-life balance</li></ol><ul><li>正在认识到工作与生活平衡问题。</li><li>必要时使用支持和咨询服务，以避免过大的压力并增进个人身心健康。</li><li>考虑他人的需求。</li></ul><p><strong>B3：Professional and career development - 专业和职业管理</strong></p><ol><li>Career management - 职业管理</li></ol><ul><li>管理自己的职业发展，设定现实可行的职业目标，确定并践行提高就业能力的方法。</li><li>通过有效的简历，申请和面试展示自己的技能，个人特质和经验。</li><li>开始建立职业网络。</li></ul><ol start="2"><li>Continuing professional development - 持续的专业发展</li></ol><ul><li>表现出自我意识和识别自己的发展能力。</li><li>认识到自己经验的可转移性，并将其明确传达给潜在的雇主或直属负责人。</li><li>简历并维护自己的成就和经验记录。</li></ul><ol start="3"><li>Responsiveness to opportunities - 对机会的反应</li></ol><ul><li>展示对研究技能到其他工作环境的可转移性的了解，以及对学术界内外的职业机会的了解。</li><li>了解并利用学术界内外的广泛就业和专业发展机会，包括工作经验和实习机会。</li></ul><ol start="4"><li>Networking - 社交圈</li></ol><ul><li>与机构内以及更广泛的研究界内的主管和同事建立并维护合作网络和工作关系。</li></ul><h2 id="•-有效地使用个人和-或在线网络来获得反馈和建议，对工作的严格评估以及对机会的响应。"><a href="#•-有效地使用个人和-或在线网络来获得反馈和建议，对工作的严格评估以及对机会的响应。" class="headerlink" title="• 有效地使用个人和/或在线网络来获得反馈和建议，对工作的严格评估以及对机会的响应。"></a>• 有效地使用个人和/或在线网络来获得反馈和建议，对工作的严格评估以及对机会的响应。</h2><h2 id="•-与社会和公共机构互动交流。"><a href="#•-与社会和公共机构互动交流。" class="headerlink" title="• 与社会和公共机构互动交流。"></a>• 与社会和公共机构互动交流。</h2><ol start="5"><li>Reputation and esteem - 声誉和自尊</li></ol><ul><li>在自己的研究领域上发声。</li><li>开始成为优秀研究者。</li></ul><h1 id="领域C：研究管理和组织能力-了解研究的专业标准和要求"><a href="#领域C：研究管理和组织能力-了解研究的专业标准和要求" class="headerlink" title="领域C：研究管理和组织能力 - 了解研究的专业标准和要求"></a>领域C：研究管理和组织能力 - 了解研究的专业标准和要求</h1><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Domain C: Research governance <span class="keyword">and</span> organisation:</span><br><span class="line">Knowledge <span class="keyword">of</span> <span class="keyword">the</span> professional standards <span class="keyword">and</span> requirements <span class="built_in">to</span> <span class="built_in">do</span> research</span><br></pre></td></tr></table></figure><ul><li>Professional conduct -职业操守</li><li>Research Management - 研究管理</li><li>Finance, funding and resources - 财务，资金和资源<br>C1: Professional conduct -职业操守</li></ul><ol><li>Health and safety - 健康和安全</li></ol><ul><li>了解相关的健康和安全问题，并养成负责任的工作习惯。</li><li>对自己的工作环境负责。</li><li>意识到对他人和更广泛环境的影响。</li></ul><ol start="2"><li>Ethics, principles and sustainability - 道德，原则和可持续性</li></ol><ul><li>了解并应用有关行为规范的行为准则和准则； 积极向主管寻求建议。</li><li>表现出对与其他研究人员，研究对象以及可能受到研究影响的其他人的权利有关的问题的意识。</li><li>注意自身对环境的影响。</li><li>了解如何以可持续的方式表现和工作。</li><li>了解社会责任的概念； 积极寻求必要的指导。</li></ul><ol start="3"><li>Legal requirements - 法律要求</li></ol><ul><li>对研究的法律要求有基本的了解，例如 Data Protection Act(《数据保护法》)，Freedom of<br>  Information Act(《信息自由法》)等。</li></ul><ol start="4"><li>IPR and copyright - 知识产权和版权</li></ol><ul><li>对适用于自己的研究的数据所有权规则有基本的了解。</li></ul><ol start="5"><li>Respect and confidentiality - 尊重和保密</li></ol><ul><li>尊重自己的研究中参与者的保密权和匿名权。</li><li>尊重同事。</li></ul><ol start="6"><li>Attribution and co-authorship - 署名和共同创作</li></ol><ul><li>了解归因概念，并一贯的公平贯彻归因。</li><li>寻求对于对于有关行为准则的建议。</li></ul><ol start="7"><li>Appropriate practice - 适当实践</li></ol><ul><li>在适当的情况下，了解并遵守与所在机构相关的学术不端行为的法规。</li></ul><h1 id="领域D：参与程度和影响力：与他人合作以确保研究产生更广泛影响的知识和技能"><a href="#领域D：参与程度和影响力：与他人合作以确保研究产生更广泛影响的知识和技能" class="headerlink" title="领域D：参与程度和影响力：与他人合作以确保研究产生更广泛影响的知识和技能"></a>领域D：参与程度和影响力：与他人合作以确保研究产生更广泛影响的知识和技能</h1><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Domain</span> D: Engagement, influence <span class="keyword">and</span> impact: The knowledge <span class="keyword">and</span> skills <span class="keyword">to</span> <span class="keyword">work</span> <span class="keyword">with</span></span><br><span class="line">others <span class="keyword">to</span> ensure the wider impact <span class="keyword">of</span> research</span><br></pre></td></tr></table></figure><ul><li>Working with others - 与他人合作</li><li>Communication and dissemination -交流与传播</li><li>Engagement and impact - 参与和影响</li></ul><p><strong>D1: Working with others - 与他人合作</strong></p><ol><li>Collegiality - 合作制</li></ol><ul><li>考虑他人</li><li>倾听，给与和接受反馈，并回复对方</li></ul><ol start="2"><li>Team working - 团队合作</li></ol><ul><li>在正式团队和非正式团队中工作并为他们的成功做出贡献时，了解自己的行为以及对他人的影<br>  响。</li><li>感谢其他团队成员（包括非学术成员）的贡献。</li><li>感谢所有直接或间接参与者的贡献。</li></ul><ol start="3"><li>People management - 人员管理<br>与主管商议进程和期限deadline。</li><li>Supervision - 监督</li></ol><ul><li>参与同级的帮助和评估，以及对于本科生的支持和评估。</li></ul><ol start="5"><li>Mentoring - 指导</li></ol><ul><li>参与教学，指导，演示或其他研究活动时，有效地支持他人的学习。</li><li>认识到指导和接受指导的重要性。</li></ul><ol start="6"><li>Influence - 影响力</li></ol><ul><li>参与讨论并欢迎挑战。</li><li>认识到自己的研究对现实生活环境的影响。</li><li>认识到学术界与那些使用研究成果来取得影响和受到影响的人们进行对话的价值。</li></ul><ol start="7"><li>Collaboration - 合作</li></ol><ul><li>意识到协同工作的价值，并从中获得尽可能大的收益。</li><li>与主管/研究负责人共同研究。</li><li>认识到自己和相邻学科/研究领域内的共同/冲突利益。</li></ul><ol start="8"><li>Equality and diversity - 平等和多样性</li></ol><ul><li>尊重个人差异。</li><li>在工作环境中培养对多样性和差异的认识。</li><li>了解机构对平等和多样性的要求。</li></ul><p><strong>D2: Communication and dissemination -交流与传播</strong></p><ol><li>Communication methods - 交流方式</li></ol><ul><li>通过各种技术，正式和非正式地构造连贯的论点，并向各种受众清楚地表达思想。</li><li>积极参与与同事的知识交流和辩论，不限于学科和领域</li><li>注重修辞技巧。</li></ul><ol start="2"><li>Communication media - 交流媒介</li></ol><h2 id="•-发展各种沟通方式，例如使用交互式技术和-或文本和视觉媒体进行面对面的交互。"><a href="#•-发展各种沟通方式，例如使用交互式技术和-或文本和视觉媒体进行面对面的交互。" class="headerlink" title="• 发展各种沟通方式，例如使用交互式技术和/或文本和视觉媒体进行面对面的交互。"></a>• 发展各种沟通方式，例如使用交互式技术和/或文本和视觉媒体进行面对面的交互。</h2><h2 id="•-拥有个人网络主页。"><a href="#•-拥有个人网络主页。" class="headerlink" title="• 拥有个人网络主页。"></a>• 拥有个人网络主页。</h2><h2 id="•-在演示中有效使用视听辅助工具。"><a href="#•-在演示中有效使用视听辅助工具。" class="headerlink" title="• 在演示中有效使用视听辅助工具。"></a>• 在演示中有效使用视听辅助工具。</h2><ol start="3"><li>Publication - 出版物</li></ol><ul><li>了解出版过程和研究成果的进一步开发过程。</li><li>发表一些出版物。</li><li>正在提高人们对出版物发行范围和多样性的认识。</li></ul><p><strong>D3: Engagement and impact - 参与和影响</strong></p><ol><li>Teaching - 教学</li></ol><ul><li>参与本科生教学。</li><li>协助对本科生项目的监督。</li><li>参加研究会议（研讨会，讲习班，会议等）。</li></ul><ol start="2"><li>Public engagement - 参与到公众活动</li></ol><ul><li>了解并欣赏与公众互动的价值，并乐于参与。</li><li>响应当地的各种机会和活动，并在公共活动中介绍自己研究的各个方面。</li></ul><ol start="3"><li>Enterprise - 创业</li></ol><ul><li>产生想法并确定机会。</li><li>在自己的机构内部或外部以创新的方式发展思想。</li><li>了解研究成果的商业开发过程。</li><li>了解在业务/商业环境中建立关系对学术界的价值。</li></ul><ol start="4"><li>Policy - 政策</li></ol><ul><li>了解相关的决策过程，并以政策友好的形式展示成果。</li><li>分析政策并了解其所处的广泛环境。</li></ul><ol start="5"><li>Society and culture - 社会和文化</li></ol><ul><li>使人们认识到研究对更广泛的社会的影响以及社会，环境和文化对研究的影响。</li><li>了解社会责任的概念。</li></ul><ol start="6"><li>Global citizenship - 全球公民</li></ol><ul><li>对国家和国际层面上进行自己的研究的背景有广泛的了解。</li></ul><p>写在文末，看到这个问题后，自己就有了写这个回答的想法，没想到东西涉及的非常多，最后写了<br>将近 5500 字，这不仅对相关的同学有所启迪，也是对自己接下来的一个界定，从上述中能看到，对<br>于一个合格的博士学位获得者，掌握的能力几乎是全方位的，进可科研，退可百搭。<br>希望与各位同学共勉。</p><p>推荐一篇短文：A PhD is not enough, 可以理解为博士生存指南，链接如下：<br>biomath.usu.edu/files/P…</p><p>看到这里的话顺便再求个赞：<br>如何系统地自学 Python？</p><p>编程零基础应当如何开始学习 Python？<br>平凡：编程语言项目大全 - PBL(Project based Learning) - 包含全部主流语言<br>如何正确的从零开始学英语？<br>英语基础差如何学习考研英语？<br>科研大牛们怎么读文献？</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><strong>^</strong> framework/developing-the-vitae-researcher-development-framework<a href="https://www.vitae.ac.uk/researchers-professional-development/about-the-vitae-researcher-development-https://link.zhihu.com/?target=https://biomath.usu.edu/files/Peter_J._Feibelman_A_PhD_Is_Not_Enough.pdf">https://www.vitae.ac.uk/researchers-professional-development/about-the-vitae-researcher-development-https://link.zhihu.com/?target=https%3A//biomath.usu.edu/files/Peter_J._Feibelman_A_PhD_Is_Not_Enough.pdf</a>)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科研论文如何想到不错的idea？</title>
      <link href="/2021/11/22/%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87%E5%A6%82%E4%BD%95%E6%83%B3%E5%88%B0%E4%B8%8D%E9%94%99%E7%9A%84idea%EF%BC%9F/"/>
      <url>/2021/11/22/%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87%E5%A6%82%E4%BD%95%E6%83%B3%E5%88%B0%E4%B8%8D%E9%94%99%E7%9A%84idea%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="知乎精选-科研论文如何想到不错的-idea？"><a href="#知乎精选-科研论文如何想到不错的-idea？" class="headerlink" title="[知乎精选] 科研论文如何想到不错的 idea？"></a>[知乎精选] 科研论文如何想到不错的 idea？</h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a><strong>Question</strong></h2><p>一篇科研论文的开始应该是一个不错的idea，在这个基础上才能有后面的实验展开。那么请问这个 idea 一般是从何而来，一个低年级博士生前期应该做哪些积累呢？补充: 本人是计算机方向的，当然其他领域的也欢迎～</p><h2 id="Answer-1"><a href="#Answer-1" class="headerlink" title="Answer #1"></a>Answer #1</h2><blockquote><p>作者：微软亚洲研究院<br>链接：<a href="https://www.zhihu.com/question/300967426/answer/638195353">https://www.zhihu.com/question/300967426/answer/638195353</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>计算机方向的来啦！</p><p>这次想分享的是微软亚洲研究院副院长刘铁岩博士在“微软AI讲堂2019校园行”的主题演讲——“<strong>形成机器学习研究的闭环</strong>”，以对偶学习、博弈学习等几项重要的工作为例，分享了他的研究经验：<strong>从实践中发现研究问题，解决现实痛点，形成研究的闭环。</strong><br>虽然是以机器学习为例，其他学科领域也可以借鉴哦~</p><p>—————— 我是演讲正文的分割线——————</p><p>今天非常荣幸能和大家分享人工智能和机器学习方面的话题，我报告的主题是“<strong>形成机器学习研究的闭环</strong>”。这并不是一个纯粹的技术讲座，而是饱含着经验分享，是有关这些年我们如何通过对于机器学习各个侧面进行360度的思考，从而形成研究的闭环。</p><p><img src="https://pic2.zhimg.com/50/v2-3bc1a3ba753f77cbb7e061c2907d1932_720w.jpg?source=1940ef5c" alt="img"></p><p>让我们通过这个小小的公式来展开今天的分享。它看似简单，却涵盖了一大类的机器学习问题。这个公式中x_i和y_i是从某一个分布P中采样得到的训练数据样本， ∑是对训练样本求和，L是损失函数，f_ω是需要训练的机器学习模型。机器学习的过程就是在训练数据上最小化损失函数，从而得到一个最优的模型ω*。其实这个过程中，蕴含了一些假设。比如：假设数据分布是事先给定且静态不变的，假设我们有足够的数据可以达到训练的渐进性能，以及假设我们不需要为实际应用中算法的部署和运算复杂度而担忧。然而，当用机器学习来解决真正的现实问题时，这些假设并不成立，我们将会面临很多新的挑战——包括来自数据规模和动态性的挑战，算法易处理性和最优性之间的平衡，以及算法效率和可扩展性的挑战等等。</p><p><img src="https://pic3.zhimg.com/50/v2-02e06df6d943f527e3457194e497c34d_720w.jpg?source=1940ef5c" alt="img"></p><p>这些挑战可能大家在写论文的时候可以选择回避，但是当技术要落地、要与产业结合的时候，就无法回避这些问题了。换言之，<strong>我们必须以应用难点为动机，摒弃实际中不合理的假设，建立整个机器学习研究的闭环。</strong></p><p><img src="https://pic1.zhimg.com/50/v2-daff006fe006d5c205e85842285b06f5_720w.jpg?source=1940ef5c" alt="img"></p><p>下面我给大家分享几个案例，看看我们是如何从实际挑战中激发研究问题，以及这样的研究又是如何反过来对实际应用有所帮助的。</p><h3 id="对偶学习Dual-Learning"><a href="#对偶学习Dual-Learning" class="headerlink" title="对偶学习Dual Learning"></a><strong>对偶学习Dual Learning</strong></h3><p><strong><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649439647&idx=1&sn=fc2fc034cf62cd9db0a821368092e6b9&chksm=82c0d41bb5b75d0d67e8e217639531855cacf0c6606a28ac17f20e7162e107694149315c0b35&scene=21%23wechat_redirect">对偶学习</a>解决的是实际应用中训练数据不足的问题。</strong>当我们没有充足的有标签数据的时候，想要进行有效的训练，就需要寻找其它信号来驱动训练过程。对偶学习利用的信号是天然存在于人工智能任务之中的，但是很少被人利用，我们称之为人工智能任务的结构对偶性。所谓结构对偶性指的是一个人工智能任务的输出恰好是另外一个任务的输入，反之亦然。例如，在机器翻译中，中英翻译和英中翻译是一对对偶任务；在语音信号处理中，语音识别和语音合成是一对对偶任务。</p><p>那么有了结构对偶性，对偶学习是如何进行模型训练的呢？我们以中英机器翻译为例。假设我们只有单语的数据，即无标注的英文文档和无标注的中文文档，和两个能力很弱的初始翻译模型。我们的任务是利用无标注的单语数据不断地学习、提高初始模型的能力，最后得到非常强的翻译模型。</p><p>为了实现这个目的，我们可以拿一个无标签的英文句子，利用初始模型将其译成中文，然后再利用反方向的初始翻译模型把这句中文译回英文。通过比较原始的英文句子和翻译回来的英文句子，以及中间的翻译结果的语法和词法，我们可以得到一系列反馈信号，来更新初始模型，周而复始，使之不断提高。当我们有海量的单语数据时，对偶学习可以不断地提升翻译模型的性能，达到很高的水准。<a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649443128&idx=1&sn=1a727bba2caa139e7a63a5817fc23622&chksm=82c0a2bcb5b72baa2f1c63f60946ad5477eaa5d6e9ae732ee43cb577374c04b1b6bb6c1e2e39&scene=21%23wechat_redirect">微软亚洲研究院2018年3月在中英新闻翻译任务上达到了媲美人类的水准</a>，对偶学习就是其背后的秘密武器之一。</p><p>对偶学习之所以有效，是因为两个对偶任务背后有着非常强的概率联系——它们的机器学习模型分别对应于联合概率的两种不同的展开方式。正因为这种联系，两个机器学习模型可以互相帮助，使对偶任务的学习更出色。目前，我们已经对对偶学习在有监督、无监督、推断、迁移学习、多智能体学习等各个层面上进行了深入的研究，在学术界产生了一定的反响，很多学者开始将对偶学习的思路应用在他们的目标问题中。</p><h3 id="博弈学习Game-Theoretic-Learning"><a href="#博弈学习Game-Theoretic-Learning" class="headerlink" title="博弈学习Game-Theoretic Learning"></a><strong>博弈学习Game-Theoretic Learning</strong></h3><p>机器学习的另一大挑战，是数据由智能体产生，其分布是动态的，并且会随着机器学习的过程发生变化。智能体之间的互动，可以用博弈论来刻画。然而博弈论也存在自己的局限性，它假设智能体完全理性，而且进行的是最坏情况的分析。</p><p><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649438844&idx=1&sn=3c8b022de15f3caa2595ad56ef4df538&scene=21%23wechat_redirect">博弈机器学习</a>就是要取二者所长，得到一个能够解决实际挑战的方法。我们仍然关心智能体的策略行为，但是这种行为是用基于数据驱动的马尔科夫模型加以描述的。具体而言，在博弈机器学习的框架中有两个模型，<strong>一个模型用来学习智能体的行为，用它可以预测在未来新的情况下智能体会做出什么样的反应，产生什么样的数据；第二个模型用来解决目标的机器学习问题，它所用的部分数据由第一个模型产生</strong>，换言之，我们不再假设所有数据是由预先给定的分布产生的。</p><p>我们以广告拍卖机制设计为例来讲解一下博弈机器学习的流程。在广告的拍卖过程中，广告主们会对关键词或者广告位进行竞价，拍卖的胜者将得到广告机会；在这个过程中，广告费和广告的相关性都会起作用。当广告机制在这两种因素之间权衡的时候，广告主会有感觉，并且相应地调整自己的出价以及广告内容，以期获得拍卖的胜利。很显然，广告主的行为数据是随着广告机制的变化而变化的，而不是从某个固定的分布中采样得来的。博弈机器学习包含不断学习的迭代过程，在广告拍卖机制更新后，广告主的行为会发生变化，我们需要相应地调整行为模型，行为模型再产生新的广告数据，而这些数据会被用来训练新的广告拍卖机制。这个过程不断重复，直到整个过程收敛，得到一个在均衡态下最好的机制。</p><h3 id="竞合学习Coopetitive-Learning"><a href="#竞合学习Coopetitive-Learning" class="headerlink" title="竞合学习Coopetitive Learning"></a><strong>竞合学习Coopetitive Learning</strong></h3><p><strong>竞合学习要解决的问题，是把一个复杂的优化问题转化为局部优化，每个局部问题用一个智能体来解决，并通过局部智能体之间的约束，保证局部优化和全局优化之间有非常强的联系。</strong>每一个智能体在做决策时，与其它智能体之间既是共享信息的合作关系，也存在对公共资源的竞争关系，形成合作与竞争并存的机制，最终实现全局最优化。</p><p>这一研究的背景是<a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649443420&idx=1&sn=9fb43dbc945214e0f2fb54a1da7d91e8&chksm=82c0a5d8b5b72cce32c201dcb2f2174d7aea51ed1f2e9463462bbb25ee44bfb853310d358965&scene=21%23wechat_redirect">我们与东方海外航运公司（OOCL）的合作</a>。在航运的应用场景中，每个港口都是局部智能体，每个港口都要对自己的物流状况作出决策，各个港口之间是上下游关系，有很强的联系；同时它们之间还存在对轮船载重资源的竞争与冲突。那如何有效地建模这种竞合关系呢？首先我们用一个图神经网络来对合作关系进行建模，其次，我们用拍卖来对竞争关系进行建模，通过求解一个次模优化问题，来决定轮船给相关港口分配怎样的资源。通过这种竞合学习，最后我们得到的局部优化和整体优化的结果非常接近，且运行效率提高了多个数量级。</p><h3 id="轻量学习Lightweight-Learning"><a href="#轻量学习Lightweight-Learning" class="headerlink" title="轻量学习Lightweight Learning"></a><strong>轻量学习Lightweight Learning</strong></h3><p>最近这几年，学术界有一种“大力出奇迹”的趋势，用到的GPU、TPU越来越多。这种情况不仅会导致学术垄断现象，还会出现一种马太效应，一些研究的边界要通过强大的计算资源才能获取，而且他人没有计算资源就无法复现。 </p><p>面对这种情况，我们做了一系列轻量机器学习的研究，我们希望告诉学术界，<strong>有时候巧妙的算法比算力更重要，不需要那么多的计算资源也可以解决很大规模的问题。</strong>在我们2015年发表LightLDA算法之前，最好的LDA系统是谷歌的LDA，用10000个CPU训练了70小时，从文本里抽取出10万个主题。我们在算法上做了创新，首次提出了采样概率的乘性因子分解，在60小时内可以用8台计算机抽取100万个主题。</p><p>我们发表在NIPS 2016和2017上的<a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649440041&idx=1&sn=615217bcd020618e595b0b642ebadc96&chksm=82c0d6adb5b75fbbd8c4eef92aab203542d0d297f7d2f1e4b0dd86a65a7096a5723e853d8be6&scene=21%23wechat_redirect">LightGBM</a>算法也提出了全新的优化思路，比如互斥特征捆绑技术和基于投票的轻量级并行框架，这些新技术让LightGBM比此前最好的XGBoost算法快一个数量级以上，精度也有所提升。LightGBM开源后，在没有任何宣传的情况下迅速在GitHub上获得了8000+星，过去两三年里很多算法竞赛、数据挖掘竞赛的冠军都使用了LightGBM。由此可见，精巧的算法创新可以降低学术的门槛，让很多人不需要砸钱买上万块GPU或者CPU也可以做很了不起的大规模的研究。</p><h3 id="分布式学习Distributed-Learning"><a href="#分布式学习Distributed-Learning" class="headerlink" title="分布式学习Distributed Learning"></a><strong>分布式学习Distributed Learning</strong></h3><p>当然，<strong>当数据和模型大到一定程度时，分布式运算不可避免。</strong><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649444062&idx=1&sn=53cf03f9930f62f62cb71aead6a8b242&chksm=82c0a75ab5b72e4c0601476c168e69e2ee7313871e1ce1bcf600f86886253841447b14e4868f&scene=21%23wechat_redirect">分布式机器学习</a>也有很多问题值得深究，比如数据如何切分？局部节点之间如何通信？局部节点训练出的机器学习子模型如何复合？每一步听起来简单，做起来都很需要技巧。</p><p>比如说通信，最简单的是使用基于MPI的同步通信，但在成百上千台机器共同处理一个计算任务时，不能保证每台机器运算速度一致，这时同步通信就好似有短板的水桶，最后整个系统被短板拖垮。近年的热点是异步通信，但异步通信会受到延迟的困扰。当一个很慢的机器把它的陈旧的模型更新同步到全局服务器上时，可能毁掉那个被其它快机器更新了很多次的新模型。为了解决这个问题，我们在ICML 2017上发表的一篇论文，首次用数学手段对延迟进行了严谨的刻画，并且提出了消除延迟的补救方法。理论和实验均表明，新方法的收敛性能优于传统的异步通信，在精度方面接近单机算法。</p><p>除此之外，在分布式机器学习方面，我们还做了很多其它工作，也对这一领域做了较为全面的总结，整合为《<a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649445456&idx=1&sn=ab700bb5478ba0ab5fbff18362afff58&chksm=82c0bdd4b5b734c2be8ff62c03bede1b1b3bc2eb29046202c194492488afbe532e08772e855a&scene=21%23wechat_redirect">分布式机器学习：算法、理论与实践</a>》一书，推荐对分布式机器学习感兴趣的读者阅读。</p><h3 id="解决现实痛点，做有用的研究"><a href="#解决现实痛点，做有用的研究" class="headerlink" title="解决现实痛点，做有用的研究"></a><strong>解决现实痛点，做有用的研究</strong></h3><p>这五个研究方向看似不同，背后其实有共通之处——每一个研究都是来源于实际应用中的痛点分析，弥补了传统机器学习算法和模型的不足。正是因为如此，我们提出的这些新的研究方法，在现实的应用场景中取得了颠覆性的效果。</p><p>• 将对偶学习应用于中英机器翻译任务，我们在2018年3月率先达到了媲美人类的水平。</p><p>• 将博弈学习和深度学习应用于智能投资，我们得到了比所有市面上的基金产品的超额收益率都高很多的投资策略，而且在风险控制方面也满足了严苛的要求。</p><p>• 将竞合学习应用于集装箱调度，我们不仅在速度上有极大的提升，还能够减少约10%的运营成本，这相当于每年节省几千万美金的支出。</p><p>• 将LightLDA算法应用于微软的广告业务，我们在用户体验没有任何下降的情况下促成了80%的利润增长，收到了产品副总裁的高度赞扬。</p><p>• 将分布式学习应用于微软CNTK平台，我们在训练速度上与其它平台相比有了非常大的提升。</p><p>我想通过这五个实际案例向大家展示，如果我们在做人工智能、机器学习研究时，有针对性地去解决现实中的痛点问题，从中发掘关键的挑战，找到技术的难点，那么我们的研究将有机会对现实世界产生非常巨大的影响。所以，<strong>从事机器学习的研究，不能闭门造车，要从实践中来，到实践中去，形成研究的闭环。</strong></p><hr><p>本账号为微软亚洲研究院的官方知乎账号。本账号立足于计算机领域，特别是人工智能相关的前沿研究，旨在为人工智能的相关研究提供范例，从专业的角度促进公众对人工智能的理解，并为研究人员提供讨论和参与的开放平台，从而共建计算机领域的未来。</p><p>微软亚洲研究院的每一位专家都是我们的智囊团，你在这个账号可以阅读到来自计算机科学领域各个不同方向的专家们的见解。请大家不要吝惜手里的“邀请”，让我们在分享中共同进步。</p><p>也欢迎大家关注我们的微博和微信 (ID:MSRAsia) 账号，了解更多我们的研究。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工科博士应该注重哪些方面的能力培养？</title>
      <link href="/2021/11/22/%E5%B7%A5%E7%A7%91%E5%8D%9A%E5%A3%AB%E5%BA%94%E8%AF%A5%E6%B3%A8%E9%87%8D%E5%93%AA%E4%BA%9B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%83%BD%E5%8A%9B%E5%9F%B9%E5%85%BB%EF%BC%9F/"/>
      <url>/2021/11/22/%E5%B7%A5%E7%A7%91%E5%8D%9A%E5%A3%AB%E5%BA%94%E8%AF%A5%E6%B3%A8%E9%87%8D%E5%93%AA%E4%BA%9B%E6%96%B9%E9%9D%A2%E7%9A%84%E8%83%BD%E5%8A%9B%E5%9F%B9%E5%85%BB%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="知乎精选-工科博士应该注重哪些方面的能力培养？"><a href="#知乎精选-工科博士应该注重哪些方面的能力培养？" class="headerlink" title="[知乎精选]工科博士应该注重哪些方面的能力培养？"></a>[知乎精选]工科博士应该注重哪些方面的能力培养？</h1><h2 id="Answer-1"><a href="#Answer-1" class="headerlink" title="Answer #1"></a>Answer #1</h2><blockquote><p>作者：Journey<br>链接：<a href="https://www.zhihu.com/question/495365481/answer/2205597092">https://www.zhihu.com/question/495365481/answer/2205597092</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>对于计算机体系结构（computer architecture）方向的博士生，有两个经常被忽略的问题需要被重视。</p><p>首先是定位-分析问题的能力。计算机体系结构低年级博士生，经常会有重构一切的冲动，觉得对于一个低效/高功耗/复杂……的结构/方法，自己重新写一个就会解决问题。但是，对现有体系过大的改变往往是无法实际应用的。更重要的是，一个结构/方法的低效/高功耗/高复杂度……往往只是结果，其背后的根源是需要去挖掘和解读的。对于同一个问题，挖掘和解读的角度不同，就会产生不同的解决方案。对于这种能力的培养，建议多读一些顶会长文的motivation部分，学习一下如何见微知著地去挖掘一个问题背后的原因并量化地进行分析。</p><p>第二个是解构-重组的能力。这方面能力欠缺所导致的问题往往出现在读博中期，特别是发表了第一篇论文之后。经常出现的问题是过度关注第一篇论文的内容，然后企图在第一篇工作的基础上继续做扩展，最后陷入“给A加点B，再加点C，或者改一下条件D”的误区，让后续工作变成了自我打补丁的过程。正确的做法是要从问题的层面去审视自己之前的工作，对于一个大的问题，可以将其分解为一系列子问题，然后针对不同子问题去分析原因并寻找解决方案，最后再去考虑如何把这些子问题的解决方案进行整合。这里有两点需要注意：（1）是把问题分解为子问题，不是把一个系统分解为子系统或者把一个工程分解为一堆子任务，这两者是不同的。一般来说，问题是蕴含在子系统或者子任务中的，是需要靠量化分析去挖掘的，而且不同问题可能相互叠加。（2）解决分解后的问题可以不必拘泥于某个特定的领域（比如访存优化、调度、cache、OS……），实际问题的最佳解决方案往往不在同一个领域，需要根据实际需要去变化，不要把自己限制在一个特定领域内去削足适履。当然，这样做需要对计算机系统中不同子系统和层自身特性以及相互关联都很熟悉，所以说做计算机体系结构研究还是需要打好基础，广泛的学习不同相关领域中的知识，不要一上来就急于“确定方向”或者“想idea”。</p><p>（这里可能涉及一个“标签”的问题：很多老前辈都说要专注一个领域，要有自己的标签。这个说法没有问题，也完全是善意的，但是“领域”、“标签”更应该来源于问题层面，如果把自己的“标签”限制在某个具体的“物”上面，比如cache、NVM、DRAM、scheduling……那么很可能会越做越窄，而且在思考解决方案时这些限制会导致错失真正的idea。所以，具体研究内容上可以跳脱一些，但是问题层面需要统一，这样才能让研究视野越来越宽广。）</p><h2 id="Answer-2-搬砖君"><a href="#Answer-2-搬砖君" class="headerlink" title="Answer #2: 搬砖君"></a>Answer #2: 搬砖君</h2><blockquote><p>作者：搬砖君<br>链接：<a href="https://www.zhihu.com/question/495365481/answer/2214477754">https://www.zhihu.com/question/495365481/answer/2214477754</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>作为一个从工科博士一路走来的人，这个问题我深有体会，核心点在于保持科研的同时，<strong>一定要及时关注你们所在行业的产业发展动态</strong>，举个例子，在你们这个行业，国内有哪些巨头企业？还有哪些独角兽企业？产业发展在一个什么阶段？有多大的市场份额？</p><p>很多博士在校园呆久了，与社会长期脱节，很容易形成安于现状的心理，以为每天勤奋的去实验室看文献，关注科研动态，坚持做实验就是一个好的博士生。可能对于做基础理论的博士生这样是可以的，<strong>但对于工科博士生，如果你的研究没有一个现实的落脚点或者应用点，那么个人觉得你的研究很有可能只是为了水论文，这一点对于工科是很致命的</strong>。我个人看法，<strong>真正的工科需要会解决实际问题的人才，而不是只会写论文的人才。</strong>之所以建议同学们保持学术目标时，时刻保持对产业界的关注，在于一旦你读博到最后一年时，你的职业规划可能会发生变化：或者发现自己已经厌倦科研圈子的内卷，或者开始想去产业界去赚钱了，或者学术没有那么有意思，不想一辈子科研了，这都是有可能的。这个时候眼光转向国内的产业界时，可能时间仓促没法做出最佳选择，这样会很被动。<strong>所以这几年在学校的时候，尤其是工科生，一定不要一心只读圣贤书，多关注国内和国际相关产业发展实际情况，然后不断与学术界的潜在选择进行对比评估，哪个选择是自己的最优选择。这样你会少走很多弯路。</strong></p><p>最后，这个社会一直在输出努力终会有汇报的价值观。但大多人工作后都会懂得“选择比努力更重要”的道理。有些行业本来就是一个下降的赛道，任凭你如何努力向上奔跑勤奋工作，可能一段时间后仍然在“原地踏步”，与另一个上升赛道的选手付出同样的努力，但差距可能会越来越远。</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 科研能力 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
