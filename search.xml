<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DARTS：Differentiable Architecture Search</title>
      <link href="/2023/06/23/DARTS/"/>
      <url>/2023/06/23/DARTS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>标题 - DARTS: Differentiable Architecture Search</p></blockquote><blockquote><p>摘要 - This paper addresses the scalability challenge of architecturesearch by formulating the task in a differentiable manner. Unlikeconventional approaches of applying evolution or reinforcement learningover a discrete and non-differentiable search space, our method is basedon the continuous relaxation of the architecture representation,allowing efficient search of the architecture using gradient descent.Extensive experiments on CIFAR-10, ImageNet, Penn Treebank andWikiText-2 show that our algorithm excels in discoveringhigh-performance convolutional architectures for image classificationand recurrent architectures for language modeling, while being orders ofmagnitude faster than state-of-the-art non-differentiable techniques.Our implementation has been made publicly available to facilitatefurther research on efficient architecture search algorithms.</p></blockquote><hr /><h1 id="note">1 💡Note</h1><h2 id="论文试图解决什么问题">1.1 论文试图解决什么问题？</h2><ul><li>Dominant approaches treated the architecture search as a black-boxoptimization problem over a discrete domain, leading to a large numberof architecture evaluation required.</li><li>Efficient architecture search</li></ul><h2 id="这是否是一个新的问题">1.2 这是否是一个新的问题？</h2><ul><li>Numerous prior studies have explored various approaches based onreinforcement learning (RL), evolution, or Bayesian optimization.</li><li>But DARTS approaches the problem from a different angle.</li></ul><h2 id="这篇文章要验证一个什么科学假设">1.3这篇文章要验证一个什么科学假设？</h2><p>Compared with previous methods, differentiable network architecturesearch based on bilevel optimization is <strong>efficient andtransferable.</strong></p><h2 id="有哪些相关研究如何归类谁是这一课题在领域内值得关注的研究员">1.4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ol type="1"><li>non-differentiable search techniques: reinforcement learning (RL),evolution, or Bayesian optimization.<ul><li>Low efficiency</li></ul></li><li>searching architectures within a continuous domain<ul><li>Seek to fine-tune a specific aspect of an architecture, as opposedto learning high-performance architecture building blocks with complexgraph topologies with a rich search space.</li></ul></li></ol><h2 id="论文中提到的解决方案之关键是什么">1.5🔴论文中提到的解决方案之关键是什么？</h2><blockquote><p>[!Note] An overview of DARTS <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="DARTS：Differentiable Architecture Search--20230621" /> (a)Operations on the edges are initially unknown.</p><ol start="2" type="a"><li><p>Continuous relaxation of the search space by placing a mixture ofcandidate operations on each edge.</p></li><li><p>Joint optimization of the mixing probabilities and the networkweights by solving a bilevel optimization problem.</p></li><li><p>Inducing the final architecture from the learned mixingprobabilities.</p></li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/1-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="1-DARTS：Differentiable Architecture Search--20230621" /><figcaption aria-hidden="true">1-DARTS：Differentiable ArchitectureSearch--20230621</figcaption></figure></blockquote><h3 id="search-space">1.5.1 Search Space</h3><blockquote><p>Searching for a computation <strong>cell</strong> as the buildingblock of the final architecture.</p></blockquote><ol type="1"><li>A cell is a <strong>directed acyclic graph</strong> consisting of anordered sequence of N nodes.</li><li>Each node: latent representation<ul><li>"Latent representation" refers to a representation form used inmachine learning and deep learning to represent hidden features orabstract representations of data.</li></ul></li><li>Each directed edge: operation</li><li>Each intermediate node is computed based on all of its predecessors:<span class="math display">\[x^{(j)}=\sum_{i&lt;j} o^{(i,j)}\left(x^{(i)}\right)\]</span></li><li>A special zero operation is also included to indicate a lack ofconnection between two nodes.</li><li>learning the cell <span class="math inline">\(\rightarrow\)</span>learning the operations on its edges.</li></ol><h3 id="continuous-relaxation-and-optimization">1.5.2 ContinuousRelaxation And Optimization</h3><ol type="1"><li><strong>Relaxation:</strong> Relax the categorical choice of aparticular operation to a softmax over all possible operations: <spanclass="math display">\[\bar{o}^{(i, j)}(x)=\sum_{o \in \mathcal{O}}\frac{\exp \left(\alpha_o^{(i, j)}\right)}{\sum_{o^{\prime} \in\mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)}o(x)\]</span><ul><li><span class="math inline">\(\alpha_(i, j)\)</span>: the operationmixing weights for a pair of nodes <span class="math inline">\((i,j)\)</span></li><li>learning a set of continuous variables <span class="math inline">\(α= \{α(i, j)\}\)</span></li></ul></li><li><strong>[[Bilevel optimization]]:</strong> After relaxation, ourgoal is to jointly learn the architecture <spanclass="math inline">\(\alpha\)</span> and the weights <spanclass="math inline">\(w\)</span> within all the mixed operations <spanclass="math display">\[\begin{array}{cl}\min _\alpha &amp;\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\\text {s.t. } &amp; w^*(\alpha)=\operatorname{argmin}_w \mathcal{L}_{\text{train }}(w, \alpha)\end{array}\]</span><ul><li>optimize the validation loss, but using gradient descent.</li><li>the architecture <span class="math inline">\(\alpha\)</span> couldbe viewed as a special type of hyper-parameter</li></ul></li></ol><h3 id="approximate-architecture-gradient">1.5.3 ApproximateArchitecture Gradient</h3><ol type="1"><li><strong>One-step Approximation:</strong> （Also can be seen inNesterov，MAML）Evaluating the architecture gradient exactly can beprohibitive due to the expensive inner optimization. <spanclass="math display">\[\begin{aligned} &amp; \nabla_\alpha\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\ \approx&amp; \nabla_\alpha \mathcal{L}_{\text {val }}\left(w-\xi \nabla_w\mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)\end{aligned}\]</span><ul><li><span class="math inline">\(\xi\)</span> helps to converge to abetter local optimum</li></ul></li><li><strong>Chain rule</strong>:<ol type="1"><li>Let: <span class="math display">\[\begin{align*}w^\prime=f(w,\alpha) &amp;= w - \xi\nabla_w \mathcal{L}_{\text{train}}(w, \alpha)\\g(w, \alpha) &amp;= \mathcal{L}_{\text{val}}(f(w, \alpha),\alpha)\end{align*}\]</span></li><li>So: <span class="math display">\[\begin{align*}\nabla_\alpha\mathcal{L}_{\text{val}}\left(w-\xi \nabla_w\mathcal{L}_{\text{train}}(w, \alpha), \alpha\right) &amp;=\frac{\partial g}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;=\frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;= \frac{\partial\mathcal{L}_{\text{val}}}{\partial f} \cdot \frac{\partial}{\partial\alpha} (w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) +\frac{d\mathcal{L}_\text{val}}{d\alpha}\end{align*}\]</span></li><li>Where: <span class="math display">\[\frac{\partial\mathcal{L}_{\text{val}}}{\partial f} = \nabla_\alpha \mathcal{L}_{v al}\left(w, \alpha\right)\]</span></li><li>And: <span class="math display">\[\frac{\partial}{\partial \alpha}(w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) = -\xi\nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha)\]</span></li><li>And: <span class="math display">\[\frac{dg}{d\alpha} = \nabla_\alpha\mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)\]</span></li><li>So: <span class="math display">\[\nabla_\alpha \mathcal{L}_{\text{val }}\left(w-\xi \nabla_w \mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)=\nabla_\alpha \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)-\xi \nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)\]</span></li></ol></li><li><strong>Finite difference approximation</strong>：The expressionabove contains an expensive matrix-vector product in its second term.<span class="math display">\[\nabla_{\alpha, w}^2 \mathcal{L}_{\text{train }}(w, \alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val}}\left(w^{\prime}, \alpha\right) \approx \frac{\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{+}, \alpha\right)-\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{-}, \alpha\right)}{2\epsilon}\]</span><ul><li>Central difference: <span class="math inline">\(f^{\prime}(x)\approx \frac{f(x+h)-f(x-h)}{2 h}\)</span></li><li>Taylor Formula:<ol type="1"><li>We have: <span class="math inline">\(f(x)=\frac{f\left(x_0\right)}{0!}+\frac{f^{\prime}\left(x_0\right)}{1!}\left(x-x_0\right)+\ldots\)</span></li><li>Then: <spanclass="math inline">\(f\left(x_0+h\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h+\ldots\)</span></li><li>Replace <span class="math inline">\(h\)</span> with <spanclass="math inline">\(hA\)</span>: <spanclass="math inline">\(\begin{aligned} &amp;f\left(x_0+hA\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h A+\ldots \\ &amp;f\left(x_0-hA\right)=f\left(x_0\right)-\frac{f^{\prime}\left(x_0\right)}{1 !}hA+\ldots\end{aligned}\)</span></li><li>Subtract one equation from another: <spanclass="math inline">\(f^{\prime}\left(x_0\right) \cdot A \approx\frac{f\left(x_0+h A\right)-f\left(x_0-h A\right)}{2 h}\)</span></li></ol></li><li>Where:<ol type="1"><li><span class="math inline">\(f^\prime(x_0)=\nabla_{\alpha, w}^2\mathcal{L}_{\text {train }}(w, \alpha)\)</span></li><li><span class="math inline">\(A=\nabla_{w^{\prime}} \mathcal{L}_{\text{val }}\left(w^{\prime}, \alpha\right)\)</span></li><li><span class="math inline">\(h=\epsilon\)</span></li><li><span class="math inline">\(w^{ \pm}=w \pm \epsilon\nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime},\alpha\right)\)</span></li></ol></li></ul></li><li><strong>First-order Approximation:</strong> <spanclass="math inline">\(\xi=0\)</span>, the second-order derivative willdisappear.</li></ol><h3 id="deriving-discrete-architectures">1.5.4 Deriving DiscreteArchitectures</h3><ol type="1"><li>The strength of an operation is defined as <spanclass="math inline">\(\frac{\exp \left(\alpha_o^{(i,j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp\left(\alpha_{o^{\prime}}^{(i, j)}\right)}\)</span>.</li><li><strong>Discretization:</strong> At the end of search, a discretearchitecture can be obtained by replacing each mixed operation with themost likely operation <span class="math display">\[o^{(i,j)}=\operatorname{argmax}_{o \in \mathcal{O}} \alpha_o^{(i,j)}\]</span></li><li>Retain the top-k strongest operations.</li></ol><h3 id="complexity-analysis">1.5.5 Complexity Analysis</h3><h2 id="论文中的实验是如何设计的">1.6 论文中的实验是如何设计的？</h2><h3 id="architecture-search">1.6.1 Architecture Search</h3><blockquote><p>Search for the cell architectures using DARTS, and determine the bestcells based on their validation performance <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/3-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="3-DARTS：Differentiable Architecture Search--20230621" /></p></blockquote><ul><li>operations sets：<ol type="1"><li>Convolutional Cells（Order: <spanclass="math inline">\(ReLU-Conv-BN\)</span>; N = 7 nodes）:<ol type="1"><li>3 × 3 and 5 × 5 separable convolutions</li><li>3 × 3 and 5 × 5 dilated separable convolutions</li><li>3 × 3 max pooling</li><li>3 × 3 average pooling</li><li>identity</li><li>zero</li></ol></li><li>Recurrent Cells（N = 12 nodes）:<ol type="1"><li>linear transformations<ol type="1"><li>tanh</li><li>relu</li><li>sigmoid</li></ol></li><li>identity</li><li>zero</li></ol></li></ol></li></ul><h3 id="architecture-evaluation">1.6.2 Architecture Evaluation</h3><blockquote><p>Use these cells to construct larger architectures, which we trainfrom scratch and report their performance on the test set.</p></blockquote><ul><li>To evaluate the selected architecture, randomly initialize itsweights (weights learned during the search process are discarded), trainit from scratch, and report its performance on the test set.</li></ul><h3 id="parameter-analysis">1.6.3 Parameter Analysis</h3><h4 id="alternative-optimization-strategies">1.6.3.1 AlternativeOptimization Strategies</h4><ol type="1"><li><span class="math inline">\(\alpha\)</span> and <spanclass="math inline">\(w\)</span> are jointly optimized over the union ofthe training and validation sets using <strong>coordinatedescent</strong><ul><li>Even worse than random search</li></ul></li><li>optimize <span class="math inline">\(\alpha\)</span> simultaneouslywith <span class="math inline">\(w\)</span> (without alteration) using<strong>SGD</strong><ul><li>Worse than DARTS</li></ul></li></ol><h4 id="search-with-increased-depth">1.6.3.2 Search with IncreasedDepth</h4><ol type="1"><li>The enlarged discrepancy of the number of channels betweenarchitecture search and final evaluation.</li><li>Searching with a deeper model might require differenthyper-parameters due to the increased number of layers to back-propthrough</li></ol><h2 id="论文中的实验及结果有没有很好地支持需要验证的科学假设">1.7论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><ol type="1"><li>Result #1 ：<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/2-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="2-DARTS：Differentiable Architecture Search--20230621" /></li><li>Result #2 :<ol type="1"><li>CIFAR-10 <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/4-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="4-DARTS：Differentiable Architecture Search--20230621" /></li><li>PTB <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/5-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="5-DARTS：Differentiable Architecture Search--20230621" /></li></ol></li><li>Result #3 :Transferability<ol type="1"><li>CIFAR-10 -&gt; ImageNet <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/6-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="6-DARTS：Differentiable Architecture Search--20230621" /></li><li>PTB -&gt; WT2<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/7-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="7-DARTS：Differentiable Architecture Search--20230621" /></li></ol></li></ol><h2 id="这篇论文到底有什么贡献">1.8 这篇论文到底有什么贡献？</h2><ol type="1"><li>A novel algorithm for differentiable network architecture searchbased on bilevel optimization.</li><li>Extensive experiments showing that gradient-based architecturesearch achieves highly competitive results and remarkable efficiencyimprovement.</li><li>Transferable architectures learned by DARTS.</li></ol><h2 id="下一步呢有什么工作可以继续深入">1.9下一步呢？有什么工作可以继续深入？</h2><ol type="1"><li>Differentiable architecture search on Graph neural networks.</li><li>Parallel DARTS</li></ol>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经典论文 </tag>
            
            <tag> 神经架构搜索 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
