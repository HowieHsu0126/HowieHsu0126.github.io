<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A New Model for Learning in Graph Domains</title>
      <link href="/2023/06/23/A-new-model-for-learning-in-graph-domains/"/>
      <url>/2023/06/23/A-new-model-for-learning-in-graph-domains/</url>
      
        <content type="html"><![CDATA[<h1 id="æ‘˜è¦ä¸ç»“è®º">1 ğŸ“‘æ‘˜è¦ä¸ç»“è®º</h1><h2 id="æ‘˜è¦">1.1 æ‘˜è¦</h2><blockquote><p>åŸæ–‡ï¼š In several applications the information is naturallyrepresented by graphs. Traditional approaches cope with graphical datastructures using a preprocessing phase which transforms the graphs intoa set of ï¬‚at vectors. However, in this way, important topologicalinformation may be lost and the achieved results may heavily depend onthe preprocessing stage. This paper presents a new neural model, calledgraph neural network (GNN), capable of directly processing graphs. GNNsextends recursive neural networks and can be applied on most of thepractically useful kinds of graphs, including directed, undirected,labelled and cyclic graphs. A learning algorithm for GNNs is proposedand some experiments are discussed which assess the properties of themodel.</p></blockquote><p><strong>ç¿»è¯‘ï¼šåœ¨ä¸€äº›åº”ç”¨ä¸­ï¼Œä¿¡æ¯è‡ªç„¶ç”¨å›¾å½¢è¡¨ç¤ºã€‚ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨é¢„å¤„ç†é˜¶æ®µå¤„ç†å›¾æ•°æ®ç»“æ„ï¼Œè¯¥é˜¶æ®µå°†å›¾å½¢è½¬æ¢ä¸ºä¸€ç»„å¹³é¢å‘é‡ã€‚ç„¶è€Œï¼Œè¿™æ ·åšå¯èƒ½ä¼šä¸¢å¤±é‡è¦çš„æ‹“æ‰‘ä¿¡æ¯ï¼Œæ‰€è·å¾—çš„ç»“æœå¯èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé¢„å¤„ç†é˜¶æ®µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»æ¨¡å‹ï¼Œç§°ä¸ºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥å¤„ç†å›¾ã€‚GNNæ‰©å±•äº†é€’å½’ç¥ç»ç½‘ç»œï¼Œå¯ä»¥åº”ç”¨äºå¤§å¤šæ•°å®é™…æœ‰ç”¨çš„å›¾ç±»å‹ï¼ŒåŒ…æ‹¬æœ‰å‘å›¾ã€æ— å‘å›¾ã€æ ‡è®°å›¾å’Œå¾ªç¯å›¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§GNNçš„å­¦ä¹ ç®—æ³•ï¼Œå¹¶è®¨è®ºäº†ä¸€äº›è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å®éªŒã€‚</strong></p><h2 id="ç»“è®º">1.2 ç»“è®º</h2><blockquote><p>åŸæ–‡ï¼šA new neural model, called graph neural network (GNN), waspresented. GNNs extend recursiveneural networks, since theycan processalar ger class of graphs and can be used on node focused problems. Somepreliminary experimental results confirmed that the model is verypromising. The experimentation of the approach on larger applications isa matter of future research. From a theoretical point of view,it isinteresting to study also the case when the input graph is notpredefined but it changes during the learning procedure.</p></blockquote><p><strong>ç¿»è¯‘ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç§°ä¸ºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€‚GNNæ‰©å±•äº†é€’å½’ç¥ç»ç½‘ç»œï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¤„ç†ä¸€ç±»æ›´å¤§çš„å›¾ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºä»¥èŠ‚ç‚¹ä¸ºä¸­å¿ƒçš„é—®é¢˜ã€‚ä¸€äº›åˆæ­¥å®éªŒç»“æœè¯å®äº†è¯¥æ¨¡å‹æ˜¯éå¸¸æœ‰å‰æ™¯çš„ã€‚è¯¥æ–¹æ³•åœ¨æ›´å¤§è§„æ¨¡åº”ç”¨ä¸Šçš„å®éªŒæ˜¯æœªæ¥ç ”ç©¶çš„é—®é¢˜ã€‚ä»ç†è®ºçš„è§’åº¦æ¥çœ‹ï¼Œç ”ç©¶è¾“å…¥å›¾å½¢ä¸æ˜¯é¢„å®šä¹‰çš„ï¼Œè€Œæ˜¯åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å‘ç”Ÿå˜åŒ–çš„æƒ…å†µä¹Ÿæ˜¯å¾ˆæœ‰è¶£çš„ã€‚</strong></p><hr /><h1 id="ç¬”è®°">2 ğŸ’¡ç¬”è®°</h1><h2 id="è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜">2.1 è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ</h2><p>åœ¨ä¸€äº›åº”ç”¨ä¸­ä¿¡æ¯ç”¨å›¾å½¢è¡¨ç¤ºã€‚ä¼ ç»Ÿæ–¹æ³•å°†å›¾å½¢è½¬æ¢ä¸ºä¸€ç»„å¹³é¢å‘é‡æ¥å¤„ç†å›¾æ•°æ®ç»“æ„ï¼Œè¿™æ ·åšå¯èƒ½ä¼šä¸¢å¤±é‡è¦çš„æ‹“æ‰‘ä¿¡æ¯ï¼Œæ‰€è·å¾—çš„ç»“æœä¹Ÿå¯èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé¢„å¤„ç†é˜¶æ®µã€‚</p><h2 id="è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ–°çš„é—®é¢˜">2.2 è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Ÿ</h2><p>å›¾æ•°æ®å¤„ç†åœ¨æœ¬æ–‡ä»¥å‰å·²æœ‰å¾ˆå¤šç ”ç©¶å·¥ä½œã€‚</p><h2 id="è¿™ç¯‡æ–‡ç« è¦éªŒè¯ä¸€ä¸ªä»€ä¹ˆç§‘å­¦å‡è®¾">2.3è¿™ç¯‡æ–‡ç« è¦éªŒè¯ä¸€ä¸ªä»€ä¹ˆç§‘å­¦å‡è®¾ï¼Ÿ</h2><p>è®ºæ–‡è¦éªŒè¯çš„æ˜¯ï¼šæœ¬æ–‡æå‡ºçš„å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆGNNï¼‰å¯ä»¥å¾ˆå¥½åœ°ç›´æ¥å¤„ç†å›¾æ•°æ®ã€‚</p><h2 id="æœ‰å“ªäº›ç›¸å…³ç ”ç©¶å¦‚ä½•å½’ç±»è°æ˜¯è¿™ä¸€è¯¾é¢˜åœ¨é¢†åŸŸå†…å€¼å¾—å…³æ³¨çš„ç ”ç©¶å‘˜">2.4æœ‰å“ªäº›ç›¸å…³ç ”ç©¶ï¼Ÿå¦‚ä½•å½’ç±»ï¼Ÿè°æ˜¯è¿™ä¸€è¯¾é¢˜åœ¨é¢†åŸŸå†…å€¼å¾—å…³æ³¨çš„ç ”ç©¶å‘˜ï¼Ÿ</h2><ul><li>åº”ç”¨ï¼š<ol type="1"><li>Node focused --&gt; Object Localization</li><li>Graph focused --&gt; Image Classification</li></ol></li><li>ä¼ ç»Ÿçš„åº”ç”¨é€šå¸¸é€šè¿‡é¢„å¤„ç†è¿‡ç¨‹æ¥å¤„ç†å›¾å½¢ï¼Œè¯¥é¢„å¤„ç†è¿‡ç¨‹å°†å›¾å½¢è½¬æ¢ä¸ºæ›´ç®€å•çš„è¡¨ç¤ºå½¢å¼ï¼ˆä¾‹å¦‚ï¼Œå‘é‡æˆ–å®æ•°åºåˆ—ï¼‰<ul><li>æ€§èƒ½å’Œæ³›åŒ–æ€§éƒ½å¾ˆå·®</li></ul></li><li>RNNæ¥ç›´æ¥å¤„ç†å›¾æ•°æ®ï¼šå°†å›¾ä¿¡æ¯ç¼–ç ä¸ºä¸èŠ‚ç‚¹ç›¸å…³è”çš„ä¸€ç»„çŠ¶æ€ï¼ŒçŠ¶æ€æ ¹æ®èŠ‚ç‚¹ä¹‹é—´çš„æ‹“æ‰‘å…³ç³»åŠ¨æ€æ›´æ–°ï¼Œæœ€åä½¿ç”¨å­˜å‚¨åœ¨çŠ¶æ€ä¸­çš„ç¼–ç è®¡ç®—è¾“å‡º<ul><li>RNNåªèƒ½å¤„ç†æœ‰å‘å›¾å’Œæ— ç¯å›¾</li><li>åªèƒ½ç”¨äºä»¥å›¾ä¸ºä¸­å¿ƒçš„é—®é¢˜</li></ul></li></ul><h2 id="è®ºæ–‡ä¸­æåˆ°çš„è§£å†³æ–¹æ¡ˆä¹‹å…³é”®æ˜¯ä»€ä¹ˆ">2.5ğŸ”´è®ºæ–‡ä¸­æåˆ°çš„è§£å†³æ–¹æ¡ˆä¹‹å…³é”®æ˜¯ä»€ä¹ˆï¼Ÿ</h2><h3 id="ä¸»ä½“æ¡†æ¶">2.5.1 ä¸»ä½“æ¡†æ¶</h3><ol type="1"><li><p>ç¬¦å·è¯´æ˜</p><ul><li>å›¾<span class="math inline">\(G\)</span>æ˜¯<spanclass="math inline">\(ï¼ˆN, Eï¼‰\)</span>å¯¹ï¼Œå…¶ä¸­<spanclass="math inline">\(N\)</span>æ˜¯èŠ‚ç‚¹é›†ï¼Œ<spanclass="math inline">\(E\)</span>æ˜¯è¾¹é›†ã€‚</li><li>ç”¨åœ†å¼§è¿æ¥åˆ°<span class="math inline">\(n\)</span>çš„èŠ‚ç‚¹ç”¨<spanclass="math inline">\(ne[n]\)</span>è¡¨ç¤ºã€‚</li><li>æ¯ä¸ªèŠ‚ç‚¹néƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾<span class="math inline">\(l_n\)</span></li><li>å›¾ä¸­çš„èŠ‚ç‚¹nè¡¨ç¤ºå¯¹è±¡æˆ–æ¦‚å¿µï¼Œè€Œè¾¹eè¡¨ç¤ºå®ƒä»¬çš„å…³ç³»ã€‚</li><li><span class="math inline">\(x_{ne[n]}\)</span>ä¸ºèŠ‚ç‚¹çŠ¶æ€</li><li><spanclass="math inline">\(l_{ne[n]}\)</span>ä¸ºnçš„é‚»å±…èŠ‚ç‚¹çš„æ ‡ç­¾</li></ul></li><li><p>ä¸ºæ¯ä¸€ä¸ªèŠ‚ç‚¹né™„åŠ ä¸€ä¸ªçŠ¶æ€å‘é‡<spanclass="math inline">\(x_n\)</span>ï¼ŒåŒ…å«é‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œæ„å»º==transitionfunction==ï¼š<spanclass="math display">\[\boldsymbol{x}_n=f_{\boldsymbol{w}}\left(\boldsymbol{l}_n,\boldsymbol{x}_{\mathrm{ne}[n]}, \boldsymbol{l}_{\mathrm{ne}[n]}\right),n \in \boldsymbol{N}\]</span> <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_1.png"alt="A new model for learning in graph domains_image_1" /></p></li><li><p>æ„å»º==output function==è®¡ç®—è¾“å‡ºï¼š<spanclass="math inline">\(\boldsymbol{o}_n=g\boldsymbol{w}\left(\boldsymbol{x}_n, \boldsymbol{l}_n\right), n \in\boldsymbol{N}\)</span></p></li><li><p>è®¡ç®—æ‰€æœ‰çš„çŠ¶æ€å’Œæ ‡ç­¾ï¼Œå¾—global transition function å’Œ globaloutput functionï¼š<span class="math display">\[\begin{aligned}\boldsymbol{x} &amp;=F_{\boldsymbol{w}}(\boldsymbol{x}, \boldsymbol{l})\\\boldsymbol{o} &amp;=G_{\boldsymbol{w}}(\boldsymbol{x}, \boldsymbol{l})\end{aligned}\]</span></p></li><li><p>Losså‡½æ•°ï¼š<span class="math inline">\(e\boldsymbol{w}=\sum_{i=1}^p\left(\boldsymbol{t}_i-\varphi\boldsymbol{w}\left(\boldsymbol{G}_i,n_i\right)\right)^2\)</span>ï¼Œå…¶ä¸­<span class="math inline">\(\varphi\boldsymbol{w}(\boldsymbol{G}_i, n_{i})=\boldsymbol{o}_n\)</span></p></li></ol><h3 id="éœ€è¦å…³æ³¨çš„é—®é¢˜">2.5.2 éœ€è¦å…³æ³¨çš„é—®é¢˜</h3><ol type="1"><li><p>å¦‚ä½•æ±‚è§£<span class="math inline">\(f_w\)</span>ï¼š<ahref="https://baike.baidu.com/item/%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86/9492042">Banachä¸åŠ¨ç‚¹åŸç†</a></p></li><li><p><span class="math display">\[\begin{aligned}&amp;\boldsymbol{x}_n(t+1)=f \boldsymbol{w}\left(\boldsymbol{l}_n,\boldsymbol{x}_{\mathrm{ne}[n]}(t),\boldsymbol{l}_{\mathrm{ne}[n]}\right) \text {, }\\&amp;\boldsymbol{o}_n(t+1)=g \boldsymbol{w}\left(\boldsymbol{x}_n(t+1),\boldsymbol{l}_n\right), \quad n \in \boldsymbol{N}\end{aligned}\]</span> <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_2.png"alt="A new model for learning in graph domains_image_2" /></p></li><li><p>å¦‚ä½•å­¦ä¹ ä¸¤å‡½æ•°ä¸­çš„å‚æ•°</p></li></ol><p>Step 1: çŠ¶æ€<spanclass="math inline">\(x_n(t)\)</span>ä½¿ç”¨ä¸Šå¼è¿­ä»£æ›´æ–°ï¼Œç›´åˆ°åˆ°è¾¾å›ºå®šç‚¹<spanclass="math inline">\(x(T)\)</span></p><p>Step 2: è®¡ç®—æŸå¤±æ¢¯åº¦<span class="math inline">\(\frac{\partial e\boldsymbol{w}(T)}{\partial\boldsymbol{w}}\)</span>ï¼Œå¹¶æ®æ­¤åŸºäºæ¢¯åº¦ä¸‹é™ç­–ç•¥æ›´æ–°å‚æ•°</p><ol start="3" type="1"><li>å¦‚ä½•å®ç°è¿™ä¸¤ä¸ªå‡½æ•°</li></ol><ul><li>Linear GNN: <span class="math display">\[ h\boldsymbol{w}\left(\boldsymbol{l}_n, \boldsymbol{x}_u,\boldsymbol{l}_u\right)=\boldsymbol{A}_{n, u}\boldsymbol{x}_u+\boldsymbol{b}_{n}\]</span></li></ul><p>å…¶ä¸­ï¼š <span class="math display">\[\begin{aligned}\boldsymbol{A}_{n, u} &amp;=\frac{\mu}{s|\operatorname{ne}[u]|} \cdot\operatorname{Resize}\left(\phi \boldsymbol{w}\left(\boldsymbol{l}_n,\boldsymbol{l}_u\right)\right) \\\boldsymbol{b}_n &amp;=\rho \boldsymbol{w}\left(\boldsymbol{l}_n\right)\end{aligned}\]</span> - Neural GNN: FNNæ¥å®ç°<spanclass="math inline">\(h_w\)</span> <span class="math display">\[e_{\boldsymbol{w}}=\sum_{i=1}^p\left(\boldsymbol{t}_i-\varphi\boldsymbol{w}\left(\boldsymbol{G}, n_i\right)\right)^2+\betaL\left(\left\|\frac{\partial F \boldsymbol{w}}{\partial\boldsymbol{x}}\right\|_1\right)\]</span></p><h2 id="è®ºæ–‡ä¸­çš„å®éªŒæ˜¯å¦‚ä½•è®¾è®¡çš„">2.6 è®ºæ–‡ä¸­çš„å®éªŒæ˜¯å¦‚ä½•è®¾è®¡çš„ï¼Ÿ</h2><ol type="1"><li>ä½¿ç”¨3å±‚å¸¦sigmoidæ¿€æ´»å‡½æ•°çš„FNNæ¥å®ç°ä¸¤ä¸ªæ¨¡å‹ä¸­çš„å‡½æ•°</li><li>çŠ¶æ€çš„ç»´åº¦ä¸º2</li><li>å®éªŒç»“æœå–5æ¬¡å¹³å‡å€¼</li><li>train 5000 epochs æ¯20 epochséªŒè¯ä¸€æ¬¡</li></ol><h2 id="ç”¨äºå®šé‡è¯„ä¼°çš„æ•°æ®é›†æ˜¯ä»€ä¹ˆè¯„ä¼°æ ‡å‡†æ˜¯ä»€ä¹ˆbaselineæ˜¯ä»€ä¹ˆ">2.7ç”¨äºå®šé‡è¯„ä¼°çš„æ•°æ®é›†æ˜¯ä»€ä¹ˆï¼Ÿè¯„ä¼°æ ‡å‡†æ˜¯ä»€ä¹ˆï¼ŸBaselineæ˜¯ä»€ä¹ˆï¼Ÿ</h2><h3 id="connection-based-problems">2.7.1 Connection-based problems</h3><ol type="1"><li><strong>The Clique problem:</strong><ol type="1"><li>æ•°æ®é›†: 1400ä¸ªå¸¦æœ‰20ä¸ªèŠ‚ç‚¹çš„éšæœºå›¾ï¼štrain 200 + val 200 + test1000</li><li>è¾“å…¥: 1ä¸ªå›¾</li><li>è¾“å‡º: æ˜¯å¦æœ‰sizeä¸º5çš„cliqueï¼ˆTrue = 1, False = -1ï¼‰</li><li>æŒ‡æ ‡: Accuracy, Time</li><li>åŸºçº¿:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li><li><strong>The Neighbors problem:</strong><ol type="1"><li>æ•°æ®é›†: ä¸€ä¸ªæœ‰500ä¸ªèŠ‚ç‚¹çš„å›¾ï¼štrain 100 + val 100 + test 300</li><li>è¾“å…¥: 1ä¸ªnode</li><li>è¾“å‡º: è¯¥nodeçš„æ‰€æœ‰é‚»å±…èŠ‚ç‚¹çš„ä¸ªæ•°</li><li>æŒ‡æ ‡: absolute relative errorï¼ŒTraining time</li><li>åŸºçº¿:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li><li><strong>The 2-order Neighborsproblem:</strong><ol type="1"><li>æ•°æ®é›†: ä¸€ä¸ªæœ‰500ä¸ªèŠ‚ç‚¹çš„å›¾ï¼štrain 100 + val 100 + test 300</li><li>è¾“å…¥: 1ä¸ªnode</li><li>è¾“å‡º: è¯¥nodeçš„æ‰€æœ‰é‚»å±…èŠ‚ç‚¹çš„ä¸ªæ•° + æ‰€æœ‰é‚»å±…çš„é‚»å±…èŠ‚ç‚¹çš„ä¸ªæ•°</li><li>æŒ‡æ ‡: absolute relative errorï¼ŒTraining time</li><li>åŸºçº¿:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li></ol><h3 id="label-based-problems">2.7.2 Label-based problems</h3><ol type="1"><li><strong>The Parity problem:</strong><ol type="1"><li>æ•°æ®é›†: 2500ä¸ªå›¾ï¼š(train + val) 500 + test2000ï¼Œæ¯å¼ å›¾ä¸Šçš„æ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªåŒ…å«8ä½å¸ƒå°”å…ƒç´ çš„å‘é‡ï¼ˆå®šä¹‰å¦‚æœè¯¥å‘é‡æœ‰å¶æ•°ä¸ª1åˆ™ä¸ºå¶èŠ‚ç‚¹ï¼Œåä¹‹ä¸ºå¥‡èŠ‚ç‚¹ï¼‰</li><li>è¾“å…¥: 1ä¸ªèŠ‚ç‚¹</li><li>è¾“å‡º: è¯¥èŠ‚ç‚¹æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹ï¼ˆTrue = 1, False = -1ï¼‰</li><li>æŒ‡æ ‡: Accuracy, Time</li><li>åŸºçº¿:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li><li>FNN (Layers = 3, Hidden = 20)</li></ol></li></ol></li></ol><h3 id="general-problems">2.7.3 General problems</h3><ol type="1"><li><strong>The Subgraph Matching problem:</strong><ol type="1"><li>æ•°æ®é›†: 600ä¸ªç›¸è¿çš„éšæœºå›¾ï¼štrain 200 + val 200 + test 200</li><li>è¾“å…¥: 1ä¸ªèŠ‚ç‚¹</li><li>è¾“å‡º: è¯¥èŠ‚ç‚¹æ˜¯å¦å±äºéšæœºç”Ÿæˆçš„å­å›¾ï¼ˆTrue = 1, False = -1ï¼‰</li><li>æŒ‡æ ‡: Accuracy</li><li>åŸºçº¿ï¼ˆå­å›¾èŠ‚ç‚¹ä¸ªæ•° = [3, 5, 7, 9]ï¼›å›¾èŠ‚ç‚¹ä¸ªæ•° = [6, 10, 14, 18]ï¼‰ï¼š<ol start="2" type="1"><li>neural GNN(State_dim = 5, Hidden = 5)</li><li>neural linear FNN(Hidden = 20)</li></ol></li></ol></li></ol><h2 id="è®ºæ–‡ä¸­çš„å®éªŒåŠç»“æœæœ‰æ²¡æœ‰å¾ˆå¥½åœ°æ”¯æŒéœ€è¦éªŒè¯çš„ç§‘å­¦å‡è®¾">2.8è®ºæ–‡ä¸­çš„å®éªŒåŠç»“æœæœ‰æ²¡æœ‰å¾ˆå¥½åœ°æ”¯æŒéœ€è¦éªŒè¯çš„ç§‘å­¦å‡è®¾ï¼Ÿ</h2><h3 id="connection-based-problems-1">2.8.1 Connection-basedproblems</h3><ol type="1"><li>The Clique problem</li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_3.png"alt="A new model for learning in graph domains_image_3" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_3</figcaption></figure><blockquote><p>ç»“è®ºï¼š 1. GNNæ¨¡å‹åœ¨è¯¥å®éªŒä¸­æ²¡æœ‰é‡åˆ°æ³›åŒ–æ€§é—®é¢˜ï¼› 2.éšè—ç¥ç»å…ƒçš„ä¸ªæ•°å¯¹Nerual GNNç»“æœæœ‰æ˜¾è‘—å½±å“ï¼Œå¯¹LinearGNNçš„ç»“æœå½±å“ä¸æ˜æ˜¾ï¼› 3. æ¯ä¸ªEpochçš„æ—¶é—´é•¿çŸ­ä¸æ•°æ®é›†å’Œç½‘ç»œå¤§å°éƒ½æœ‰å…³</p></blockquote><ol start="2" type="1"><li>The Neighborsproblem &amp; The 2-order Neighborsproblem</li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_4.png"alt="A new model for learning in graph domains_image_4" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_4</figcaption></figure><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_5.png"alt="A new model for learning in graph domains_image_5" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_5</figcaption></figure><h3 id="label-based-problems-1">2.8.2 Label-based problems</h3><p>The Parity problemï¼š <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_6.png"alt="A new model for learning in graph domains_image_6" /></p><blockquote><p>ç»“è®ºï¼šGNNæ¨¡å‹èƒ½å¤Ÿåœ¨è§£å†³é—®é¢˜ä¸éœ€è¦æ­¤ç±»ä¿¡æ¯çš„æƒ…å†µä¸‹ä¸¢å¼ƒå›¾æ‹“æ‰‘ä¸­åŒ…å«çš„ä¿¡æ¯ã€‚</p></blockquote><h3 id="general-problems-1">2.8.3 General problems</h3><p>The Subgraph Matching problemï¼š [[A new model for learning in graphdomains_image_7.png]]</p><h2 id="è¿™ç¯‡è®ºæ–‡åˆ°åº•æœ‰ä»€ä¹ˆè´¡çŒ®">2.9 è¿™ç¯‡è®ºæ–‡åˆ°åº•æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ</h2><p>æå‡ºäº†GNNæ¨¡å‹æ¥å¤„ç†å¾ˆå¤šå›¾é—®é¢˜ï¼Œå®éªŒè¡¨æ˜è¯¥æ¨¡å‹å¾ˆæœ‰å‰æ™¯ã€‚</p><h2 id="ä¸‹ä¸€æ­¥å‘¢æœ‰ä»€ä¹ˆå·¥ä½œå¯ä»¥ç»§ç»­æ·±å…¥">2.10ä¸‹ä¸€æ­¥å‘¢ï¼Ÿæœ‰ä»€ä¹ˆå·¥ä½œå¯ä»¥ç»§ç»­æ·±å…¥ï¼Ÿ</h2><p>å¦‚ä½•è§£å†³å›¾æœªå…ˆå®šä¹‰æ¡ä»¶ä¸‹çš„GNNå­¦ä¹ é—®é¢˜ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ–‡çŒ®é˜…è¯» </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ç»å…¸è®ºæ–‡ </tag>
            
            <tag> å›¾ç¥ç»ç½‘ç»œ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DARTSï¼šDifferentiable Architecture Search</title>
      <link href="/2023/06/23/DARTS/"/>
      <url>/2023/06/23/DARTS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>This paper addresses the scalability challenge of architecture searchby formulating the task in a differentiable manner. Unlike conventionalapproaches of applying evolution or reinforcement learning over adiscrete and non-differentiable search space, our method is based on thecontinuous relaxation of the architecture representation, allowingefficient search of the architecture using gradient descent. Extensiveexperiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 showthat our algorithm excels in discovering high-performance convolutionalarchitectures for image classification and recurrent architectures forlanguage modeling, while being orders of magnitude faster thanstate-of-the-art non-differentiable techniques. Our implementation hasbeen made publicly available to facilitate further research on efficientarchitecture search algorithms.</p></blockquote><hr /><h1 id="note">1 ğŸ’¡Note</h1><h2 id="è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜">1.1 è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ</h2><ul><li>Dominant approaches treated the architecture search as a black-boxoptimization problem over a discrete domain, leading to a large numberof architecture evaluation required.</li><li>Efficient architecture search</li></ul><h2 id="è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ–°çš„é—®é¢˜">1.2 è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Ÿ</h2><ul><li>Numerous prior studies have explored various approaches based onreinforcement learning (RL), evolution, or Bayesian optimization.</li><li>But DARTS approaches the problem from a different angle.</li></ul><h2 id="è¿™ç¯‡æ–‡ç« è¦éªŒè¯ä¸€ä¸ªä»€ä¹ˆç§‘å­¦å‡è®¾">1.3è¿™ç¯‡æ–‡ç« è¦éªŒè¯ä¸€ä¸ªä»€ä¹ˆç§‘å­¦å‡è®¾ï¼Ÿ</h2><p>Compared with previous methods, differentiable network architecturesearch based on bilevel optimization is <strong>efficient andtransferable.</strong></p><h2 id="æœ‰å“ªäº›ç›¸å…³ç ”ç©¶å¦‚ä½•å½’ç±»è°æ˜¯è¿™ä¸€è¯¾é¢˜åœ¨é¢†åŸŸå†…å€¼å¾—å…³æ³¨çš„ç ”ç©¶å‘˜">1.4æœ‰å“ªäº›ç›¸å…³ç ”ç©¶ï¼Ÿå¦‚ä½•å½’ç±»ï¼Ÿè°æ˜¯è¿™ä¸€è¯¾é¢˜åœ¨é¢†åŸŸå†…å€¼å¾—å…³æ³¨çš„ç ”ç©¶å‘˜ï¼Ÿ</h2><ol type="1"><li>non-differentiable search techniques: reinforcement learning (RL),evolution, or Bayesian optimization.<ul><li>Low efficiency</li></ul></li><li>searching architectures within a continuous domain<ul><li>Seek to fine-tune a specific aspect of an architecture, as opposedto learning high-performance architecture building blocks with complexgraph topologies with a rich search space.</li></ul></li></ol><h2 id="è®ºæ–‡ä¸­æåˆ°çš„è§£å†³æ–¹æ¡ˆä¹‹å…³é”®æ˜¯ä»€ä¹ˆ">1.5ğŸ”´è®ºæ–‡ä¸­æåˆ°çš„è§£å†³æ–¹æ¡ˆä¹‹å…³é”®æ˜¯ä»€ä¹ˆï¼Ÿ</h2><blockquote><p>[!Note] An overview of DARTS <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="DARTSï¼šDifferentiable Architecture Search--20230621" /> (a)Operations on the edges are initially unknown.</p><ol start="2" type="a"><li><p>Continuous relaxation of the search space by placing a mixture ofcandidate operations on each edge.</p></li><li><p>Joint optimization of the mixing probabilities and the networkweights by solving a bilevel optimization problem.</p></li><li><p>Inducing the final architecture from the learned mixingprobabilities.</p></li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/1-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="1-DARTSï¼šDifferentiable Architecture Search--20230621" /><figcaption aria-hidden="true">1-DARTSï¼šDifferentiable ArchitectureSearch--20230621</figcaption></figure></blockquote><h3 id="search-space">1.5.1 Search Space</h3><blockquote><p>Searching for a computation <strong>cell</strong> as the buildingblock of the final architecture.</p></blockquote><ol type="1"><li>A cell is a <strong>directed acyclic graph</strong> consisting of anordered sequence of N nodes.</li><li>Each node: latent representation<ul><li>"Latent representation" refers to a representation form used inmachine learning and deep learning to represent hidden features orabstract representations of data.</li></ul></li><li>Each directed edge: operation</li><li>Each intermediate node is computed based on all of its predecessors:<span class="math display">\[x^{(j)}=\sum_{i&lt;j} o^{(i,j)}\left(x^{(i)}\right)\]</span></li><li>A special zero operation is also included to indicate a lack ofconnection between two nodes.</li><li>learning the cell <span class="math inline">\(\rightarrow\)</span>learning the operations on its edges.</li></ol><h3 id="continuous-relaxation-and-optimization">1.5.2 ContinuousRelaxation And Optimization</h3><ol type="1"><li><strong>Relaxation:</strong> Relax the categorical choice of aparticular operation to a softmax over all possible operations: <spanclass="math display">\[\bar{o}^{(i, j)}(x)=\sum_{o \in \mathcal{O}}\frac{\exp \left(\alpha_o^{(i, j)}\right)}{\sum_{o^{\prime} \in\mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)}o(x)\]</span><ul><li><span class="math inline">\(\alpha_(i, j)\)</span>: the operationmixing weights for a pair of nodes <span class="math inline">\((i,j)\)</span></li><li>learning a set of continuous variables <span class="math inline">\(Î±= \{Î±(i, j)\}\)</span></li></ul></li><li><strong>[[Bilevel optimization]]:</strong> After relaxation, ourgoal is to jointly learn the architecture <spanclass="math inline">\(\alpha\)</span> and the weights <spanclass="math inline">\(w\)</span> within all the mixed operations <spanclass="math display">\[\begin{array}{cl}\min _\alpha &amp;\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\\text {s.t. } &amp; w^*(\alpha)=\operatorname{argmin}_w \mathcal{L}_{\text{train }}(w, \alpha)\end{array}\]</span><ul><li>optimize the validation loss, but using gradient descent.</li><li>the architecture <span class="math inline">\(\alpha\)</span> couldbe viewed as a special type of hyper-parameter</li></ul></li></ol><h3 id="approximate-architecture-gradient">1.5.3 ApproximateArchitecture Gradient</h3><ol type="1"><li><strong>One-step Approximation:</strong> ï¼ˆAlso can be seen inNesterovï¼ŒMAMLï¼‰Evaluating the architecture gradient exactly can beprohibitive due to the expensive inner optimization. <spanclass="math display">\[\begin{aligned} &amp; \nabla_\alpha\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\ \approx&amp; \nabla_\alpha \mathcal{L}_{\text {val }}\left(w-\xi \nabla_w\mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)\end{aligned}\]</span><ul><li><span class="math inline">\(\xi\)</span> helps to converge to abetter local optimum</li></ul></li><li><strong>Chain rule</strong>:<ol type="1"><li>Let: <span class="math display">\[\begin{align*}w^\prime=f(w,\alpha) &amp;= w - \xi\nabla_w \mathcal{L}_{\text{train}}(w, \alpha)\\g(w, \alpha) &amp;= \mathcal{L}_{\text{val}}(f(w, \alpha),\alpha)\end{align*}\]</span></li><li>So: <span class="math display">\[\begin{align*}\nabla_\alpha\mathcal{L}_{\text{val}}\left(w-\xi \nabla_w\mathcal{L}_{\text{train}}(w, \alpha), \alpha\right) &amp;=\frac{\partial g}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;=\frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;= \frac{\partial\mathcal{L}_{\text{val}}}{\partial f} \cdot \frac{\partial}{\partial\alpha} (w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) +\frac{d\mathcal{L}_\text{val}}{d\alpha}\end{align*}\]</span></li><li>Where: <span class="math display">\[\frac{\partial\mathcal{L}_{\text{val}}}{\partial f} = \nabla_\alpha \mathcal{L}_{v al}\left(w, \alpha\right)\]</span></li><li>And: <span class="math display">\[\frac{\partial}{\partial \alpha}(w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) = -\xi\nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha)\]</span></li><li>And: <span class="math display">\[\frac{dg}{d\alpha} = \nabla_\alpha\mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)\]</span></li><li>So: <span class="math display">\[\nabla_\alpha \mathcal{L}_{\text{val }}\left(w-\xi \nabla_w \mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)=\nabla_\alpha \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)-\xi \nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)\]</span></li></ol></li><li><strong>Finite difference approximation</strong>ï¼šThe expressionabove contains an expensive matrix-vector product in its second term.<span class="math display">\[\nabla_{\alpha, w}^2 \mathcal{L}_{\text{train }}(w, \alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val}}\left(w^{\prime}, \alpha\right) \approx \frac{\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{+}, \alpha\right)-\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{-}, \alpha\right)}{2\epsilon}\]</span><ul><li>Central difference: <span class="math inline">\(f^{\prime}(x)\approx \frac{f(x+h)-f(x-h)}{2 h}\)</span></li><li>Taylor Formula:<ol type="1"><li>We have: <span class="math inline">\(f(x)=\frac{f\left(x_0\right)}{0!}+\frac{f^{\prime}\left(x_0\right)}{1!}\left(x-x_0\right)+\ldots\)</span></li><li>Then: <spanclass="math inline">\(f\left(x_0+h\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h+\ldots\)</span></li><li>Replace <span class="math inline">\(h\)</span> with <spanclass="math inline">\(hA\)</span>: <spanclass="math inline">\(\begin{aligned} &amp;f\left(x_0+hA\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h A+\ldots \\ &amp;f\left(x_0-hA\right)=f\left(x_0\right)-\frac{f^{\prime}\left(x_0\right)}{1 !}hA+\ldots\end{aligned}\)</span></li><li>Subtract one equation from another: <spanclass="math inline">\(f^{\prime}\left(x_0\right) \cdot A \approx\frac{f\left(x_0+h A\right)-f\left(x_0-h A\right)}{2 h}\)</span></li></ol></li><li>Where:<ol type="1"><li><span class="math inline">\(f^\prime(x_0)=\nabla_{\alpha, w}^2\mathcal{L}_{\text {train }}(w, \alpha)\)</span></li><li><span class="math inline">\(A=\nabla_{w^{\prime}} \mathcal{L}_{\text{val }}\left(w^{\prime}, \alpha\right)\)</span></li><li><span class="math inline">\(h=\epsilon\)</span></li><li><span class="math inline">\(w^{ \pm}=w \pm \epsilon\nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime},\alpha\right)\)</span></li></ol></li></ul></li><li><strong>First-order Approximation:</strong> <spanclass="math inline">\(\xi=0\)</span>, the second-order derivative willdisappear.</li></ol><h3 id="deriving-discrete-architectures">1.5.4 Deriving DiscreteArchitectures</h3><ol type="1"><li>The strength of an operation is defined as <spanclass="math inline">\(\frac{\exp \left(\alpha_o^{(i,j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp\left(\alpha_{o^{\prime}}^{(i, j)}\right)}\)</span>.</li><li><strong>Discretization:</strong> At the end of search, a discretearchitecture can be obtained by replacing each mixed operation with themost likely operation <span class="math display">\[o^{(i,j)}=\operatorname{argmax}_{o \in \mathcal{O}} \alpha_o^{(i,j)}\]</span></li><li>Retain the top-k strongest operations.</li></ol><h3 id="complexity-analysis">1.5.5 Complexity Analysis</h3><h2 id="è®ºæ–‡ä¸­çš„å®éªŒæ˜¯å¦‚ä½•è®¾è®¡çš„">1.6 è®ºæ–‡ä¸­çš„å®éªŒæ˜¯å¦‚ä½•è®¾è®¡çš„ï¼Ÿ</h2><h3 id="architecture-search">1.6.1 Architecture Search</h3><blockquote><p>Search for the cell architectures using DARTS, and determine the bestcells based on their validation performance <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/3-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="3-DARTSï¼šDifferentiable Architecture Search--20230621" /></p></blockquote><ul><li>operations setsï¼š<ol type="1"><li>Convolutional Cellsï¼ˆOrder: <spanclass="math inline">\(ReLU-Conv-BN\)</span>; N = 7 nodesï¼‰:<ol type="1"><li>3 Ã— 3 and 5 Ã— 5 separable convolutions</li><li>3 Ã— 3 and 5 Ã— 5 dilated separable convolutions</li><li>3 Ã— 3 max pooling</li><li>3 Ã— 3 average pooling</li><li>identity</li><li>zero</li></ol></li><li>Recurrent Cellsï¼ˆN = 12 nodesï¼‰:<ol type="1"><li>linear transformations<ol type="1"><li>tanh</li><li>relu</li><li>sigmoid</li></ol></li><li>identity</li><li>zero</li></ol></li></ol></li></ul><h3 id="architecture-evaluation">1.6.2 Architecture Evaluation</h3><blockquote><p>Use these cells to construct larger architectures, which we trainfrom scratch and report their performance on the test set.</p></blockquote><ul><li>To evaluate the selected architecture, randomly initialize itsweights (weights learned during the search process are discarded), trainit from scratch, and report its performance on the test set.</li></ul><h3 id="parameter-analysis">1.6.3 Parameter Analysis</h3><h4 id="alternative-optimization-strategies">1.6.3.1 AlternativeOptimization Strategies</h4><ol type="1"><li><span class="math inline">\(\alpha\)</span> and <spanclass="math inline">\(w\)</span> are jointly optimized over the union ofthe training and validation sets using <strong>coordinatedescent</strong><ul><li>Even worse than random search</li></ul></li><li>optimize <span class="math inline">\(\alpha\)</span> simultaneouslywith <span class="math inline">\(w\)</span> (without alteration) using<strong>SGD</strong><ul><li>Worse than DARTS</li></ul></li></ol><h4 id="search-with-increased-depth">1.6.3.2 Search with IncreasedDepth</h4><ol type="1"><li>The enlarged discrepancy of the number of channels betweenarchitecture search and final evaluation.</li><li>Searching with a deeper model might require differenthyper-parameters due to the increased number of layers to back-propthrough</li></ol><h2 id="è®ºæ–‡ä¸­çš„å®éªŒåŠç»“æœæœ‰æ²¡æœ‰å¾ˆå¥½åœ°æ”¯æŒéœ€è¦éªŒè¯çš„ç§‘å­¦å‡è®¾">1.7è®ºæ–‡ä¸­çš„å®éªŒåŠç»“æœæœ‰æ²¡æœ‰å¾ˆå¥½åœ°æ”¯æŒéœ€è¦éªŒè¯çš„ç§‘å­¦å‡è®¾ï¼Ÿ</h2><ol type="1"><li>Result #1 ï¼š<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/2-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="2-DARTSï¼šDifferentiable Architecture Search--20230621" /></li><li>Result #2 :<ol type="1"><li>CIFAR-10 <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/4-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="4-DARTSï¼šDifferentiable Architecture Search--20230621" /></li><li>PTB <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/5-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="5-DARTSï¼šDifferentiable Architecture Search--20230621" /></li></ol></li><li>Result #3 :Transferability<ol type="1"><li>CIFAR-10 -&gt; ImageNet <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/6-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="6-DARTSï¼šDifferentiable Architecture Search--20230621" /></li><li>PTB -&gt; WT2<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/7-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="7-DARTSï¼šDifferentiable Architecture Search--20230621" /></li></ol></li></ol><h2 id="è¿™ç¯‡è®ºæ–‡åˆ°åº•æœ‰ä»€ä¹ˆè´¡çŒ®">1.8 è¿™ç¯‡è®ºæ–‡åˆ°åº•æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ</h2><ol type="1"><li>A novel algorithm for differentiable network architecture searchbased on bilevel optimization.</li><li>Extensive experiments showing that gradient-based architecturesearch achieves highly competitive results and remarkable efficiencyimprovement.</li><li>Transferable architectures learned by DARTS.</li></ol><h2 id="ä¸‹ä¸€æ­¥å‘¢æœ‰ä»€ä¹ˆå·¥ä½œå¯ä»¥ç»§ç»­æ·±å…¥">1.9ä¸‹ä¸€æ­¥å‘¢ï¼Ÿæœ‰ä»€ä¹ˆå·¥ä½œå¯ä»¥ç»§ç»­æ·±å…¥ï¼Ÿ</h2><ol type="1"><li>Differentiable architecture search on Graph neural networks.</li><li>Parallel DARTS</li></ol><h1 id="ä»£ç è§£æ">2 ä»£ç è§£æ</h1><p><ahref="https://github.com/HowieHsu0126/Haowei-Workshop/tree/main/NAS/DARTS">DARTS</a></p>]]></content>
      
      
      <categories>
          
          <category> æ–‡çŒ®é˜…è¯» </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ç»å…¸è®ºæ–‡ </tag>
            
            <tag> ç¥ç»æ¶æ„æœç´¢ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
