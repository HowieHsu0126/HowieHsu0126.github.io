<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A New Model for Learning in Graph Domains</title>
      <link href="/2023/06/23/A-new-model-for-learning-in-graph-domains/"/>
      <url>/2023/06/23/A-new-model-for-learning-in-graph-domains/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要与结论">1 📑摘要与结论</h1><h2 id="摘要">1.1 摘要</h2><blockquote><p>原文： In several applications the information is naturallyrepresented by graphs. Traditional approaches cope with graphical datastructures using a preprocessing phase which transforms the graphs intoa set of ﬂat vectors. However, in this way, important topologicalinformation may be lost and the achieved results may heavily depend onthe preprocessing stage. This paper presents a new neural model, calledgraph neural network (GNN), capable of directly processing graphs. GNNsextends recursive neural networks and can be applied on most of thepractically useful kinds of graphs, including directed, undirected,labelled and cyclic graphs. A learning algorithm for GNNs is proposedand some experiments are discussed which assess the properties of themodel.</p></blockquote><p><strong>翻译：在一些应用中，信息自然用图形表示。传统方法使用预处理阶段处理图数据结构，该阶段将图形转换为一组平面向量。然而，这样做可能会丢失重要的拓扑信息，所获得的结果可能在很大程度上取决于预处理阶段。本文提出了一种新的神经模型，称为图神经网络（GNN），能够直接处理图。GNN扩展了递归神经网络，可以应用于大多数实际有用的图类型，包括有向图、无向图、标记图和循环图。本文提出了一种GNN的学习算法，并讨论了一些评估模型性能的实验。</strong></p><h2 id="结论">1.2 结论</h2><blockquote><p>原文：A new neural model, called graph neural network (GNN), waspresented. GNNs extend recursiveneural networks, since theycan processalar ger class of graphs and can be used on node focused problems. Somepreliminary experimental results confirmed that the model is verypromising. The experimentation of the approach on larger applications isa matter of future research. From a theoretical point of view,it isinteresting to study also the case when the input graph is notpredefined but it changes during the learning procedure.</p></blockquote><p><strong>翻译：提出了一种新的神经网络模型，称为图神经网络（GNN）。GNN扩展了递归神经网络，因为它们可以处理一类更大的图，并且可以用于以节点为中心的问题。一些初步实验结果证实了该模型是非常有前景的。该方法在更大规模应用上的实验是未来研究的问题。从理论的角度来看，研究输入图形不是预定义的，而是在学习过程中发生变化的情况也是很有趣的。</strong></p><hr /><h1 id="笔记">2 💡笔记</h1><h2 id="论文试图解决什么问题">2.1 论文试图解决什么问题？</h2><p>在一些应用中信息用图形表示。传统方法将图形转换为一组平面向量来处理图数据结构，这样做可能会丢失重要的拓扑信息，所获得的结果也可能在很大程度上取决于预处理阶段。</p><h2 id="这是否是一个新的问题">2.2 这是否是一个新的问题？</h2><p>图数据处理在本文以前已有很多研究工作。</p><h2 id="这篇文章要验证一个什么科学假设">2.3这篇文章要验证一个什么科学假设？</h2><p>论文要验证的是：本文提出的图神经网络模型（GNN）可以很好地直接处理图数据。</p><h2 id="有哪些相关研究如何归类谁是这一课题在领域内值得关注的研究员">2.4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ul><li>应用：<ol type="1"><li>Node focused --&gt; Object Localization</li><li>Graph focused --&gt; Image Classification</li></ol></li><li>传统的应用通常通过预处理过程来处理图形，该预处理过程将图形转换为更简单的表示形式（例如，向量或实数序列）<ul><li>性能和泛化性都很差</li></ul></li><li>RNN来直接处理图数据：将图信息编码为与节点相关联的一组状态，状态根据节点之间的拓扑关系动态更新，最后使用存储在状态中的编码计算输出<ul><li>RNN只能处理有向图和无环图</li><li>只能用于以图为中心的问题</li></ul></li></ul><h2 id="论文中提到的解决方案之关键是什么">2.5🔴论文中提到的解决方案之关键是什么？</h2><h3 id="主体框架">2.5.1 主体框架</h3><ol type="1"><li><p>符号说明</p><ul><li>图<span class="math inline">\(G\)</span>是<spanclass="math inline">\(（N, E）\)</span>对，其中<spanclass="math inline">\(N\)</span>是节点集，<spanclass="math inline">\(E\)</span>是边集。</li><li>用圆弧连接到<span class="math inline">\(n\)</span>的节点用<spanclass="math inline">\(ne[n]\)</span>表示。</li><li>每个节点n都有一个标签<span class="math inline">\(l_n\)</span></li><li>图中的节点n表示对象或概念，而边e表示它们的关系。</li><li><span class="math inline">\(x_{ne[n]}\)</span>为节点状态</li><li><spanclass="math inline">\(l_{ne[n]}\)</span>为n的邻居节点的标签</li></ul></li><li><p>为每一个节点n附加一个状态向量<spanclass="math inline">\(x_n\)</span>，包含邻居节点的信息，构建==transitionfunction==：<spanclass="math display">\[\boldsymbol{x}_n=f_{\boldsymbol{w}}\left(\boldsymbol{l}_n,\boldsymbol{x}_{\mathrm{ne}[n]}, \boldsymbol{l}_{\mathrm{ne}[n]}\right),n \in \boldsymbol{N}\]</span> <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_1.png"alt="A new model for learning in graph domains_image_1" /></p></li><li><p>构建==output function==计算输出：<spanclass="math inline">\(\boldsymbol{o}_n=g\boldsymbol{w}\left(\boldsymbol{x}_n, \boldsymbol{l}_n\right), n \in\boldsymbol{N}\)</span></p></li><li><p>计算所有的状态和标签，得global transition function 和 globaloutput function：<span class="math display">\[\begin{aligned}\boldsymbol{x} &amp;=F_{\boldsymbol{w}}(\boldsymbol{x}, \boldsymbol{l})\\\boldsymbol{o} &amp;=G_{\boldsymbol{w}}(\boldsymbol{x}, \boldsymbol{l})\end{aligned}\]</span></p></li><li><p>Loss函数：<span class="math inline">\(e\boldsymbol{w}=\sum_{i=1}^p\left(\boldsymbol{t}_i-\varphi\boldsymbol{w}\left(\boldsymbol{G}_i,n_i\right)\right)^2\)</span>，其中<span class="math inline">\(\varphi\boldsymbol{w}(\boldsymbol{G}_i, n_{i})=\boldsymbol{o}_n\)</span></p></li></ol><h3 id="需要关注的问题">2.5.2 需要关注的问题</h3><ol type="1"><li><p>如何求解<span class="math inline">\(f_w\)</span>：<ahref="https://baike.baidu.com/item/%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86/9492042">Banach不动点原理</a></p></li><li><p><span class="math display">\[\begin{aligned}&amp;\boldsymbol{x}_n(t+1)=f \boldsymbol{w}\left(\boldsymbol{l}_n,\boldsymbol{x}_{\mathrm{ne}[n]}(t),\boldsymbol{l}_{\mathrm{ne}[n]}\right) \text {, }\\&amp;\boldsymbol{o}_n(t+1)=g \boldsymbol{w}\left(\boldsymbol{x}_n(t+1),\boldsymbol{l}_n\right), \quad n \in \boldsymbol{N}\end{aligned}\]</span> <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_2.png"alt="A new model for learning in graph domains_image_2" /></p></li><li><p>如何学习两函数中的参数</p></li></ol><p>Step 1: 状态<spanclass="math inline">\(x_n(t)\)</span>使用上式迭代更新，直到到达固定点<spanclass="math inline">\(x(T)\)</span></p><p>Step 2: 计算损失梯度<span class="math inline">\(\frac{\partial e\boldsymbol{w}(T)}{\partial\boldsymbol{w}}\)</span>，并据此基于梯度下降策略更新参数</p><ol start="3" type="1"><li>如何实现这两个函数</li></ol><ul><li>Linear GNN: <span class="math display">\[ h\boldsymbol{w}\left(\boldsymbol{l}_n, \boldsymbol{x}_u,\boldsymbol{l}_u\right)=\boldsymbol{A}_{n, u}\boldsymbol{x}_u+\boldsymbol{b}_{n}\]</span></li></ul><p>其中： <span class="math display">\[\begin{aligned}\boldsymbol{A}_{n, u} &amp;=\frac{\mu}{s|\operatorname{ne}[u]|} \cdot\operatorname{Resize}\left(\phi \boldsymbol{w}\left(\boldsymbol{l}_n,\boldsymbol{l}_u\right)\right) \\\boldsymbol{b}_n &amp;=\rho \boldsymbol{w}\left(\boldsymbol{l}_n\right)\end{aligned}\]</span> - Neural GNN: FNN来实现<spanclass="math inline">\(h_w\)</span> <span class="math display">\[e_{\boldsymbol{w}}=\sum_{i=1}^p\left(\boldsymbol{t}_i-\varphi\boldsymbol{w}\left(\boldsymbol{G}, n_i\right)\right)^2+\betaL\left(\left\|\frac{\partial F \boldsymbol{w}}{\partial\boldsymbol{x}}\right\|_1\right)\]</span></p><h2 id="论文中的实验是如何设计的">2.6 论文中的实验是如何设计的？</h2><ol type="1"><li>使用3层带sigmoid激活函数的FNN来实现两个模型中的函数</li><li>状态的维度为2</li><li>实验结果取5次平均值</li><li>train 5000 epochs 每20 epochs验证一次</li></ol><h2 id="用于定量评估的数据集是什么评估标准是什么baseline是什么">2.7用于定量评估的数据集是什么？评估标准是什么？Baseline是什么？</h2><h3 id="connection-based-problems">2.7.1 Connection-based problems</h3><ol type="1"><li><strong>The Clique problem:</strong><ol type="1"><li>数据集: 1400个带有20个节点的随机图：train 200 + val 200 + test1000</li><li>输入: 1个图</li><li>输出: 是否有size为5的clique（True = 1, False = -1）</li><li>指标: Accuracy, Time</li><li>基线:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li><li><strong>The Neighbors problem:</strong><ol type="1"><li>数据集: 一个有500个节点的图：train 100 + val 100 + test 300</li><li>输入: 1个node</li><li>输出: 该node的所有邻居节点的个数</li><li>指标: absolute relative error，Training time</li><li>基线:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li><li><strong>The 2-order Neighborsproblem:</strong><ol type="1"><li>数据集: 一个有500个节点的图：train 100 + val 100 + test 300</li><li>输入: 1个node</li><li>输出: 该node的所有邻居节点的个数 + 所有邻居的邻居节点的个数</li><li>指标: absolute relative error，Training time</li><li>基线:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li></ol></li></ol></li></ol><h3 id="label-based-problems">2.7.2 Label-based problems</h3><ol type="1"><li><strong>The Parity problem:</strong><ol type="1"><li>数据集: 2500个图：(train + val) 500 + test2000，每张图上的每个节点有一个包含8位布尔元素的向量（定义如果该向量有偶数个1则为偶节点，反之为奇节点）</li><li>输入: 1个节点</li><li>输出: 该节点是否为偶节点（True = 1, False = -1）</li><li>指标: Accuracy, Time</li><li>基线:<ol type="1"><li>neural (Hidden = [2, 5, 10, 20, 30])</li><li>linear (Hidden = [2, 5, 10, 20, 30])</li><li>FNN (Layers = 3, Hidden = 20)</li></ol></li></ol></li></ol><h3 id="general-problems">2.7.3 General problems</h3><ol type="1"><li><strong>The Subgraph Matching problem:</strong><ol type="1"><li>数据集: 600个相连的随机图：train 200 + val 200 + test 200</li><li>输入: 1个节点</li><li>输出: 该节点是否属于随机生成的子图（True = 1, False = -1）</li><li>指标: Accuracy</li><li>基线（子图节点个数 = [3, 5, 7, 9]；图节点个数 = [6, 10, 14, 18]）：<ol start="2" type="1"><li>neural GNN(State_dim = 5, Hidden = 5)</li><li>neural linear FNN(Hidden = 20)</li></ol></li></ol></li></ol><h2 id="论文中的实验及结果有没有很好地支持需要验证的科学假设">2.8论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><h3 id="connection-based-problems-1">2.8.1 Connection-basedproblems</h3><ol type="1"><li>The Clique problem</li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_3.png"alt="A new model for learning in graph domains_image_3" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_3</figcaption></figure><blockquote><p>结论： 1. GNN模型在该实验中没有遇到泛化性问题； 2.隐藏神经元的个数对Nerual GNN结果有显著影响，对LinearGNN的结果影响不明显； 3. 每个Epoch的时间长短与数据集和网络大小都有关</p></blockquote><ol start="2" type="1"><li>The Neighborsproblem &amp; The 2-order Neighborsproblem</li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_4.png"alt="A new model for learning in graph domains_image_4" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_4</figcaption></figure><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_5.png"alt="A new model for learning in graph domains_image_5" /><figcaption aria-hidden="true">A new model for learning in graphdomains_image_5</figcaption></figure><h3 id="label-based-problems-1">2.8.2 Label-based problems</h3><p>The Parity problem： <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/A%20new%20model%20for%20learning%20in%20graph%20domains_image_6.png"alt="A new model for learning in graph domains_image_6" /></p><blockquote><p>结论：GNN模型能够在解决问题不需要此类信息的情况下丢弃图拓扑中包含的信息。</p></blockquote><h3 id="general-problems-1">2.8.3 General problems</h3><p>The Subgraph Matching problem： [[A new model for learning in graphdomains_image_7.png]]</p><h2 id="这篇论文到底有什么贡献">2.9 这篇论文到底有什么贡献？</h2><p>提出了GNN模型来处理很多图问题，实验表明该模型很有前景。</p><h2 id="下一步呢有什么工作可以继续深入">2.10下一步呢？有什么工作可以继续深入？</h2><p>如何解决图未先定义条件下的GNN学习问题。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经典论文 </tag>
            
            <tag> 图神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DARTS：Differentiable Architecture Search</title>
      <link href="/2023/06/23/DARTS/"/>
      <url>/2023/06/23/DARTS/</url>
      
        <content type="html"><![CDATA[<blockquote><p>This paper addresses the scalability challenge of architecture searchby formulating the task in a differentiable manner. Unlike conventionalapproaches of applying evolution or reinforcement learning over adiscrete and non-differentiable search space, our method is based on thecontinuous relaxation of the architecture representation, allowingefficient search of the architecture using gradient descent. Extensiveexperiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 showthat our algorithm excels in discovering high-performance convolutionalarchitectures for image classification and recurrent architectures forlanguage modeling, while being orders of magnitude faster thanstate-of-the-art non-differentiable techniques. Our implementation hasbeen made publicly available to facilitate further research on efficientarchitecture search algorithms.</p></blockquote><hr /><h1 id="note">1 💡Note</h1><h2 id="论文试图解决什么问题">1.1 论文试图解决什么问题？</h2><ul><li>Dominant approaches treated the architecture search as a black-boxoptimization problem over a discrete domain, leading to a large numberof architecture evaluation required.</li><li>Efficient architecture search</li></ul><h2 id="这是否是一个新的问题">1.2 这是否是一个新的问题？</h2><ul><li>Numerous prior studies have explored various approaches based onreinforcement learning (RL), evolution, or Bayesian optimization.</li><li>But DARTS approaches the problem from a different angle.</li></ul><h2 id="这篇文章要验证一个什么科学假设">1.3这篇文章要验证一个什么科学假设？</h2><p>Compared with previous methods, differentiable network architecturesearch based on bilevel optimization is <strong>efficient andtransferable.</strong></p><h2 id="有哪些相关研究如何归类谁是这一课题在领域内值得关注的研究员">1.4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ol type="1"><li>non-differentiable search techniques: reinforcement learning (RL),evolution, or Bayesian optimization.<ul><li>Low efficiency</li></ul></li><li>searching architectures within a continuous domain<ul><li>Seek to fine-tune a specific aspect of an architecture, as opposedto learning high-performance architecture building blocks with complexgraph topologies with a rich search space.</li></ul></li></ol><h2 id="论文中提到的解决方案之关键是什么">1.5🔴论文中提到的解决方案之关键是什么？</h2><blockquote><p>[!Note] An overview of DARTS <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="DARTS：Differentiable Architecture Search--20230621" /> (a)Operations on the edges are initially unknown.</p><ol start="2" type="a"><li><p>Continuous relaxation of the search space by placing a mixture ofcandidate operations on each edge.</p></li><li><p>Joint optimization of the mixing probabilities and the networkweights by solving a bilevel optimization problem.</p></li><li><p>Inducing the final architecture from the learned mixingprobabilities.</p></li></ol><figure><imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/1-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="1-DARTS：Differentiable Architecture Search--20230621" /><figcaption aria-hidden="true">1-DARTS：Differentiable ArchitectureSearch--20230621</figcaption></figure></blockquote><h3 id="search-space">1.5.1 Search Space</h3><blockquote><p>Searching for a computation <strong>cell</strong> as the buildingblock of the final architecture.</p></blockquote><ol type="1"><li>A cell is a <strong>directed acyclic graph</strong> consisting of anordered sequence of N nodes.</li><li>Each node: latent representation<ul><li>"Latent representation" refers to a representation form used inmachine learning and deep learning to represent hidden features orabstract representations of data.</li></ul></li><li>Each directed edge: operation</li><li>Each intermediate node is computed based on all of its predecessors:<span class="math display">\[x^{(j)}=\sum_{i&lt;j} o^{(i,j)}\left(x^{(i)}\right)\]</span></li><li>A special zero operation is also included to indicate a lack ofconnection between two nodes.</li><li>learning the cell <span class="math inline">\(\rightarrow\)</span>learning the operations on its edges.</li></ol><h3 id="continuous-relaxation-and-optimization">1.5.2 ContinuousRelaxation And Optimization</h3><ol type="1"><li><strong>Relaxation:</strong> Relax the categorical choice of aparticular operation to a softmax over all possible operations: <spanclass="math display">\[\bar{o}^{(i, j)}(x)=\sum_{o \in \mathcal{O}}\frac{\exp \left(\alpha_o^{(i, j)}\right)}{\sum_{o^{\prime} \in\mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)}o(x)\]</span><ul><li><span class="math inline">\(\alpha_(i, j)\)</span>: the operationmixing weights for a pair of nodes <span class="math inline">\((i,j)\)</span></li><li>learning a set of continuous variables <span class="math inline">\(α= \{α(i, j)\}\)</span></li></ul></li><li><strong>[[Bilevel optimization]]:</strong> After relaxation, ourgoal is to jointly learn the architecture <spanclass="math inline">\(\alpha\)</span> and the weights <spanclass="math inline">\(w\)</span> within all the mixed operations <spanclass="math display">\[\begin{array}{cl}\min _\alpha &amp;\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\\text {s.t. } &amp; w^*(\alpha)=\operatorname{argmin}_w \mathcal{L}_{\text{train }}(w, \alpha)\end{array}\]</span><ul><li>optimize the validation loss, but using gradient descent.</li><li>the architecture <span class="math inline">\(\alpha\)</span> couldbe viewed as a special type of hyper-parameter</li></ul></li></ol><h3 id="approximate-architecture-gradient">1.5.3 ApproximateArchitecture Gradient</h3><ol type="1"><li><strong>One-step Approximation:</strong> （Also can be seen inNesterov，MAML）Evaluating the architecture gradient exactly can beprohibitive due to the expensive inner optimization. <spanclass="math display">\[\begin{aligned} &amp; \nabla_\alpha\mathcal{L}_{\text {val }}\left(w^*(\alpha), \alpha\right) \\ \approx&amp; \nabla_\alpha \mathcal{L}_{\text {val }}\left(w-\xi \nabla_w\mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)\end{aligned}\]</span><ul><li><span class="math inline">\(\xi\)</span> helps to converge to abetter local optimum</li></ul></li><li><strong>Chain rule</strong>:<ol type="1"><li>Let: <span class="math display">\[\begin{align*}w^\prime=f(w,\alpha) &amp;= w - \xi\nabla_w \mathcal{L}_{\text{train}}(w, \alpha)\\g(w, \alpha) &amp;= \mathcal{L}_{\text{val}}(f(w, \alpha),\alpha)\end{align*}\]</span></li><li>So: <span class="math display">\[\begin{align*}\nabla_\alpha\mathcal{L}_{\text{val}}\left(w-\xi \nabla_w\mathcal{L}_{\text{train}}(w, \alpha), \alpha\right) &amp;=\frac{\partial g}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;=\frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial \alpha} +\frac{dg}{d\alpha}\\&amp;= \frac{\partial\mathcal{L}_{\text{val}}}{\partial f} \cdot \frac{\partial}{\partial\alpha} (w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) +\frac{d\mathcal{L}_\text{val}}{d\alpha}\end{align*}\]</span></li><li>Where: <span class="math display">\[\frac{\partial\mathcal{L}_{\text{val}}}{\partial f} = \nabla_\alpha \mathcal{L}_{v al}\left(w, \alpha\right)\]</span></li><li>And: <span class="math display">\[\frac{\partial}{\partial \alpha}(w - \xi \nabla_w \mathcal{L}_{\text{train}}(w, \alpha)) = -\xi\nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha)\]</span></li><li>And: <span class="math display">\[\frac{dg}{d\alpha} = \nabla_\alpha\mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)\]</span></li><li>So: <span class="math display">\[\nabla_\alpha \mathcal{L}_{\text{val }}\left(w-\xi \nabla_w \mathcal{L}_{\text {train }}(w, \alpha),\alpha\right)=\nabla_\alpha \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)-\xi \nabla_{\alpha, w}^2 \mathcal{L}_{\text {train }}(w,\alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val }}\left(w^{\prime},\alpha\right)\]</span></li></ol></li><li><strong>Finite difference approximation</strong>：The expressionabove contains an expensive matrix-vector product in its second term.<span class="math display">\[\nabla_{\alpha, w}^2 \mathcal{L}_{\text{train }}(w, \alpha) \nabla_{w^{\prime}} \mathcal{L}_{\text {val}}\left(w^{\prime}, \alpha\right) \approx \frac{\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{+}, \alpha\right)-\nabla_\alpha\mathcal{L}_{\text {train }}\left(w^{-}, \alpha\right)}{2\epsilon}\]</span><ul><li>Central difference: <span class="math inline">\(f^{\prime}(x)\approx \frac{f(x+h)-f(x-h)}{2 h}\)</span></li><li>Taylor Formula:<ol type="1"><li>We have: <span class="math inline">\(f(x)=\frac{f\left(x_0\right)}{0!}+\frac{f^{\prime}\left(x_0\right)}{1!}\left(x-x_0\right)+\ldots\)</span></li><li>Then: <spanclass="math inline">\(f\left(x_0+h\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h+\ldots\)</span></li><li>Replace <span class="math inline">\(h\)</span> with <spanclass="math inline">\(hA\)</span>: <spanclass="math inline">\(\begin{aligned} &amp;f\left(x_0+hA\right)=f\left(x_0\right)+\frac{f^{\prime}\left(x_0\right)}{1!} h A+\ldots \\ &amp;f\left(x_0-hA\right)=f\left(x_0\right)-\frac{f^{\prime}\left(x_0\right)}{1 !}hA+\ldots\end{aligned}\)</span></li><li>Subtract one equation from another: <spanclass="math inline">\(f^{\prime}\left(x_0\right) \cdot A \approx\frac{f\left(x_0+h A\right)-f\left(x_0-h A\right)}{2 h}\)</span></li></ol></li><li>Where:<ol type="1"><li><span class="math inline">\(f^\prime(x_0)=\nabla_{\alpha, w}^2\mathcal{L}_{\text {train }}(w, \alpha)\)</span></li><li><span class="math inline">\(A=\nabla_{w^{\prime}} \mathcal{L}_{\text{val }}\left(w^{\prime}, \alpha\right)\)</span></li><li><span class="math inline">\(h=\epsilon\)</span></li><li><span class="math inline">\(w^{ \pm}=w \pm \epsilon\nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime},\alpha\right)\)</span></li></ol></li></ul></li><li><strong>First-order Approximation:</strong> <spanclass="math inline">\(\xi=0\)</span>, the second-order derivative willdisappear.</li></ol><h3 id="deriving-discrete-architectures">1.5.4 Deriving DiscreteArchitectures</h3><ol type="1"><li>The strength of an operation is defined as <spanclass="math inline">\(\frac{\exp \left(\alpha_o^{(i,j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp\left(\alpha_{o^{\prime}}^{(i, j)}\right)}\)</span>.</li><li><strong>Discretization:</strong> At the end of search, a discretearchitecture can be obtained by replacing each mixed operation with themost likely operation <span class="math display">\[o^{(i,j)}=\operatorname{argmax}_{o \in \mathcal{O}} \alpha_o^{(i,j)}\]</span></li><li>Retain the top-k strongest operations.</li></ol><h3 id="complexity-analysis">1.5.5 Complexity Analysis</h3><h2 id="论文中的实验是如何设计的">1.6 论文中的实验是如何设计的？</h2><h3 id="architecture-search">1.6.1 Architecture Search</h3><blockquote><p>Search for the cell architectures using DARTS, and determine the bestcells based on their validation performance <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/3-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="3-DARTS：Differentiable Architecture Search--20230621" /></p></blockquote><ul><li>operations sets：<ol type="1"><li>Convolutional Cells（Order: <spanclass="math inline">\(ReLU-Conv-BN\)</span>; N = 7 nodes）:<ol type="1"><li>3 × 3 and 5 × 5 separable convolutions</li><li>3 × 3 and 5 × 5 dilated separable convolutions</li><li>3 × 3 max pooling</li><li>3 × 3 average pooling</li><li>identity</li><li>zero</li></ol></li><li>Recurrent Cells（N = 12 nodes）:<ol type="1"><li>linear transformations<ol type="1"><li>tanh</li><li>relu</li><li>sigmoid</li></ol></li><li>identity</li><li>zero</li></ol></li></ol></li></ul><h3 id="architecture-evaluation">1.6.2 Architecture Evaluation</h3><blockquote><p>Use these cells to construct larger architectures, which we trainfrom scratch and report their performance on the test set.</p></blockquote><ul><li>To evaluate the selected architecture, randomly initialize itsweights (weights learned during the search process are discarded), trainit from scratch, and report its performance on the test set.</li></ul><h3 id="parameter-analysis">1.6.3 Parameter Analysis</h3><h4 id="alternative-optimization-strategies">1.6.3.1 AlternativeOptimization Strategies</h4><ol type="1"><li><span class="math inline">\(\alpha\)</span> and <spanclass="math inline">\(w\)</span> are jointly optimized over the union ofthe training and validation sets using <strong>coordinatedescent</strong><ul><li>Even worse than random search</li></ul></li><li>optimize <span class="math inline">\(\alpha\)</span> simultaneouslywith <span class="math inline">\(w\)</span> (without alteration) using<strong>SGD</strong><ul><li>Worse than DARTS</li></ul></li></ol><h4 id="search-with-increased-depth">1.6.3.2 Search with IncreasedDepth</h4><ol type="1"><li>The enlarged discrepancy of the number of channels betweenarchitecture search and final evaluation.</li><li>Searching with a deeper model might require differenthyper-parameters due to the increased number of layers to back-propthrough</li></ol><h2 id="论文中的实验及结果有没有很好地支持需要验证的科学假设">1.7论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><ol type="1"><li>Result #1 ：<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/2-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="2-DARTS：Differentiable Architecture Search--20230621" /></li><li>Result #2 :<ol type="1"><li>CIFAR-10 <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/4-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="4-DARTS：Differentiable Architecture Search--20230621" /></li><li>PTB <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/5-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="5-DARTS：Differentiable Architecture Search--20230621" /></li></ol></li><li>Result #3 :Transferability<ol type="1"><li>CIFAR-10 -&gt; ImageNet <imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/6-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="6-DARTS：Differentiable Architecture Search--20230621" /></li><li>PTB -&gt; WT2<imgsrc="https://image-hosting-service-1318887812.cos.ap-nanjing.myqcloud.com/7-DARTS%EF%BC%9ADifferentiable%20Architecture%20Search--20230621.png"alt="7-DARTS：Differentiable Architecture Search--20230621" /></li></ol></li></ol><h2 id="这篇论文到底有什么贡献">1.8 这篇论文到底有什么贡献？</h2><ol type="1"><li>A novel algorithm for differentiable network architecture searchbased on bilevel optimization.</li><li>Extensive experiments showing that gradient-based architecturesearch achieves highly competitive results and remarkable efficiencyimprovement.</li><li>Transferable architectures learned by DARTS.</li></ol><h2 id="下一步呢有什么工作可以继续深入">1.9下一步呢？有什么工作可以继续深入？</h2><ol type="1"><li>Differentiable architecture search on Graph neural networks.</li><li>Parallel DARTS</li></ol><h1 id="代码解析">2 代码解析</h1><p><ahref="https://github.com/HowieHsu0126/Haowei-Workshop/tree/main/NAS/DARTS">DARTS</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经典论文 </tag>
            
            <tag> 神经架构搜索 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
