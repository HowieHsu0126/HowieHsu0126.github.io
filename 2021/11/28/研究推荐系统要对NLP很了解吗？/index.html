<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="研究推荐系统要对NLP很了解吗？"><meta name="keywords" content="自然语言处理,知乎"><meta name="author" content="Haowei"><meta name="copyright" content="Haowei"><title>研究推荐系统要对NLP很了解吗？ | Haowei-Hub</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="manifest" href="/manifest.json"><link rel="manifest" href="/manifest.json"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b052bfe0975d2cbea2572b7ad21630a2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">1.研究动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%92%8CNLP%E7%9A%84%E5%8F%91%E5%B1%95%E5%85%B3%E7%B3%BB"><span class="toc-number">2.</span> <span class="toc-text">2. 推荐系统和NLP的发展关系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%8E%A8%E8%8D%90%E4%BB%A5NLP%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%BA%E5%9F%BA%E7%A1%80%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 推荐以NLP的发展为基础的原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%97%A9%E6%9C%9F%E9%9D%9E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 早期非深度学习时代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 深度学习时代</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">4.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Haowei</div><div class="author-info__description text-center">Chance favors the prepared mind.</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/HowieHsu0126">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">54</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">18</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://gitee.com/Howie0126">Gitee</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Haowei-Hub</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">博客</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">研究推荐系统要对NLP很了解吗？</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-28</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Natural-Language-Processing/">Natural Language Processing</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">5.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 20 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>作者：蘑菇先生<br>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/317441966/answer/2186924692">https://www.zhihu.com/question/317441966/answer/2186924692</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<p>最近在读Recsys2021上的paper：<strong>Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation</strong>，文章也聊到了NLP发展和推荐系统发展的关系，可以说推荐系统是在NLP的肩膀上前进的。结论是，<strong>关注不同领域，尤其是NLP领域的发展，对于推荐系统的研究和工作很可能都会有帮助。</strong></p>
<p>我们不妨以这篇论文为例，来看看作者的研究动机以及推荐系统和NLP发展的关系。</p>
<h2 id="1-研究动机"><a href="#1-研究动机" class="headerlink" title="1.研究动机"></a><strong>1.研究动机</strong></h2><p>近年来，<strong>序列/会话推荐</strong>上的进展很多都源自于<strong>NLP模型和预训练技术</strong>的发展，可以说<strong>推荐系统是在NLP的肩膀上前进的</strong>。尤其是<strong>Transformers架构</strong>，在BERT等预训练模型中广泛应用，也在序列推荐中初露端倪。然而，<strong>推荐系统的发展实际上是滞后于NLP的</strong>。NLP中花式Transformers架构层出不穷，比如：邱锡鹏组的survey[2]集中展示了Transformer架构的演进；NLP中的开源社区也十分活跃，比如HuggingFace的<strong>开源库Transformers</strong>[4]涵盖了大部分主流的Transformer实现，总之，Transformers在NLP研究中正如火如荼地开展着。</p>
<p>然而，在<strong>推荐系统</strong>中的应用很多只停留在最原始的Transformer[3]。很大程度上是缺乏一个类似HuggingFace的统一轮子，研究者想在此基础上做迭代实际上相比于NLP会<strong>困难不少</strong>。</p>
<p>为了弥补这种发展鸿沟，作者开源了一个基于HuggingFace开源库[4]的序列推荐包<strong>Transformers4Rec</strong>，目的是希望推荐系统社区能够更快地follow到NLP社区在<strong>Transformers中的进展</strong>，并在序列/会话推荐任务中实现<strong>开箱即用</strong>。完整的代码开源在了<a href="https://link.zhihu.com/?target=https://github.com/NVIDIA-Merlin/Transformers4Rec/">github</a>。</p>
<h2 id="2-推荐系统和NLP的发展关系"><a href="#2-推荐系统和NLP的发展关系" class="headerlink" title="2. 推荐系统和NLP的发展关系"></a><strong>2. 推荐系统和NLP的发展关系</strong></h2><p>近年来，关于序列推荐的工作也是层出不穷，survey[5]集中展示了<strong>基于深度学习的序列推荐</strong>研究进展。在大部分的序列推荐场景中，可能都只用了用户最新的交互行为数据，或者由于用户是匿名的，所以只能用到<strong>当前会话session下的序列行为</strong>，这也就是典型的<strong>session会话推荐场景</strong>，属于序列推荐中的一种。</p>
<h3 id="2-1-推荐以NLP的发展为基础的原因"><a href="#2-1-推荐以NLP的发展为基础的原因" class="headerlink" title="2.1 推荐以NLP的发展为基础的原因"></a><strong>2.1 推荐以NLP的发展为基础的原因</strong></h3><p>在最近10年内，大部分序列推荐的工作是在NLP发展的基础上开展的，个人认为主要是因为三方面：</p>
<ul>
<li><strong>a. 推荐问题</strong>和<strong>NLP问题</strong>的<strong>抽象形式非常相似</strong>[6]，如付鹏大佬所言， 推荐系统(尤其是序列推荐)的基本问题可以抽象成求解 <img src="https://www.zhihu.com/equation?tex=p(%5Ctext%7Bitem_i%7D+%7C+%5Ctext%7Buser%7D,+%5Ctext%7Bhistory_item_i%7D,+%5Ctext%7Bhistory_item_2%7D,...,+%5Ctext%7Bhistory_item_k%7D)" alt="[公式]">，即求解指定<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bitem%7D" alt="[公式]">在指定<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Buser%7D" alt="[公式]">的历史行为记录下可能产生行为的联合条件概率。而NLP的<strong>语言模型</strong>中，第<img src="https://www.zhihu.com/equation?tex=i" alt="[公式]">个word的概率也正是类似的形式：<img src="https://www.zhihu.com/equation?tex=p(w_i%7Cw_%7Bi-1%7D,+w_%7Bi-2%7D,+...,+w_1)" alt="[公式]">。  当省略<img src="https://www.zhihu.com/equation?tex=user" alt="[公式]">的信息，只使用历史行为记录的时候，推荐系统和NLP的问题变得<strong>惊人相似</strong>。</li>
<li><strong>b. 文本是推荐系统重要的side information</strong>。推荐系统中的side information多种多样，文本是其中最重要的来源之一。因此，基于NLP技术对文本做特征提取并输入到推荐系统中进行建模，就是一种很自然的想法。这样的前提下，利用最新的nlp进展来提升<strong>文本的表达能力</strong>，从而提升推荐系统的性能，也是一种很自然的动机。</li>
<li><strong>c. NLP的研究起步远早于推荐系统</strong>。NLP最早可以追溯到20世纪40年代和50年代[7]，经历了符号、规则、统计、RNN、word2vec、表征学习到预训练模型等。而推荐系统最早能追溯到20世纪90年代[8]，由哥伦比亚大学的Jussi Karlgren教授在一份报告中提出，最早的协同过滤推荐也是在90年代才被提出的，经历了从协同过滤、基于内容的推荐、矩阵分解SVD、分解机FM、Facebook提出的GBDT+LR、Youtube提出的DNN、花式DNN、Graph Embedding，GNN等等。</li>
</ul>
<p><strong>NLP发展始终快推荐系统一步</strong>，而二者的<strong>抽象问题又非常相似</strong>，且文本是推荐系统中重要的<strong>side information</strong>，因此很容易出现<strong>推荐系统社区</strong>研究人员在<strong>NLP社区</strong>的研究进展的基础上，直接应用或改进后用到推荐系统中。</p>
<h3 id="2-2-早期非深度学习时代"><a href="#2-2-早期非深度学习时代" class="headerlink" title="2.2 早期非深度学习时代"></a><strong>2.2 早期非深度学习时代</strong></h3><p>早期<strong>非深度学习时代</strong>，推荐系统受NLP的影响工作例如：</p>
<ul>
<li><strong>TF-IDF</strong>，推荐中的item frequency和NLP中的word frequency存在同样的头部或长尾特点，因此TF-IDF在NLP中可以用于提取关键词，在推荐系统中也能用来推荐Item。  </li>
<li><strong>SVD,LSA</strong>，隐语义分析最早被拿来抽取NLP的词向量，在推荐系统中，矩阵分解同样适用于user或者item的向量表示，在Netflix比赛中SVD和进阶版SVD++夺得了冠军。  </li>
<li><strong>LDA</strong>，LDA最早用于NLP中的主题发现或关键词提取[31]，后来也应用到推荐系统领域做基于内容的推荐或可解释推荐，这部分工作在深度学习流行起来之前非常多，各种基于概率图模型的推荐和自编码器等，都是受到LDA的影响。比如：荣获<strong>KDD2021时间检验研究奖</strong>的工作：<strong>协同主题回归</strong>[30]，探讨了传统基于矩阵分解的协同过滤方法和主题模型LDA的融合，能够提供非常好的解释性和为用户建立画像标签。  </li>
</ul>
<h3 id="2-3-深度学习时代"><a href="#2-3-深度学习时代" class="headerlink" title="2.3 深度学习时代"></a><strong>2.3 深度学习时代</strong></h3><p>进入深度学习时代后，NLP对推荐系统的影响就更加深远了，先看一张图：</p>
<p><img src="https://pic2.zhimg.com/50/v2-2cfbef9100d7b2d75a80657bc1083d1c_720w.jpg?source=1940ef5c" alt="img"></p>
<ul>
<li><strong>word2vec</strong>，这个影响就非常深远了，Mikolov大佬的作品，最早用于做词向量表征。也广泛影响着推荐系统的研究： </li>
<li><strong>item2vec</strong>，直接把word sequence替换成用户的历史交互行为序列item sequence，就能取得很好的效果，如电商领域的Prod2vec[14]。此外在各种各样的比赛中，基本上也都会有用。核心是利用了item-item之间的共现性。</li>
<li><strong>DeepWalk</strong>，稍微进阶一点的，DeepWalk，除了预处理在图上游走形成<strong>序列之外</strong>，其余的都是照搬word2vec那一套，包括训练使用skip-gram也是源自word2vec的训练方法。话说回来，基于概率的随机游走生成序列的高效实现方法<strong>Alias Table</strong>，在<strong>LDA</strong>中也被<strong>玩过了</strong>。</li>
<li><strong>负采样</strong>，目前推荐系统中非常注重样本负采样，关于负采样策略的”鼻祖”，我觉得<strong>word2vec</strong>论文绝对是其中之一。如基于item频次分布构造层次哈夫曼树；基于item频次的unigram分布做随机负采样等。尤其在<strong>推荐系统</strong>的召回阶段广泛应用。</li>
<li><strong>RNN</strong>，NLP中常用的LSTM, GRU等也被广泛地应用在推荐系统序列推荐或者用作序列特征提取器中。经典的工作比如GRU4Rec等，还比如，推荐系统中引入文本side information时，经常使用LSTM/GRU来做特征提取器，并作为DNN的输入。</li>
<li><strong>Attention</strong>。说起Attention，最早的起源实际上源于cv领域，当人类观察外界事物的时候，一般不会把事物当成一个整体去看，往往倾向于根据需要选择性的去获取被观察事物的某些重要部分。在seq2seq流行之后，就广泛地应用在了<strong>NLP领域</strong>，最早的应用比如Bengio大佬的机器翻译工作[10]，在Decoder端用注意力机制引入Encoder的上下文信息，让Decoder自适应的选择合适的信息进行建模。在推荐系统中，最早的attention工作如：NARM[15] (Neural Attentive Recommendation Machine)和Attentional FM[16]。而影响比较深远、且更经典的Attention应用是<strong>DIN</strong> [11] (Deep Interest Networks)，使用目标item对历史行为序列做attention。这些工作开辟了attention在推荐系统中应用的浪潮。</li>
<li><strong>Transformers</strong>。Transformers中最经典的结构非self-attention莫属。在推荐系统中，不仅Transformers本身应用广泛[13,17]，包括核心结构self-attention机制、多头机制等也被广泛应用，如经典的SR-GNN[12]就用了self-attention。</li>
<li><strong>BERT</strong>。BERT对推荐系统的影响在近两年来也产生了不少工作，例如BERT4Rec[18]，引入了双向Self-Attention进行序列建模，还比如：UBERT[19]，借鉴BERT的思想提升用户表征的表达能力。</li>
</ul>
<p>除了上述研究之外，还有不少最新<strong>前沿的推荐系统研究</strong>实际上也是深受NLP领域的影响，有一些还有着推荐系统领域”<strong>独特的味道</strong>“。</p>
<ul>
<li><strong>表征学习</strong>：最早的表征学习源自于NLP中做词向量。目前也广泛地在推荐系统研究领域开展着，且有着更明显的特点。推荐系统的稀疏性问题比NLP领域更严重：NLP领域的词表大小有限，但是推荐系统领域则面临着海量的用户和物品。因此，如何得到一个更合理的表征向量，对推荐系统来说非常重要。这方面工作例如KDD21上Google[20]和华为[21]的工作。</li>
<li><strong>图神经网络</strong>：图神经网络可能是极少数NLP和推荐系统齐头并进的领域，或者说在NLP和推荐系统上的发展有各自的特色。在NLP中，通常更注重图的构造，如何基于word、token、entity、句法树、依存树甚至引入知识图谱等，实际上更重要，这部分工作可以参考KDD21的tutorials：Deep Learning on Graphs for Natural Language Processing[23]，介绍的很仔细。在推荐系统领域，图的构造也很重要，但通常最优雅最自然的graph就是推荐系统天然的user-item二分图，因此推荐系统在GNN方面的研究则更注重模型结构上的改造、如何解决过平滑问题、可解释性、异构图建模等问题，包括GC-MC、LightGCN、SR-GNN等。</li>
<li><strong>知识图谱</strong>：知识图谱源于NLP的概念，在推荐系统中也早有应用。这部分研究最经典的如TransE[32]。近年来基于知识图谱的推荐主要和图网络结合在一起做，例如KGAT[33]等，这部分工作可以Follow斯坦福王鸿伟大佬的工作[34]。</li>
<li><strong>对比学习</strong>：对比学习可能最早要追溯到cv领域。后来在NLP中也广泛应用，例如SimCSE[24]。再后来又影响了推荐系统，目前基于对比学习的自监督推荐系统工作也非常多[22]，今年还涌现了不少引入对比学习做纠偏的工作[25]或抗噪的工作[28]。</li>
<li><strong>知识蒸馏</strong>：这方面工作实际上最早也是源于cv，出自Hinton大佬之手[26]。然后近年来随着预训练/BERT的兴起，对BERT如何做蒸馏成为了热点。而推荐系统领域应用知识蒸馏，最主流的方式是<strong>大模型蒸馏小模型</strong>或者<strong>后置跨链路蒸馏前置链路</strong>[27]。归根结底都是性能和准确性的权衡。</li>
<li><strong>预训练/迁移学习</strong>：也是得益于BERT的发展，目前推荐系统领域应用预训练/迁移学习的场景也变的非常多。例如：刘知远老师组的综述：<strong>基于预训练的推荐系统知识迁移综述</strong>[29]，能够有效解决推荐系统中的数据稀疏问题。</li>
</ul>
<p><strong>Transfromers4Rec</strong>工作实际上也是受到Transfomers、BERT以及NLP开源社区的影响所产生的。不同领域之间互相影响当然也是件好事，例如NLP领域也经常借鉴CV领域的成果，例如残差网络、CNN等等。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>从上面的介绍，我们可以看到NLP研究进展如何深远的影响着推荐系统的研究，可以说推荐系统的发展很大程度上是在NLP的肩膀上前进的。</p>
<p>到目前为止，我们看到的大多是单向影响，即：<strong>起步较早</strong>的领域深度影响着<strong>新兴领域</strong>，那么是否存在反哺的现象呢？也算是本篇文章的一个遗留问题，例如：<strong>推荐系统社区是否有反哺NLP社区呢？</strong> 这个问题可能也等价于推荐系统是否有一些独创/开创性的研究，是已经或有可能潜在影响其他领域的研究？(可能是个知乎好问题)。</p>
<p>这个问题也很大，需要花一些时间去学习和积累才能感受到。以读者目前的浅薄认知而言，我觉得有，至少我认为推荐系统在工程架构方面的进展是前沿的，是领先其他领域的。比如：实时推理引擎如TF Serving那一套，最早是服务于大规模实时推荐系统的，但对NLP尤其是预训练模型做实时serving还是有帮助的，小规模场景开箱即用，大规模场景做TF Serving优化后，也能使用。还有比如推荐系统中算首创的工作协同过滤或特征交互的研究进展，经典的FM等，也能一定程度上启发NLP研究。还有比如搜索/推荐场景常用的多阶段pipeline，召回+粗排+精排，也经常用在对话系统中。更多的以后有机会可以一起探讨探讨。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a><strong>参考</strong></h2><p>[1] de Souza Pereira Moreira G, Rabhi S, Lee J M, et al. <strong>Transformers4Rec: Bridging the Gap between NLP and Sequential/Session-Based Recommendation</strong>[C]//Fifteenth ACM Conference on Recommender Systems. 2021: 143-153.</p>
<p>[2] Lin T, Wang Y, Liu X, et al. <strong>A Survey of Transformers</strong>[J]. arXiv preprint arXiv:2106.04554, 2021.</p>
<p>[3] Vaswani A, Shazeer N, Parmar N, et al. <strong>Attention is all you need</strong>[C]//Advances in neural information processing systems. 2017: 5998-6008.</p>
<p>[4] <strong>State-of-the-art Natural Language Processing</strong> for Jax, PyTorch and TensorFlow: <a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
<p>[5] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. <strong>Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations</strong>. ACM Transactions on Information Systems (TOIS) 39, 1 (2020), 1–42.</p>
<p>[6] <strong>研究推荐系统要对NLP很了解吗？付鹏的回答：</strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/317441966/answer/633112869">https://www.zhihu.com/question/317441966/answer/633112869</a></p>
<p>[7] <strong>自然语言处理 NLP 的百年发展史</strong>, <a href="https://link.zhihu.com/?target=http://imgtec.eetrend.com/blog/2020/100052025.html">http://imgtec.eetrend.com/blog/2020/100052025.html</a></p>
<p>[8] <strong>一文尽览推荐系统模型演变史</strong>, <a href="https://link.zhihu.com/?target=https://cloud.tencent.com/developer/article/1652169">https://cloud.tencent.com/developer/article/1652169</a></p>
<p>[9] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. <strong>Session-based recommendations with recurrent neural networks</strong>. arXiv preprint arXiv:1511.06939 (2015).</p>
<p>[10] Bahdanau D, Cho K, Bengio Y. <strong>Neural machine translation by jointly learning to align and translate</strong>[J]. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>[11] Zhou G, Zhu X, Song C, et al. <strong>Deep interest network for click-through rate prediction</strong>[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2018: 1059-1068.</p>
<p>[12] Wu S, Tang Y, Zhu Y, et al. <strong>Session-based recommendation with graph neural networks</strong>[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 346-353.</p>
<p>[13] <strong>Transformer 在美团搜索排序中的实践</strong>: <a href="https://link.zhihu.com/?target=https://tech.meituan.com/2020/04/16/transformer-in-meituan.html">https://tech.meituan.com/2020/04/16/transformer-in-meituan.html</a></p>
<p>[14] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati, Jaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. <strong>E-commerce in your inbox: Product recommendations at scale</strong>. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 1809–1818.</p>
<p>[15] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, and Jun Ma. 2017. <strong>Neural Attentive Session-based Recommendation</strong>. arXiv:1711.04725 [cs.IR]</p>
<p>[16] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. <strong>Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks</strong>. arXiv:1708.04617 [cs.LG]</p>
<p>[17] Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wilfred Ng. 2019. <strong>SDM: Sequential deep matching model for online large-scale recommender system</strong>. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2635–2643.</p>
<p>[18] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. <strong>BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer</strong>. In Proceedings of the 28th ACM international conference on information and knowledge management. 1441–1450.</p>
<p>[19] Qiu Z, Wu X, Gao J, et al. <strong>U-BERT: Pre-training User Representations for Improved Recommendation</strong>[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(5): 4320-4327.</p>
<p>[20] Kang W C, Cheng D Z, Yao T, et al. <strong>Learning to Embed Categorical Features without Embedding Tables for Recommendation</strong>[C]//Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. 2021: 840-850.</p>
<p>[21] Guo H, Chen B, Tang R, et al. <strong>An Embedding Learning Framework for Numerical Features in CTR Prediction</strong>[J]. arXiv preprint arXiv:2012.08986, 2020.</p>
<p>[22] Wu J, Wang X, Feng F, et al. <strong>Self-supervised graph learning for recommendation</strong>[C]//Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021: 726-735.</p>
<p>[23] KDD2021, <strong>Deep Learning on Graphs for Natural Language Processing</strong>: <a href="https://link.zhihu.com/?target=https://dlg4nlp.github.io/">https://dlg4nlp.github.io/</a></p>
<p>[24] Gao T, Yao X, Chen D. <strong>SimCSE: Simple Contrastive Learning of Sentence Embeddings</strong>[J]. arXiv preprint arXiv:2104.08821, 2021.</p>
<p>[25] Zhou C, Ma J, Zhang J, et al. <strong>Contrastive learning for debiased candidate generation in large-scale recommender systems</strong>[C]//Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. 2021: 3985-3995.</p>
<p>[26] Hinton G, Vinyals O, Dean J. <strong>Distilling the knowledge in a neural network</strong>[J]. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>[27] <strong>张俊林，知识蒸馏在推荐系统的应用</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143155437">https://zhuanlan.zhihu.com/p/143155437</a></p>
<p>[28] <strong>张俊林，利用Contrastive Learning对抗数据噪声：对比学习在微博场景的实践</strong>，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/370782081">https://zhuanlan.zhihu.com/p/370782081</a></p>
<p>[29] Zeng Z, Xiao C, Yao Y, et al. <strong>Knowledge transfer via pre-training for recommendation: A review and prospect</strong>[J]. Frontiers in big Data, 2021, 4</p>
<p>[30] Wang C, Blei D M. <strong>Collaborative topic modeling for recommending scientific articles</strong>[C]//Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011: 448-456.</p>
<p>[31] Blei D M, Ng A Y, Jordan M I. <strong>Latent dirichlet allocation</strong>[J]. the Journal of machine Learning research, 2003, 3: 993-1022.</p>
<p>[32] Bordes A, Usunier N, Garcia-Duran A, et al. <strong>Translating embeddings for modeling multi-relational dat</strong>a[J]. Advances in neural information processing systems, 2013, 26.</p>
<p>[33] Wang X, He X, Cao Y, et al. Kgat: <strong>Knowledge graph attention network for recommendation</strong>[C]//Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2019: 950-958.</p>
<p>[34] Hongwei Wang, <a href="https://link.zhihu.com/?target=https://scholar.google.com/citations?user=3C__4wsAAAAJ&hl=zh-CN">https://scholar.google.com/citations?user=3C__4wsAAAAJ&amp;hl=zh-CN</a></p>
<p>原文也可参见我的知乎专栏：<a target="_blank" rel="noopener" href="https://www.zhihu.com/column/mgxs-note">蘑菇先生学习记</a>。</p>
<hr>
<blockquote>
<p>作者：付鹏<br>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/317441966/answer/633112869">https://www.zhihu.com/question/317441966/answer/633112869</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<p>照理来说是不需要的。但是对NLP有所涉猎对推荐系统大有裨益。</p>
<p><strong>一方面，许多推荐系统产品形态离不开NLP。</strong></p>
<ul>
<li>视频推荐有标题、作者、评论</li>
<li>电商推荐有标题、介绍、评论</li>
<li>新闻推荐有文章、评论</li>
</ul>
<p>目前的互联网服务形态，不管是UGC还是PGC还是什么样的产品，用户的参与都是极为重要的，只要有用户参与，就会有文本贡献，而这些强用户行为，是对推荐非常重要的。</p>
<p><strong>另一方面，推荐系统的问题抽象之后和NLP问题非常相似。</strong></p>
<p>这一点很重要，但是很多人都忽略掉了。</p>
<p>推荐系统的基本问题可以抽象成求解 <img src="https://www.zhihu.com/equation?tex=p(item_i%7Cuser,+history%5C_item_1,+history%5C_item_2,+...+history%5C_item_k)" alt="[公式]"> ，即求解指定 item在指定user的历史行为记录下可能产生行为的联合条件概率，其中 <img src="https://www.zhihu.com/equation?tex=item,+history%5C_item" alt="[公式]"> 的形式都是item id。</p>
<p>而NLP的language model中，第k个word的概率也正是类似的形式： <img src="https://www.zhihu.com/equation?tex=p(w_i%7Cw_%7Bi-1%7D,+w_%7Bi-2%7D,+...+w_1)" alt="[公式]"> 。</p>
<p>当我们省略user的信息，只使用历史记录的时候，推荐系统和NLP的基本问题变得惊人相似。</p>
<p>这当然不是巧合，有这个结果的原因就是，<strong>从某种角度，推荐系统和NLP问题都可以看做是在解决序列上的问题。</strong></p>
<p>所以，你可以看到许许多多的NLP技术都能被应用于推荐系统，并且大放异彩：</p>
<ul>
<li>idf是很好的NLP的权重方法，在推荐系统同样适用</li>
<li>SVD最早可以被拿来抽取NLP的词向量，后来在Netflix比赛中SVD和进阶版SVD++夺得了冠军</li>
<li>word2vec的模型，直接把word替换成用户的history，就能取得很好的效果，形成了推荐系统的item2vec算法</li>
<li>后期神经网络RNN/Attention能够被用来处理NLP的一系列问题，再过一两年，推荐系统上也开始大规模使用RNN/Attention</li>
<li>甚至于CNN被从CV领域借鉴过来，发现可以抽取文本特征之后，推荐系统也能够用CNN来抽取序列特征</li>
<li>…</li>
</ul>
<p>尤其以attention mechanism 和 item2vec 最为典型，如果没有NLP上的这些发现，很难说推荐系统会不会发展出同样的技术。</p>
<p>而推荐系统和其他的领域是没有这样高的契合度的。可以说NLP就是推荐系统的一个宝库也不为过，所以我推荐所有学习和从事推荐系统的同学都去学学NLP。</p>
<p><strong>如果你在做一个事情，身边有另一个事情有大把的经验可以直接借鉴，那为什么不去学呢？</strong></p>
<hr>
<blockquote>
<p>作者：大葱<br>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/317441966/answer/632942028">https://www.zhihu.com/question/317441966/answer/632942028</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<p>我个人觉得做推荐但对其他相关领域例如cv和nlp等有一定深度的涉猎是有很大帮助的。主要有几点吧，个人意见，仅供参考：</p>
<ol>
<li><p>众所周知，推荐领域经过这么多年的发展，已经不是仅仅依赖用户购买商品的交互数据来进行推荐了(一般我们认为这种方法是协同过滤)。往往在交互数据的基础上加入很多新信息(side information)是对推荐效果有一定提升的。而目前这类的side information主要形式也不外乎图片，文字这两种主要形式。所以怎么从这些新数据源提取有价值的信息并融入到推荐模型中，是需要cv和nlp的知识；</p>
</li>
<li><p>现在推荐无论工业界还是学术界基本都已经进入深度学习时代。虽然这么说显得技术含量不高，但是我个人愚见基于应用的深度学习算法里，模型架构很重要。由于一些场景的类似，在cv和nlp中很多已经成熟的架构用在推荐中也是有很直接的效果的。比如说在nlp中很火的skip-gram模型，在推荐中就被很多团队应用起来甚至做了基于应用场景的微调后取得了很不错的效果。还有cv界的attention机制，现在在推荐里也是司空见惯了；</p>
</li>
<li><p>最后想说回相似性，比如nlp中，词这个概念就和推荐中商品这个概念有很多接近的属性。比如说都有热门/冷门的区别，对一个session中用户行为的建模也经常参考nlp中对句子建模的方式。在模型训练中不是每个batch都能训练到所有的word/item。nlp领域怎么解决这些问题，都值得借鉴，反之亦然；</p>
</li>
</ol>
<p>等等这些理由，我觉得推荐现在要做好，确实是个很综合性的考量，不是说要你样样都精，但能做到对不同领域一专多能总是好的。另外，我觉得推荐还是一个对数据非常非常敏感的任务，重中之重应该是对数据的分析，清洗和研究。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Haowei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://howiehsu0126.github.io/2021/11/28/研究推荐系统要对NLP很了解吗？/">http://howiehsu0126.github.io/2021/11/28/研究推荐系统要对NLP很了解吗？/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://howiehsu0126.github.io">Haowei-Hub</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><a class="post-meta__tags" href="/tags/%E7%9F%A5%E4%B9%8E/">知乎</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/28/NLP%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7%E8%B7%AF%E7%BA%BF%E5%9B%BE%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E3%80%81%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E3%80%81%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A8%E8%A6%86%E7%9B%96-%E8%90%8C%E6%96%B0%E6%88%90%E9%95%BF%E5%BF%85%E5%A4%87/"><i class="fa fa-chevron-left">  </i><span>NLP打怪升级路线图：基础概念、常用方法、最佳模型，知识点全覆盖 | 萌新成长必备</span></a></div><div class="next-post pull-right"><a href="/2021/11/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E6%83%B3%E5%A5%BD%E5%A5%BD%E6%90%9E%E7%A7%91%E7%A0%94-%E8%AF%BB%E5%8D%9A%E5%A3%AB-%E5%BB%BA%E8%AE%AE%E5%B9%B3%E6%97%B6%E5%88%B7leetcode%E5%90%97-%E4%BC%9A%E4%B8%8D%E4%BC%9A%E6%B5%AA%E8%B4%B9%E6%97%B6%E9%97%B4%EF%BC%9F/"><span>计算机专业想好好搞科研 读博士 建议平时刷leetcode吗 会不会浪费时间？</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="lv-container" data-id="city" data-uid="MTAyMC81NDgxNS8zMTI4NQ"><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer class="footer-bg" style="background-image: url(https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F5ade6dc8f672ef4555bb5fb4ad3563a633d17faa.jpg&amp;refer=http%3A%2F%2Fi0.hdslb.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1640106300&amp;t=69b0263777fe687cedee393c0b465909)"><div class="layout" id="footer"><div class="copyright">&copy;2021 By Haowei</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">AI | Math | English & French | Computer Science | Life</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>